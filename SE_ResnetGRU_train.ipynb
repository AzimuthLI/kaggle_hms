{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time, ctime\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim.lr_scheduler import OneCycleLR,  CosineAnnealingWarmRestarts\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from scipy.signal import butter, lfilter, iirnotch, freqz\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import rel_entr, softmax\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dir:  ./outputs/\n",
      "{'Fp1': 0, 'T3': 1, 'C3': 2, 'O1': 3, 'Fp2': 4, 'C4': 5, 'T4': 6, 'O2': 7}\n"
     ]
    }
   ],
   "source": [
    "class KagglePaths:\n",
    "    OUTPUT_DIR = \"/kaggle/working/\"\n",
    "    PRE_LOADED_EEGS = '/kaggle/input/brain-eeg-spectrograms/eeg_specs.npy'\n",
    "    PRE_LOADED_SPECTROGRAMS = '/kaggle/input/brain-spectrograms/specs.npy'\n",
    "    TRAIN_CSV = \"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\"\n",
    "    TRAIN_EEGS = \"/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/\"\n",
    "    TRAIN_SPECTROGRAMS = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n",
    "    TEST_CSV = \"/kaggle/input/hms-harmful-brain-activity-classification/test.csv\"\n",
    "    TEST_SPECTROGRAMS = \"/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/\"\n",
    "    TEST_EEGS = \"/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/\"\n",
    "\n",
    "\n",
    "class LocalPaths:\n",
    "    OUTPUT_DIR = \"./outputs/\"\n",
    "    PRE_LOADED_EEGS = './inputs/brain-eeg-spectrograms/eeg_specs.npy'\n",
    "    PRE_LOADED_SPECTROGRAMS = './inputs/brain-spectrograms/specs.npy'\n",
    "    TRAIN_CSV = \"./inputs/hms-harmful-brain-activity-classification/train.csv\"\n",
    "    TRAIN_EEGS = \"./inputs/hms-harmful-brain-activity-classification/train_eegs\"\n",
    "    TRAIN_SPECTROGRAMS = \"./inputs/hms-harmful-brain-activity-classification/train_spectrograms\"\n",
    "    TEST_CSV = \"./inputs/hms-harmful-brain-activity-classification/test.csv\"\n",
    "    TEST_SPECTROGRAMS = \"./inputs/hms-harmful-brain-activity-classification/test_spectrograms\"\n",
    "    TEST_EEGS = \"./inputs/hms-harmful-brain-activity-classification/test_eegs\"\n",
    "\n",
    "PATHS = KagglePaths if os.path.exists(\"/kaggle\") else LocalPaths\n",
    "\n",
    "print(\"Output Dir: \", PATHS.OUTPUT_DIR)\n",
    "\n",
    "EEG_FEAT_ALL = [\n",
    "    'Fp1', 'F3', 'C3', 'P3', \n",
    "    'F7', 'T3', 'T5', 'O1', \n",
    "    'Fz', 'Cz', 'Pz', 'Fp2', \n",
    "    'F4', 'C4', 'P4', 'F8', \n",
    "    'T4', 'T6', 'O2', 'EKG'\n",
    "    ]\n",
    "\n",
    "EEG_FEAT_USE =  ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\n",
    "EEG_FEAT_INDEX = {x:y for x,y in zip(EEG_FEAT_USE, range(len(EEG_FEAT_USE)))}\n",
    "\n",
    "BRAIN_ACTIVITY = ['seizure', 'lpd', 'gpd', 'lrda', 'grda', 'other']\n",
    "TARGETS = [f\"{lb}_vote\" for lb in BRAIN_ACTIVITY]\n",
    "TARGETS_PRED = [f\"{lb}_pred\" for lb in BRAIN_ACTIVITY]\n",
    "\n",
    "print(EEG_FEAT_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_dir, logger_name=\"train_model.log\"):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger_file = os.path.join(log_dir, logger_name)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=logger_file, mode=\"a+\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "def eeg_from_parquet(parquet_path: str, use_feature=EEG_FEAT_USE, display: bool = False) -> np.ndarray:\n",
    "    # === Extract full length EEG Sequence ===\n",
    "    # fill missing values with mean\n",
    "    # first fill missing values with mean of each column\n",
    "    # then if all values are missing, fill with 0\n",
    "    eeg = pd.read_parquet(parquet_path, columns=use_feature)\n",
    "    eeg = eeg.fillna(eeg.mean(skipna=True)).fillna(0)\n",
    "    data = eeg.values.astype(np.float32)\n",
    "    \n",
    "    rows = len(eeg)\n",
    "    offset = (rows - 10_000) // 2 # 50 * 200 = 10_000\n",
    "    data = data[offset:offset+10_000, :]\n",
    "\n",
    "    if display:\n",
    "        fig, ax = plt.subplots(len(use_feature), 1, figsize=(10, 2*len(use_feature)), sharex=True)\n",
    "        \n",
    "        for i, feat in enumerate(use_feature):\n",
    "            ax[i].plot(data[:, i], label=feat)\n",
    "            ax[i].legend()\n",
    "            ax[i].grid()\n",
    "       \n",
    "        name = parquet_path.split('/')[-1].split('.')[0]\n",
    "        ax[0].set_title(f'EEG {name}',size=16)\n",
    "        fig.tight_layout()\n",
    "        plt.show()    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    SEED = 20\n",
    "    SPLIT_ENTROPY = 5.5\n",
    "    MODEL_NAME = \"ResnetGRU_v3_SE\"\n",
    "    MODEL_BACKBONE = \"reset_gru\"\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    EARLY_STOP_ROUNDS = 5\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    DROP_RATE = 0.15 # default: 0.1\n",
    "    DROP_PATH_RATE = 0.25 # default: 0.2\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    AMP = True\n",
    "    PRINT_FREQ = 100\n",
    "    NUM_WORKERS = 0 \n",
    "    MAX_GRAD_NORM = 1e7\n",
    "    REGULARIZATION = 0.15\n",
    "    RESNET_GRU_BANDPASS = (0.5, 20) #(0.5, 20)\n",
    "    RESNET_GRU_LOWPASS = 20\n",
    "    RESNET_GRU_NOTCH = (60, 30) # (Freq, Quality)\n",
    "    RESNET_GRU_IN_CHANNELS = 8\n",
    "    RESNET_GRU_KERNELS = [3, 5, 7, 9, 11]\n",
    "    RESNET_GRU_FIXED_KERNEL_SIZE = 5\n",
    "    RESNET_GRU_DOWNSAMPLE = 5\n",
    "    RESNET_GRU_HIDDEN_SIZE = 304 \n",
    "    RESNET_GRU_DILATED = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(ModelConfig.SEED)\n",
    "logger = get_logger(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_train.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7198b633c14c47e2bf2ee3ce46e6ed84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 10.6 s, total: 1min 31s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CREATE_EEGS = True\n",
    "ALL_EEG_SIGNALS = {}\n",
    "eeg_paths = list(Path(PATHS.TRAIN_EEGS).glob('*.parquet'))\n",
    "preload_eegs_path = Path('./inputs/eegs_full.npy')\n",
    "\n",
    "if CREATE_EEGS:\n",
    "    count = 0\n",
    "    for parquet_path in tqdm(eeg_paths, total=len(eeg_paths)):\n",
    "        eeg_id = int(parquet_path.stem)\n",
    "        eeg_path = str(parquet_path)\n",
    "        data = eeg_from_parquet(eeg_path, display=False)\n",
    "        ALL_EEG_SIGNALS[eeg_id] = data\n",
    "        count += 1\n",
    "    np.save(\"./inputs/eegs_full.npy\", ALL_EEG_SIGNALS)\n",
    "else:\n",
    "    ALL_EEG_SIGNALS = np.load(preload_eegs_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_non_overlap_samples(df_csv, targets):\n",
    "    # Reference Discussion:\n",
    "    # https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021\n",
    "\n",
    "    tgt_list = targets.tolist()\n",
    "    brain_activity = ['seizure', 'lpd', 'gpd', 'lrda', 'grda', 'other']\n",
    "\n",
    "    agg_dict = {\n",
    "        'spectrogram_id': 'first',\n",
    "        'spectrogram_label_offset_seconds': ['min', 'max'],\n",
    "        'patient_id': 'first',\n",
    "        'expert_consensus': 'first'\n",
    "    }\n",
    "\n",
    "    groupby = df_csv.groupby(['eeg_id'] + tgt_list)\n",
    "    train = groupby.agg(agg_dict)\n",
    "    train = train.reset_index()\n",
    "    train.columns = ['_'.join(col).strip() for col in train.columns.values]\n",
    "    train.columns = [\"eeg_id\"] + tgt_list + ['spectrogram_id', 'min', 'max', 'patient_id', 'target']\n",
    "    \n",
    "    train['total_votes'] = train[tgt_list].sum(axis=1)\n",
    "    train[tgt_list] = train[tgt_list].div(train['total_votes'], axis=0)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:  ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "train_all.shape =  (20183, 13)\n",
      "train_all nan_count:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>target</th>\n",
       "      <th>total_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>Other</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>LPD</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>5955</td>\n",
       "      <td>Other</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>38549</td>\n",
       "      <td>GPD</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40955</td>\n",
       "      <td>Other</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "0  568657           0.0  0.000000      0.25   0.000000   0.166667    0.583333   \n",
       "1  582999           0.0  0.857143      0.00   0.071429   0.000000    0.071429   \n",
       "2  642382           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "3  751790           0.0  0.000000      1.00   0.000000   0.000000    0.000000   \n",
       "4  778705           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "\n",
       "   spectrogram_id     min     max  patient_id target  total_votes  \n",
       "0       789577333     0.0    16.0       20654  Other         12.0  \n",
       "1      1552638400     0.0    38.0       20230    LPD         14.0  \n",
       "2        14960202  1008.0  1032.0        5955  Other          1.0  \n",
       "3       618728447   908.0   908.0       38549    GPD          1.0  \n",
       "4        52296320     0.0     0.0       40955  Other          2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "train_hard.shape =  (6492, 13)\n",
      "train_hard nan_count:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>target</th>\n",
       "      <th>total_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>Other</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>LPD</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1895581</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>128369999</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>47999</td>\n",
       "      <td>Other</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2482631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>978166025</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>20606</td>\n",
       "      <td>Other</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2521897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>673742515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62117</td>\n",
       "      <td>Other</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  \\\n",
       "0   568657      0.000000  0.000000  0.250000   0.000000   0.166667   \n",
       "1   582999      0.000000  0.857143  0.000000   0.071429   0.000000   \n",
       "2  1895581      0.076923  0.000000  0.000000   0.000000   0.076923   \n",
       "3  2482631      0.000000  0.000000  0.133333   0.066667   0.133333   \n",
       "4  2521897      0.000000  0.000000  0.083333   0.083333   0.333333   \n",
       "\n",
       "   other_vote  spectrogram_id     min     max  patient_id target  total_votes  \n",
       "0    0.583333       789577333     0.0    16.0       20654  Other         12.0  \n",
       "1    0.071429      1552638400     0.0    38.0       20230    LPD         14.0  \n",
       "2    0.846154       128369999  1138.0  1138.0       47999  Other         13.0  \n",
       "3    0.666667       978166025  1902.0  1944.0       20606  Other         15.0  \n",
       "4    0.500000       673742515     0.0     4.0       62117  Other         12.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original Split \n",
    "\n",
    "train_csv = pd.read_csv(PATHS.TRAIN_CSV)\n",
    "targets = train_csv.columns[-6:]\n",
    "\n",
    "print(\"targets: \", targets.to_list())\n",
    "\n",
    "train_csv['total_votes'] = train_csv[targets].sum(axis=1)\n",
    "train_csv[targets] = train_csv[targets].astype('float32')\n",
    "\n",
    "targets_prob = [f\"{t.split('_')[0]}_prob\" for t in targets]\n",
    "train_csv[targets_prob] = train_csv[targets].div(train_csv['total_votes'], axis=0)\n",
    "\n",
    "hard_csv = train_csv[train_csv['total_votes'] >= 6].copy().reset_index(drop=True)\n",
    "\n",
    "train_all = gen_non_overlap_samples(train_csv, targets)\n",
    "train_hard = gen_non_overlap_samples(hard_csv, targets)\n",
    "\n",
    "print(\"train_all.shape = \", train_all.shape)\n",
    "print(\"train_all nan_count: \", train_all.isnull().sum().sum())\n",
    "display(train_all.head())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"train_hard.shape = \", train_hard.shape)\n",
    "print(\"train_hard nan_count: \", train_hard.isnull().sum().sum())\n",
    "display(train_hard.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(ax, data, title, fs=200):\n",
    "    n = len(data)\n",
    "    yf = fft(data)\n",
    "    xf = fftfreq(n, 1/fs)\n",
    "    ax.plot(xf[:n//2], 2.0/n * np.abs(yf[0:n//2]))\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "\n",
    "def bandpass_filter(data, lowcut=0.5, highcut=20, fs=200, order=2):\n",
    "    low = 2 * lowcut / fs \n",
    "    high = 2 * highcut / fs\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = lfilter(b, a, data, axis=0)\n",
    "    return y\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff_freq=20, sampling_rate=200, order=4):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "def notch_filter(data, fs=200, f0=60, Q=30):\n",
    "    b, a = iirnotch(f0, Q, fs)\n",
    "    y = lfilter(b, a, data, axis=0)\n",
    "    return y\n",
    "\n",
    "def filter_eeg(eeg_seq, bandpass=(0.5, 20), lowpass=20, notch=(60, 30), fs=200):\n",
    "\n",
    "    if bandpass is not None:\n",
    "        eeg_seq = bandpass_filter(eeg_seq, lowcut=bandpass[0], highcut=bandpass[1], fs=fs, order=2)\n",
    "    if lowpass is not None:\n",
    "        eeg_seq = butter_lowpass_filter(eeg_seq, cutoff_freq=lowpass, sampling_rate=fs, order=4)\n",
    "    if notch is not None:\n",
    "        eeg_seq = notch_filter(eeg_seq, fs=fs, f0=notch[0], Q=notch[1])\n",
    "\n",
    "    return eeg_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGSeqDataset(Dataset):\n",
    "    def __init__(self, df, config, eegs, mode='train', verbose=False):\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.verbose = verbose\n",
    "        self.downsample = config.RESNET_GRU_DOWNSAMPLE\n",
    "        self.bandpass = config.RESNET_GRU_BANDPASS\n",
    "        self.lowpass = config.RESNET_GRU_LOWPASS\n",
    "        self.notch = config.RESNET_GRU_NOTCH\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X, y_prob = self.__data_generation(idx)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            X = X[::self.downsample, :]\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y_prob, dtype=torch.float32)\n",
    "    \n",
    "    def __data_generation(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Row {index}\", row[['eeg_id', 'eeg_off_min', 'target']].tolist())\n",
    "\n",
    "        X = np.zeros((10_000, 8), dtype='float32')\n",
    "        \n",
    "        # # start_sec = int((row['eeg_off_min'] + row['eeg_off_max']) // 2)\n",
    "        # eeg_seq = self.eegs[row.eeg_id]\n",
    "        # len_seq = eeg_seq.shape[0]\n",
    "        # start_at = int(row['eeg_off_min']) + (len_seq - 10_000) // 2 \n",
    "        # # !!! use randomly sampled offset !!!\n",
    "        # # start_sec = int(row['eeg_off_sample']) \n",
    "        # data = eeg_seq[start_at:start_at+10_000, :]\n",
    "        \n",
    "        data = self.eegs[row.eeg_id]\n",
    "\n",
    "        # === Feature engineering ===\n",
    "        X[:,0] = data[:,EEG_FEAT_INDEX['Fp1']] - data[:,EEG_FEAT_INDEX['T3']]\n",
    "        X[:,1] = data[:,EEG_FEAT_INDEX['T3']] - data[:,EEG_FEAT_INDEX['O1']]\n",
    "\n",
    "        X[:,2] = data[:,EEG_FEAT_INDEX['Fp1']] - data[:,EEG_FEAT_INDEX['C3']]\n",
    "        X[:,3] = data[:,EEG_FEAT_INDEX['C3']] - data[:,EEG_FEAT_INDEX['O1']]\n",
    "\n",
    "        X[:,4] = data[:,EEG_FEAT_INDEX['Fp2']] - data[:,EEG_FEAT_INDEX['C4']]\n",
    "        X[:,5] = data[:,EEG_FEAT_INDEX['C4']] - data[:,EEG_FEAT_INDEX['O2']]\n",
    "\n",
    "        X[:,6] = data[:,EEG_FEAT_INDEX['Fp2']] - data[:,EEG_FEAT_INDEX['T4']]\n",
    "        X[:,7] = data[:,EEG_FEAT_INDEX['T4']] - data[:,EEG_FEAT_INDEX['O2']]\n",
    "\n",
    "        # === Standarize ===\n",
    "        X = np.clip(X,-1024, 1024)\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        # === Butter Low-pass Filter ===\n",
    "        # ??? change to bandpass filter (low=0.5, hight=20, order=2) ???\n",
    "        X = filter_eeg(X, bandpass=self.bandpass, lowpass=self.lowpass, notch=self.notch, fs=200)\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            y_prob = row[TARGETS].values.astype(np.float32)\n",
    "        else:\n",
    "            y_prob = np.zeros(6, dtype='float32')\n",
    "\n",
    "        return X, y_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the dataset\n",
    "# train_dataset = EEGSeqDataset(train_all, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "# train_loader = DataLoader(train_dataset, drop_last=True, batch_size=16, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     X, y = batch\n",
    "#     print(f\"X shape: {X.shape}\")\n",
    "#     print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "#     fig, axes = plt.subplots(4, 1, figsize=(20, 20))\n",
    "#     ax_idx = 0\n",
    "#     for item in np.random.choice(range(X.shape[0]), 4):\n",
    "#         offset = 0\n",
    "#         for col in range(X.shape[-1]):\n",
    "#             if col != 0:\n",
    "#                 offset -= X[item,:,col].min()\n",
    "#             axes[ax_idx].plot(np.arange(X.shape[1]), X[item,:,col]+offset, label=f'feature {col+1}')\n",
    "#             offset += X[item,:,col].max()\n",
    "#         print(y[item])\n",
    "#         # axes[ax_idx].set_title(f'Weight = {weights[item]}',size=14)\n",
    "#         axes[ax_idx].legend()\n",
    "#         ax_idx += 1\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "#     break\n",
    "\n",
    "# del train_dataset, train_loader\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsampling, dropout=0.0, dilation=1):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=in_channels),\n",
    "            nn.Hardswish(), #nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            nn.Hardswish(), #nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding, dilation=dilation, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        \n",
    "        if self.downsampling is not None:\n",
    "            identity = self.downsampling(identity)\n",
    "            out = self.downsampling(out)\n",
    "            \n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling \n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        att_w = self.softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channel, channel // reduction, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channel // reduction, channel, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class ResNetGRU(nn.Module):\n",
    "    def __init__(self, config=ModelConfig, num_classes=6):\n",
    "        super(ResNetGRU, self).__init__()\n",
    "\n",
    "        self.planes = 24\n",
    "        self.kernels = config.RESNET_GRU_KERNELS\n",
    "        self.in_channels = config.RESNET_GRU_IN_CHANNELS\n",
    "        self.use_dilation = config.RESNET_GRU_DILATED\n",
    "\n",
    "        self.conv_in_size = self.planes * len(self.kernels)\n",
    "\n",
    "        fixed_kernel_size = config.RESNET_GRU_FIXED_KERNEL_SIZE\n",
    "        rnn_hidden_size = 128\n",
    "        hidden_size = self.conv_in_size + rnn_hidden_size*2 #config.RESNET_GRU_HIDDEN_SIZE\n",
    "\n",
    "        # Define the separate convolutional layers\n",
    "        self.parallel_conv = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_channels, \n",
    "                out_channels=self.planes, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=1, \n",
    "                padding=kernel_size//2, \n",
    "                bias=False\n",
    "            ) for kernel_size in self.kernels\n",
    "        ])\n",
    "\n",
    "        # Define the ResNet part of the model\n",
    "        self.resnet_part = self._make_resnet_part(fixed_kernel_size, n_blocks=9)\n",
    "        # Define the GRU part of the model\n",
    "        self.rnn = nn.GRU(input_size=self.in_channels, hidden_size=rnn_hidden_size, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # Define the pooling layers\n",
    "        self.pooling_resnet = SelfAttentionPooling(self.conv_in_size)\n",
    "        self.pooling_rnn = SelfAttentionPooling(rnn_hidden_size * 2)\n",
    "        \n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "\n",
    "    def _make_parallel_conv_layers(self):\n",
    "\n",
    "        return nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.in_channels, \n",
    "                out_channels=self.planes, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=1, \n",
    "                padding=kernel_size//2, \n",
    "                bias=False\n",
    "            ) for kernel_size in self.kernels\n",
    "        ])\n",
    "\n",
    "    def _make_resnet_part(self, fixed_kernel_size, n_blocks=9):\n",
    "        # prepare resnet layers\n",
    "        if self.use_dilation:\n",
    "            dilation_rates = [1, 2, 2, 2, 2, 4, 4, 4, 4] #[1] * n_blocks\n",
    "        else:\n",
    "            dilation_rates = [1] * n_blocks\n",
    "        paddings = [fixed_kernel_size//2 * rate for rate in dilation_rates]\n",
    "        \n",
    "        ds_layer = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        ds_layers = [ds_layer if i % 2 == 0 else None for i in range(n_blocks-1)] + [None]\n",
    "        \n",
    "        resnet_layers = [\n",
    "            ResNet_1D_Block(\n",
    "                in_channels=self.conv_in_size,\n",
    "                out_channels=self.conv_in_size, \n",
    "                kernel_size=fixed_kernel_size, \n",
    "                stride=1, \n",
    "                padding=paddings[i], \n",
    "                downsampling=ds_layers[i],\n",
    "                dropout=0.0,\n",
    "                dilation=dilation_rates[i]\n",
    "                )\n",
    "            for i in range(n_blocks)\n",
    "        ]\n",
    "        \n",
    "        # return the resnet encoder\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.conv_in_size),\n",
    "            nn.SiLU(), #nn.ReLU(inplace=False),\n",
    "            SqueezeExcitation(self.conv_in_size, reduction=4), \n",
    "            *resnet_layers,\n",
    "            nn.BatchNorm1d(num_features=self.conv_in_size),\n",
    "            nn.SiLU(), #nn.ReLU(inplace=False),\n",
    "            nn.AvgPool1d(kernel_size=5, stride=5, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        cov_sep = [conv(x) for conv in self.parallel_conv]\n",
    "        cov_out = torch.cat(cov_sep, dim=1) # Vstack -> (N, sum(C_i), L)\n",
    "        \n",
    "        # - pass through the resnet part\n",
    "        resnet_out = self.resnet_part(cov_out)\n",
    "        out_1 = self.pooling_resnet(resnet_out.permute(0, 2, 1))\n",
    "        \n",
    "        # - extract features using rnn\n",
    "        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "        out_2 = self.pooling_rnn(rnn_out)\n",
    "\n",
    "        # - concat features and pass to FC\n",
    "        feat = torch.cat([out_1, out_2], dim=1) \n",
    "        result = self.fc(feat)  \n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([4, 2000, 8])\n",
      "y shape: torch.Size([4, 6])\n",
      "torch.Size([4, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EEGSeqDataset(train_all, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "train_loader = DataLoader(train_dataset, drop_last=True, batch_size=4, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "model = ResNetGRU(config=ModelConfig, num_classes=6)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    X, y = batch\n",
    "\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    y_pred = model(X)\n",
    "    print(y_pred.shape)\n",
    "    break \n",
    "\n",
    "del model, train_dataset, train_loader, X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 00:23:27 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 26%   36C    P2    55W / 260W |   1894MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1423      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1822      G   /usr/bin/gnome-shell                4MiB |\n",
      "|    0   N/A  N/A    506842      C   ...a3/envs/kaggle/bin/python     1259MiB |\n",
      "|    0   N/A  N/A    507715      C   ...a3/envs/kaggle/bin/python      617MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, config, logger):\n",
    "\n",
    "        self.model = model\n",
    "        self.logger = logger\n",
    "        self.config = config\n",
    "        \n",
    "        self.early_stop_rounds = config.EARLY_STOP_ROUNDS\n",
    "        self.early_stop_counter = 0\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.gamma = config.REGULARIZATION\n",
    "        \n",
    "        # self.criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    \n",
    "    def criterion(self, y_pred, y_true, weights=None, mode='train'):\n",
    "        kl_loss = self.kl_div_loss(F.log_softmax(y_pred, dim=1), y_true)\n",
    "        if (self.gamma is not None) & (mode == 'train'):\n",
    "            softmax_probs = F.softmax(y_pred, dim=1)  # Compute softmax probabilities\n",
    "            entropy_loss = -(softmax_probs * torch.log(softmax_probs + 1e-9)).sum(dim=1).mean(dim=0) # Compute entropy, add epsilon to avoid log(0)\n",
    "            return kl_loss - self.gamma * entropy_loss\n",
    "        else:\n",
    "            return kl_loss\n",
    "        \n",
    "    def train(self, train_loader, valid_loader, from_checkpoint=None):\n",
    "\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=8e-3, weight_decay=self.config.WEIGHT_DECAY)\n",
    "\n",
    "        # CosineAnnealingWarmRestarts( \n",
    "        #     self.optimizer,\n",
    "        #     T_0=20,\n",
    "        #     eta_min=1e-6,\n",
    "        #     T_mult=1,\n",
    "        #     last_epoch=-1\n",
    "        # )\n",
    "        self.scheduler =  OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=1e-4,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            final_div_factor=100,\n",
    "        )\n",
    "\n",
    "        if from_checkpoint is not None:\n",
    "            self.model.load_state_dict(torch.load(from_checkpoint, map_location=self.device))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        best_weights, best_preds, best_loss = None, None, float(\"inf\")\n",
    "        loss_records = {\"train\": [], \"valid\": []}\n",
    "\n",
    "        for epoch in range(self.config.EPOCHS):\n",
    "            start_epoch = time()\n",
    "\n",
    "            train_loss, _ = self._train_or_valid_epoch(epoch, train_loader, is_train=True)\n",
    "            valid_loss, valid_preds = self._train_or_valid_epoch(epoch, valid_loader, is_train=False)\n",
    "\n",
    "            loss_records[\"train\"].append(train_loss)\n",
    "            loss_records[\"valid\"].append(valid_loss)\n",
    "\n",
    "            elapsed = time() - start_epoch\n",
    "\n",
    "            info = f\"{'-' * 100}\\nEpoch {epoch + 1} - \"\n",
    "            info += f\"Average Loss: (train) {train_loss:.4f}; (valid) {valid_loss:.4f} | Time: {elapsed:.2f}s\"\n",
    "            self.logger.info(info)\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_weights = self.model.state_dict()\n",
    "                best_preds = valid_preds\n",
    "                self.logger.info(f\"Best model found in epoch {epoch + 1} | valid loss: {best_loss:.4f}\")\n",
    "                self.early_stop_counter = 0\n",
    "            \n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "                if self.early_stop_counter >= self.early_stop_rounds:\n",
    "                    self.logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "        return best_weights, best_preds, loss_records\n",
    "\n",
    "    def _train_or_valid_epoch(self, epoch_id, dataloader, is_train=True):\n",
    "\n",
    "        self.model.train() if is_train else self.model.eval()\n",
    "        mode = \"Train\" if is_train else \"Valid\"\n",
    "\n",
    "        len_loader = len(dataloader)\n",
    "        scaler = GradScaler(enabled=self.config.AMP)\n",
    "        loss_meter, predicts_record = AverageMeter(), []\n",
    "\n",
    "        start = time()\n",
    "        pbar = tqdm(dataloader, total=len(dataloader), unit=\"batch\", desc=f\"{mode} [{epoch_id}]\")\n",
    "        for step, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "            if is_train:\n",
    "                with autocast(enabled=self.config.AMP):\n",
    "                    y_pred = self.model(X)\n",
    "                    loss = self.criterion(y_pred, y)\n",
    "                if self.config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                    loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS\n",
    "                scaler.scale(loss).backward()\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.MAX_GRAD_NORM)\n",
    "                if (step + 1) % self.config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scheduler.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = self.model(X)\n",
    "                    loss = self.criterion(y_pred, y, mode='valid')\n",
    "                if self.config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                    loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                predicts_record.append(y_pred.to('cpu').numpy())\n",
    "            \n",
    "            loss_meter.update(loss.item(), y.size(0))\n",
    "            end = time()\n",
    "\n",
    "            if (step % self.config.PRINT_FREQ == 0) or (step == (len_loader - 1)):\n",
    "                lr = self.scheduler.get_last_lr()[0]\n",
    "                info = f\"Epoch {epoch_id + 1} [{step}/{len_loader}] | {mode} Loss: {loss_meter.avg:.4f}\"\n",
    "                if is_train:\n",
    "                    info += f\" Grad: {grad_norm:.4f} LR: {lr:.4e}\"\n",
    "                info += f\" | Elapse: {end - start:.2f}s\"\n",
    "                print(info)\n",
    "\n",
    "        if not is_train:\n",
    "            predicts_record = np.concatenate(predicts_record)\n",
    "            \n",
    "        return loss_meter.avg, predicts_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(model, fold_id, train_folds, valid_folds, logger, stage=1, checkpoint=None):\n",
    "\n",
    "    train_dataset = EEGSeqDataset(train_folds, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "    valid_dataset = EEGSeqDataset(valid_folds, ModelConfig, ALL_EEG_SIGNALS, mode=\"valid\")\n",
    "\n",
    "    # ======== DATALOADERS ==========\n",
    "    loader_kwargs = {\n",
    "        \"batch_size\": ModelConfig.BATCH_SIZE,\n",
    "        \"num_workers\": ModelConfig.NUM_WORKERS,\n",
    "        \"pin_memory\": True,\n",
    "        \"shuffle\": False,\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, drop_last=True, collate_fn=None, **loader_kwargs)\n",
    "    valid_loader = DataLoader(valid_dataset, drop_last=False, collate_fn=None, **loader_kwargs)\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        print(f\"Loading model from checkpoint: {checkpoint}\")\n",
    "\n",
    "    trainer = Trainer(model, ModelConfig, logger)\n",
    "    best_weights, best_preds, loss_records = trainer.train(\n",
    "        train_loader, valid_loader, from_checkpoint=checkpoint)\n",
    "\n",
    "    save_model_name = f\"{ModelConfig.MODEL_NAME}_fold_{fold_id}_stage_{stage}.pth\"\n",
    "    torch.save(best_weights, os.path.join(PATHS.OUTPUT_DIR, save_model_name))\n",
    "\n",
    "    del train_dataset, valid_dataset, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return best_preds, loss_records\n",
    "\n",
    "def evaluate_oof(oof_df):\n",
    "    '''\n",
    "    Evaluate the out-of-fold dataframe using KL Divergence (torch and kaggle)\n",
    "    '''\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    labels = torch.tensor(oof_df[TARGETS].values.astype('float32'))\n",
    "    preds = F.log_softmax(\n",
    "        torch.tensor(oof_df[TARGETS_PRED].values.astype('float32'), requires_grad=False),\n",
    "        dim=1\n",
    "    )\n",
    "    kl_torch = kl_loss(preds, labels).item()\n",
    "\n",
    "    return kl_torch\n",
    "\n",
    "def prepare_k_fold(df, k_folds=5):\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=ModelConfig.SEED)\n",
    "    unique_spec_id = df['spectrogram_id'].unique()\n",
    "    df['fold'] = k_folds\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(unique_spec_id)):\n",
    "        df.loc[df['spectrogram_id'].isin(unique_spec_id[valid_index]), 'fold'] = fold\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kl_divergence import score as kaggle_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "TARGET2ID = {'Seizure': 0, 'LPD': 1, 'GPD': 2, 'LRDA': 3, 'GRDA': 4, 'Other': 5}\n",
    "\n",
    "def calc_kaggle_score(oof_df):\n",
    "    submission_df = oof_df[['eeg_id']+TARGETS_PRED].copy()\n",
    "    submission_df.columns = ['eeg_id'] + TARGETS\n",
    "    solution_df = oof_df[['eeg_id']+TARGETS].copy()\n",
    "    return kaggle_score(solution_df, submission_df, 'eeg_id')\n",
    "\n",
    "def analyze_oof(oof_csv):\n",
    "\n",
    "    kl_criteria = nn.KLDivLoss(reduction='batchmean')\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    oof_df = pd.read_csv(oof_csv)\n",
    "    oof_df['target_pred'] = oof_df[TARGETS_PRED].apply(lambda x: np.argmax(x), axis=1)\n",
    "    oof_df['target_id'] = oof_df[TARGETS].apply(lambda x: np.argmax(x), axis=1)\n",
    "    \n",
    "    oof_df[\"kl_loss\"] = oof_df.apply(\n",
    "    lambda row: \n",
    "        kl_criteria(\n",
    "            F.log_softmax(\n",
    "                    torch.tensor(row[TARGETS_PRED].values.astype(np.float32)).unsqueeze(0)\n",
    "                , dim=1\n",
    "                ), \n",
    "            torch.tensor(row[TARGETS].values.astype(np.float32))\n",
    "            ).numpy(),\n",
    "    axis=1)\n",
    "\n",
    "    oof_df[\"kl_loss\"] = oof_df['kl_loss'].astype(np.float32)\n",
    "\n",
    "    oof_df[TARGETS_PRED] = softmax( torch.tensor(oof_df[TARGETS_PRED].values.astype(np.float32)) )\n",
    "\n",
    "    oof_df.head()\n",
    "\n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Script Start: Sun Apr  7 00:23:40 2024\n",
      "Model Configurations:\n",
      "SEED: 20\n",
      "SPLIT_ENTROPY: 5.5\n",
      "MODEL_NAME: ResnetGRU_v3_SE\n",
      "MODEL_BACKBONE: reset_gru\n",
      "BATCH_SIZE: 32\n",
      "EPOCHS: 20\n",
      "EARLY_STOP_ROUNDS: 5\n",
      "GRADIENT_ACCUMULATION_STEPS: 1\n",
      "DROP_RATE: 0.15\n",
      "DROP_PATH_RATE: 0.25\n",
      "WEIGHT_DECAY: 0.01\n",
      "AMP: True\n",
      "PRINT_FREQ: 100\n",
      "NUM_WORKERS: 0\n",
      "MAX_GRAD_NORM: 10000000.0\n",
      "REGULARIZATION: 0.15\n",
      "RESNET_GRU_BANDPASS: (0.5, 20)\n",
      "RESNET_GRU_LOWPASS: 20\n",
      "RESNET_GRU_NOTCH: (60, 30)\n",
      "RESNET_GRU_IN_CHANNELS: 8\n",
      "RESNET_GRU_KERNELS: [3, 5, 7, 9, 11]\n",
      "RESNET_GRU_FIXED_KERNEL_SIZE: 5\n",
      "RESNET_GRU_DOWNSAMPLE: 5\n",
      "RESNET_GRU_HIDDEN_SIZE: 304\n",
      "RESNET_GRU_DILATED: False\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "Fold: 0\n",
      "====================================================================================================\n",
      "- Stage 1 | Train: 16195; Valid: 3988 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d9e3a376f3473baf91f0571c37a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [0]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/506] | Train Loss: 1.2061 Grad: 82958.7344 LR: 4.0002e-06 | Elapse: 0.70s\n",
      "Epoch 1 [100/506] | Train Loss: 1.1408 Grad: 87266.6484 LR: 6.3447e-06 | Elapse: 10.51s\n",
      "Epoch 1 [200/506] | Train Loss: 1.1346 Grad: 112337.5781 LR: 1.3062e-05 | Elapse: 20.29s\n",
      "Epoch 1 [300/506] | Train Loss: 1.1160 Grad: 69706.6172 LR: 2.3509e-05 | Elapse: 30.07s\n",
      "Epoch 1 [400/506] | Train Loss: 1.0925 Grad: 65764.5312 LR: 3.6686e-05 | Elapse: 39.87s\n",
      "Epoch 1 [500/506] | Train Loss: 1.0628 Grad: 111425.9766 LR: 5.1329e-05 | Elapse: 49.66s\n",
      "Epoch 1 [505/506] | Train Loss: 1.0612 Grad: 574752.1250 LR: 5.2075e-05 | Elapse: 50.15s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb20edf67d6f40648007b9f5eeeb2b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [0]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/125] | Valid Loss: 1.0078 | Elapse: 0.10s\n",
      "Epoch 1 [100/125] | Valid Loss: 1.1656 | Elapse: 9.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 - Average Loss: (train) 1.0612; (valid) 1.1623 | Time: 61.63s\n",
      "Best model found in epoch 1 | valid loss: 1.1623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [124/125] | Valid Loss: 1.1623 | Elapse: 11.48s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e46a90f1d548a8b3ccf3e1999f9b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [1]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [0/506] | Train Loss: 1.0038 Grad: 57585.3086 LR: 5.2224e-05 | Elapse: 0.10s\n",
      "Epoch 2 [100/506] | Train Loss: 0.8673 Grad: 158182.2500 LR: 6.6890e-05 | Elapse: 9.89s\n",
      "Epoch 2 [200/506] | Train Loss: 0.8496 Grad: 103871.8672 LR: 8.0129e-05 | Elapse: 19.64s\n",
      "Epoch 2 [300/506] | Train Loss: 0.8046 Grad: 119906.0547 LR: 9.0674e-05 | Elapse: 29.40s\n",
      "Epoch 2 [400/506] | Train Loss: 0.7721 Grad: 171081.7969 LR: 9.7515e-05 | Elapse: 39.16s\n",
      "Epoch 2 [500/506] | Train Loss: 0.7435 Grad: 159912.8750 LR: 9.9996e-05 | Elapse: 48.93s\n",
      "Epoch 2 [505/506] | Train Loss: 0.7417 Grad: 163751.1562 LR: 1.0000e-04 | Elapse: 49.42s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ddd165e467438faa497b7e08ab4117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [1]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [0/125] | Valid Loss: 0.6858 | Elapse: 0.10s\n",
      "Epoch 2 [100/125] | Valid Loss: 0.7986 | Elapse: 9.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2 - Average Loss: (train) 0.7417; (valid) 0.8000 | Time: 60.89s\n",
      "Best model found in epoch 2 | valid loss: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [124/125] | Valid Loss: 0.8000 | Elapse: 11.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a262b86304e45f4bba3c016fc7bd784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [2]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [0/506] | Train Loss: 0.6930 Grad: 191819.7656 LR: 1.0000e-04 | Elapse: 0.10s\n",
      "Epoch 3 [100/506] | Train Loss: 0.5907 Grad: 155486.7656 LR: 9.9969e-05 | Elapse: 9.89s\n",
      "Epoch 3 [200/506] | Train Loss: 0.5859 Grad: 156682.0312 LR: 9.9879e-05 | Elapse: 19.69s\n",
      "Epoch 3 [300/506] | Train Loss: 0.5744 Grad: 113158.1797 LR: 9.9729e-05 | Elapse: 29.45s\n",
      "Epoch 3 [400/506] | Train Loss: 0.5700 Grad: 180347.2031 LR: 9.9520e-05 | Elapse: 39.22s\n",
      "Epoch 3 [500/506] | Train Loss: 0.5624 Grad: 142693.6562 LR: 9.9253e-05 | Elapse: 48.99s\n",
      "Epoch 3 [505/506] | Train Loss: 0.5614 Grad: 141124.8125 LR: 9.9238e-05 | Elapse: 49.48s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1639298bab74457bb1bec7eb3c730017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [2]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [0/125] | Valid Loss: 0.6219 | Elapse: 0.10s\n",
      "Epoch 3 [100/125] | Valid Loss: 0.6937 | Elapse: 9.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3 - Average Loss: (train) 0.5614; (valid) 0.6962 | Time: 60.95s\n",
      "Best model found in epoch 3 | valid loss: 0.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [124/125] | Valid Loss: 0.6962 | Elapse: 11.47s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7397b59d6d7f48aabc5609f8b7a44d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [3]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [0/506] | Train Loss: 0.5262 Grad: 184832.0625 LR: 9.9235e-05 | Elapse: 0.10s\n",
      "Epoch 4 [100/506] | Train Loss: 0.5053 Grad: 187679.1719 LR: 9.8905e-05 | Elapse: 9.89s\n",
      "Epoch 4 [200/506] | Train Loss: 0.5037 Grad: 167793.8438 LR: 9.8517e-05 | Elapse: 19.67s\n",
      "Epoch 4 [300/506] | Train Loss: 0.5015 Grad: 111831.6328 LR: 9.8071e-05 | Elapse: 29.47s\n",
      "Epoch 4 [400/506] | Train Loss: 0.5009 Grad: 177471.8906 LR: 9.7569e-05 | Elapse: 39.23s\n",
      "Epoch 4 [500/506] | Train Loss: 0.4962 Grad: 164753.8281 LR: 9.7009e-05 | Elapse: 49.00s\n",
      "Epoch 4 [505/506] | Train Loss: 0.4954 Grad: 150137.7812 LR: 9.6980e-05 | Elapse: 49.48s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee07ec0fc234bd0922feb2955696682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [3]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [0/125] | Valid Loss: 0.5627 | Elapse: 0.10s\n",
      "Epoch 4 [100/125] | Valid Loss: 0.6642 | Elapse: 9.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4 - Average Loss: (train) 0.4954; (valid) 0.6679 | Time: 60.89s\n",
      "Best model found in epoch 4 | valid loss: 0.6679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [124/125] | Valid Loss: 0.6679 | Elapse: 11.41s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f0f10eeb514aabb51cb7510391396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [4]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [0/506] | Train Loss: 0.4717 Grad: 187825.2188 LR: 9.6974e-05 | Elapse: 0.10s\n",
      "Epoch 5 [100/506] | Train Loss: 0.4581 Grad: 229537.2188 LR: 9.6355e-05 | Elapse: 9.87s\n",
      "Epoch 5 [200/506] | Train Loss: 0.4555 Grad: 182509.7344 LR: 9.5682e-05 | Elapse: 19.66s\n",
      "Epoch 5 [300/506] | Train Loss: 0.4562 Grad: 118825.1172 LR: 9.4954e-05 | Elapse: 29.45s\n",
      "Epoch 5 [400/506] | Train Loss: 0.4566 Grad: 186943.2656 LR: 9.4172e-05 | Elapse: 39.24s\n",
      "Epoch 5 [500/506] | Train Loss: 0.4517 Grad: 205836.5156 LR: 9.3338e-05 | Elapse: 49.01s\n",
      "Epoch 5 [505/506] | Train Loss: 0.4509 Grad: 167574.8281 LR: 9.3295e-05 | Elapse: 49.50s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b75515c9e6c4892ba13d4b7629c85a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [4]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [0/125] | Valid Loss: 0.5321 | Elapse: 0.10s\n",
      "Epoch 5 [100/125] | Valid Loss: 0.6668 | Elapse: 9.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5 - Average Loss: (train) 0.4509; (valid) 0.6713 | Time: 60.91s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [124/125] | Valid Loss: 0.6713 | Elapse: 11.41s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b4ab6a2af646f88990443ba1c6f05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [5]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [0/506] | Train Loss: 0.4328 Grad: 215049.7969 LR: 9.3287e-05 | Elapse: 0.10s\n",
      "Epoch 6 [100/506] | Train Loss: 0.4131 Grad: 258486.0938 LR: 9.2398e-05 | Elapse: 9.87s\n",
      "Epoch 6 [200/506] | Train Loss: 0.4130 Grad: 212748.2812 LR: 9.1459e-05 | Elapse: 19.67s\n",
      "Epoch 6 [300/506] | Train Loss: 0.4142 Grad: 139978.6250 LR: 9.0471e-05 | Elapse: 29.47s\n",
      "Epoch 6 [400/506] | Train Loss: 0.4139 Grad: 207271.1719 LR: 8.9434e-05 | Elapse: 39.27s\n",
      "Epoch 6 [500/506] | Train Loss: 0.4095 Grad: 223868.1875 LR: 8.8351e-05 | Elapse: 49.07s\n",
      "Epoch 6 [505/506] | Train Loss: 0.4088 Grad: 166953.6719 LR: 8.8296e-05 | Elapse: 49.56s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c74f9e9cbd943dba4b138698088cd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [5]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [0/125] | Valid Loss: 0.5575 | Elapse: 0.10s\n",
      "Epoch 6 [100/125] | Valid Loss: 0.6883 | Elapse: 9.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6 - Average Loss: (train) 0.4088; (valid) 0.6948 | Time: 61.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [124/125] | Valid Loss: 0.6948 | Elapse: 11.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1900af6c26f94f83ae491322f259bc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [6]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [0/506] | Train Loss: 0.3854 Grad: 213579.3750 LR: 8.8285e-05 | Elapse: 0.10s\n",
      "Epoch 7 [100/506] | Train Loss: 0.3952 Grad: 642542.9375 LR: 8.7153e-05 | Elapse: 9.90s\n",
      "Epoch 7 [200/506] | Train Loss: 0.3832 Grad: 321296.8125 LR: 8.5977e-05 | Elapse: 19.71s\n",
      "Epoch 7 [300/506] | Train Loss: 0.3843 Grad: 218610.8438 LR: 8.4759e-05 | Elapse: 29.51s\n",
      "Epoch 7 [400/506] | Train Loss: 0.3816 Grad: 152247.1719 LR: 8.3499e-05 | Elapse: 39.31s\n",
      "Epoch 7 [500/506] | Train Loss: 0.3765 Grad: 262209.6562 LR: 8.2199e-05 | Elapse: 49.12s\n",
      "Epoch 7 [505/506] | Train Loss: 0.3764 Grad: 194942.0625 LR: 8.2133e-05 | Elapse: 49.61s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251d9fd23ffe4e5789bc5285c23718b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [6]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [0/125] | Valid Loss: 0.5261 | Elapse: 0.10s\n",
      "Epoch 7 [100/125] | Valid Loss: 0.6783 | Elapse: 9.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7 - Average Loss: (train) 0.3764; (valid) 0.6859 | Time: 61.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [124/125] | Valid Loss: 0.6859 | Elapse: 11.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0412e55f15d744dcae7cf59f002656ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [7]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [0/506] | Train Loss: 0.3590 Grad: 263377.7500 LR: 8.2120e-05 | Elapse: 0.10s\n",
      "Epoch 8 [100/506] | Train Loss: 0.3819 Grad: 815934.6875 LR: 8.0780e-05 | Elapse: 9.90s\n",
      "Epoch 8 [200/506] | Train Loss: 0.3671 Grad: 281406.5625 LR: 7.9403e-05 | Elapse: 19.70s\n",
      "Epoch 8 [300/506] | Train Loss: 0.3626 Grad: 197260.2656 LR: 7.7991e-05 | Elapse: 29.50s\n",
      "Epoch 8 [400/506] | Train Loss: 0.3617 Grad: 201836.5156 LR: 7.6546e-05 | Elapse: 39.30s\n",
      "Epoch 8 [500/506] | Train Loss: 0.3554 Grad: 367489.8438 LR: 7.5070e-05 | Elapse: 49.12s\n",
      "Epoch 8 [505/506] | Train Loss: 0.3550 Grad: 243883.9219 LR: 7.4995e-05 | Elapse: 49.61s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66bd0b7e52948de8f63bf38d8e4cc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [7]:   0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [0/125] | Valid Loss: 0.5524 | Elapse: 0.10s\n",
      "Epoch 8 [100/125] | Valid Loss: 0.6613 | Elapse: 9.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8 - Average Loss: (train) 0.3550; (valid) 0.6675 | Time: 61.07s\n",
      "Best model found in epoch 8 | valid loss: 0.6675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [124/125] | Valid Loss: 0.6675 | Elapse: 11.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290172811c4546dcb5de98a18765e9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [8]:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [0/506] | Train Loss: 0.3318 Grad: 276256.8750 LR: 7.4980e-05 | Elapse: 0.10s\n"
     ]
    }
   ],
   "source": [
    "# Major Train Loop\n",
    "# ================== Logger ==================\n",
    "logger.info(f\"{'*' * 100}\")\n",
    "logger.info(f\"Script Start: {ctime()}\")\n",
    "logger.info(f\"Model Configurations:\")\n",
    "for key, value in ModelConfig.__dict__.items():\n",
    "    if not key.startswith(\"__\"):\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "logger.info(f\"{'*' * 100}\")\n",
    "\n",
    "# ================== Prepare Training ==================\n",
    "oof_stage_1, oof_stage_2 = pd.DataFrame(), pd.DataFrame()\n",
    "loss_history_1, loss_history_2 = [], []\n",
    "t_start = time()\n",
    "\n",
    "K_FOLDS = 5\n",
    "train_all = prepare_k_fold(train_all, k_folds=K_FOLDS)\n",
    "\n",
    "for fold in range(0, K_FOLDS):\n",
    "    tik_total = time()\n",
    "    tik = time()\n",
    "\n",
    "    valid_folds = train_all[(train_all['fold'] == fold) ].reset_index(drop=True)\n",
    "    train_folds = train_all[(train_all['fold'] != fold) ].reset_index(drop=True)\n",
    "    train_size, valid_size = train_folds.shape[0], valid_folds.shape[0]\n",
    "\n",
    "    # ================== Stage 1: Train ====================\n",
    "\n",
    "    model = ResNetGRU(config=ModelConfig, num_classes=6)\n",
    "\n",
    "    ## STAGE 1\n",
    "    logger.info(f\"{'=' * 100}\\nFold: {fold}\\n{'=' * 100}\")\n",
    "    logger.info(f\"- Stage 1 | Train: {train_size}; Valid: {valid_size} -\")\n",
    "    valid_predicts, loss_records = train_fold(\n",
    "        model, fold, train_folds, valid_folds, logger, stage=1, checkpoint=None)\n",
    "\n",
    "    loss_history_1.append(loss_records)\n",
    "    valid_folds[TARGETS_PRED] = valid_predicts\n",
    "    kl_loss_torch = evaluate_oof(valid_folds)\n",
    "    info = f\"{'=' * 100}\\nFold {fold} Valid Loss: {kl_loss_torch}\\n\"\n",
    "    info += f\"Elapse: {(time() - tik) / 60:.2f} min \\n{'=' * 100}\"\n",
    "    logger.info(info)\n",
    "\n",
    "    oof_stage_1 = pd.concat([oof_stage_1, valid_folds], axis=0).reset_index(drop=True)\n",
    "    oof_stage_1.to_csv(os.path.join(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_oof_1.csv\"), index=False)\n",
    "\n",
    "    # ================== Stage 2: Train ====================\n",
    "    tik = time()\n",
    "\n",
    "    model = ResNetGRU(config=ModelConfig, num_classes=6)\n",
    "    \n",
    "    train_folds_2 = train_hard[~train_hard['eeg_id'].isin(valid_folds['eeg_id'])].reset_index(drop=True)\n",
    "    valid_folds_2 = train_hard[ train_hard['eeg_id'].isin(valid_folds['eeg_id'])].reset_index(drop=True)\n",
    "    train_size = train_folds_2.shape[0]\n",
    "    valid_size = valid_folds_2.shape[0]\n",
    "    \n",
    "    ## STAGE 2\n",
    "    logger.info(f\"- Stage 2 | Train: {train_size}; Valid: {valid_size} -\")\n",
    "\n",
    "    # model_dir = \"/home/shiyi/kaggle_hms/outputs/ResnetGRU_Originalsplit/Reg015\"\n",
    "    # checkpoint = list(Path(model_dir).glob(f\"*_fold_{fold}_stage_1.pth\"))[0]\n",
    "    checkpoint = list(Path(PATHS.OUTPUT_DIR).glob(f\"{ModelConfig.MODEL_NAME}_fold_{fold}_stage_1.pth\"))[0]\n",
    "\n",
    "    valid_predicts, loss_records = train_fold(\n",
    "        model, fold, train_folds_2, valid_folds_2, logger, stage=2, checkpoint=checkpoint)\n",
    "    \n",
    "    loss_history_2.append(loss_records)\n",
    "    valid_folds_2[TARGETS_PRED] = valid_predicts\n",
    "    kl_loss_torch = evaluate_oof(valid_folds_2)\n",
    "    info = f\"{'=' * 100}\\nFold {fold} Valid Loss: {kl_loss_torch}\\n\"\n",
    "    info += f\"Elapse: {(time() - tik) / 60:.2f} min \\n{'=' * 100}\"\n",
    "    logger.info(info)\n",
    "\n",
    "    oof_stage_2 = pd.concat([oof_stage_2, valid_folds_2], axis=0).reset_index(drop=True)\n",
    "    oof_stage_2.to_csv(os.path.join(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_oof_2.csv\"), index=False)\n",
    "\n",
    "    logger.info(f\"Fold {fold} Elapse: {(time() - tik_total) / 60:.2f} min\")\n",
    "\n",
    "info = f\"{'=' * 100}\\nTraining Complete!\\n\"\n",
    "cv_results_1 = evaluate_oof(oof_stage_1)\n",
    "cv_results_2 = evaluate_oof(oof_stage_2)\n",
    "info += f\"CV Result: Stage 1: {cv_results_1} | Stage 2: {cv_results_2}\\n\"\n",
    "info += f\"Elapse: {(time() - t_start) / 60:.2f} min \\n{'=' * 100}\"\n",
    "logger.info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharey=False)\n",
    "\n",
    "for i, loss in enumerate(loss_history_1):\n",
    "    # ax1.plot(loss['train'], marker=\"*\", ls=\"-\", label=f\"Fold {i} Train\")\n",
    "    ax1.plot(loss['valid'], marker=\"o\", ls=\":\", label=f\"Fold {i} Valid\")\n",
    "\n",
    "for i, loss in enumerate(loss_history_2):\n",
    "    # ax2.plot(loss['train'], marker=\"*\", ls=\"-\", label=f\"Fold {i} Train\")\n",
    "    ax2.plot(loss['valid'], marker=\"o\", ls=\":\", label=f\"Fold {i} Valid\")\n",
    "\n",
    "ax1.set_title(\"Stage 1 Loss\")\n",
    "ax2.set_title(\"Stage 2 Loss\")\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(Path(PATHS.OUTPUT_DIR) / f\"{ModelConfig.MODEL_NAME}_loss_history.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f'./outputs/{ModelConfig.MODEL_NAME}_oof_1.csv'\n",
    "print(\"CSV Path: \", csv_path)\n",
    "\n",
    "oof_df = analyze_oof(csv_path)\n",
    "\n",
    "print(\"Kaggle Score: \", calc_kaggle_score(oof_df))\n",
    "print(\"Average KL Loss: \", oof_df[\"kl_loss\"].mean())\n",
    "\n",
    "display(oof_df.head())\n",
    "\n",
    "pred_entropy = oof_df[TARGETS_PRED].apply(lambda x: entropy(x), axis=1)\n",
    "print(\"Average Prediction Entropy: \", pred_entropy.mean())\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(oof_df['target_id'], oof_df['target_pred']) # (y_true, y_pred)\n",
    "cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=TARGET2ID.keys(), yticklabels=TARGET2ID.keys())\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title(csv_path.split('/')[-1].split('.')[0], fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./outputs/{csv_path.split('/')[-1].split('.')[0]}_CM.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f'./outputs/{ModelConfig.MODEL_NAME}_oof_2.csv'\n",
    "print(\"CSV Path: \", csv_path)\n",
    "\n",
    "oof_df = analyze_oof(csv_path)\n",
    "\n",
    "print(\"Kaggle Score: \", calc_kaggle_score(oof_df))\n",
    "print(\"Average KL Loss: \", oof_df[\"kl_loss\"].mean())\n",
    "\n",
    "display(oof_df.head())\n",
    "\n",
    "\n",
    "pred_entropy = oof_df[TARGETS_PRED].apply(lambda x: entropy(x), axis=1)\n",
    "print(\"Average Prediction Entropy: \", pred_entropy.mean())\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(oof_df['target_id'], oof_df['target_pred']) # (y_true, y_pred)\n",
    "cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=TARGET2ID.keys(), yticklabels=TARGET2ID.keys())\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title(csv_path.split('/')[-1].split('.')[0], fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./outputs/{csv_path.split('/')[-1].split('.')[0]}_CM.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "oof_samples = oof_df.loc[0:len(oof_df):250]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    row = oof_samples.iloc[i]\n",
    "    x = np.arange(6)\n",
    "    ax.plot(x, row[TARGETS].T, marker=\"o\", ls=\"-\", label=\"True\")\n",
    "    ax.plot(x, row[TARGETS_PRED].T, marker=\"*\", ls=\"--\", label=\"Predicted\")\n",
    "    ax.set_title(f\"{row['target']} | KL Loss: {row['kl_loss']:.4f}\")\n",
    "    ax.legend()\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./outputs/{csv_path.split('/')[-1].split('.')[0]}_samples.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
