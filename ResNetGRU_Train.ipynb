{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from time import time, ctime\n",
    "\n",
    "from engine_hms_trainer import (\n",
    "    seed_everything, calc_entropy, evaluate_oof, get_logger, TARGETS, TARGETS_PRED, Trainer\n",
    ")\n",
    "from engine_hms_model import (\n",
    "    KagglePaths, LocalPaths, ModelConfig\n",
    ")\n",
    "\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import rel_entr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dir:  ./outputs/\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PATHS = KagglePaths if os.path.exists(KagglePaths.OUTPUT_DIR) else LocalPaths\n",
    "print(\"Output Dir: \", PATHS.OUTPUT_DIR)\n",
    "\n",
    "EEG_FEAT_ALL = [\n",
    "    'Fp1', 'F3', 'C3', 'P3', \n",
    "    'F7', 'T3', 'T5', 'O1', \n",
    "    'Fz', 'Cz', 'Pz', 'Fp2', \n",
    "    'F4', 'C4', 'P4', 'F8', \n",
    "    'T4', 'T6', 'O2', 'EKG'\n",
    "    ]\n",
    "\n",
    "EEG_FEAT_USE =  ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\n",
    "EEF_FEAT_INDEX = {x:y for x,y in zip(EEG_FEAT_USE, range(len(EEG_FEAT_USE)))}\n",
    "\n",
    "seed_everything(ModelConfig.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_from_parquet(parquet_path: str, use_feature=EEG_FEAT_USE, display: bool = False) -> np.ndarray:\n",
    "    # === Extract full length EEG Sequence ===\n",
    "    # fill missing values with mean\n",
    "    # first fill missing values with mean of each column\n",
    "    # then if all values are missing, fill with 0\n",
    "    eeg = pd.read_parquet(parquet_path, columns=use_feature)\n",
    "    eeg = eeg.fillna(eeg.mean(skipna=True)).fillna(0)\n",
    "    data = eeg.values.astype(np.float32)\n",
    "\n",
    "    if display:\n",
    "        fig, ax = plt.subplots(len(use_feature), 1, figsize=(15, 3*len(use_feature)), sharex=True)\n",
    "        \n",
    "        for i, feat in enumerate(use_feature):\n",
    "            ax[i].plot(data[:, i], label=feat)\n",
    "            ax[i].legend()\n",
    "            ax[i].grid()\n",
    "       \n",
    "        name = parquet_path.split('/')[-1].split('.')[0]\n",
    "        ax[0].set_title(f'EEG {name}',size=16)\n",
    "        fig.tight_layout()\n",
    "        plt.show()    \n",
    "    return data\n",
    "\n",
    "def get_non_overlap(train_csv, targets):\n",
    "\n",
    "    df = train_csv.groupby(['eeg_id'] + targets).agg({\n",
    "            'spectrogram_id': 'first',\n",
    "            'spectrogram_label_offset_seconds': ['min', 'max'],\n",
    "            'eeg_label_offset_seconds': ['min', 'max'],\n",
    "            'patient_id': 'first',\n",
    "            'expert_consensus': 'first',\n",
    "            'total_votes': 'sum',\n",
    "            'entropy': 'mean',\n",
    "            'is_hard': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "    df.columns = [\"eeg_id\"] + targets + \\\n",
    "        ['spectrogram_id', 'min', 'max', 'eeg_off_min', 'eeg_off_max', 'patient_id', 'target', 'total_votes', 'average_entropy', 'is_hard']\n",
    "\n",
    "    df[targets] = df[targets].apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_EEGS = False\n",
    "ALL_EEG_SIGNALS = {}\n",
    "eeg_paths = list(Path(PATHS.TRAIN_EEGS).glob('*.parquet'))\n",
    "preload_eegs_path = Path('./inputs/eegs_full.npy')\n",
    "\n",
    "if CREATE_EEGS:\n",
    "    count = 0\n",
    "    for parquet_path in tqdm(eeg_paths, total=len(eeg_paths)):\n",
    "        eeg_id = int(parquet_path.stem)\n",
    "        eeg_path = str(parquet_path)\n",
    "        data = eeg_from_parquet(eeg_path, display=count<1)\n",
    "        ALL_EEG_SIGNALS[eeg_id] = data\n",
    "        count += 1\n",
    "    np.save(\"./inputs/eegs_full.npy\", ALL_EEG_SIGNALS)\n",
    "else:\n",
    "    ALL_EEG_SIGNALS = np.load(preload_eegs_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:  ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "train_all:  (20183, 17)\n",
      "hard samples:  (4096, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>eeg_off_min</th>\n",
       "      <th>eeg_off_max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>target</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>average_entropy</th>\n",
       "      <th>is_hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>Other</td>\n",
       "      <td>48</td>\n",
       "      <td>4.584192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>LPD</td>\n",
       "      <td>154</td>\n",
       "      <td>4.870032</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5955</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38549</td>\n",
       "      <td>GPD</td>\n",
       "      <td>1</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40955</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1629671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2036345030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>37481</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>51</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1895581</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>128369999</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47999</td>\n",
       "      <td>Other</td>\n",
       "      <td>13</td>\n",
       "      <td>4.847483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2061593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>320962633</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23828</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2078097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2074135650</td>\n",
       "      <td>3342.0</td>\n",
       "      <td>3342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61174</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2366870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1232582129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23633</td>\n",
       "      <td>Other</td>\n",
       "      <td>18</td>\n",
       "      <td>6.134196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2482631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>978166025</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20606</td>\n",
       "      <td>Other</td>\n",
       "      <td>105</td>\n",
       "      <td>3.236383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2521897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>673742515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62117</td>\n",
       "      <td>Other</td>\n",
       "      <td>24</td>\n",
       "      <td>3.172763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2918824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1211648246</td>\n",
       "      <td>3462.0</td>\n",
       "      <td>3462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14965</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>2</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3108700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>223960986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55677</td>\n",
       "      <td>Other</td>\n",
       "      <td>3</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3625731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2091405434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6935</td>\n",
       "      <td>GRDA</td>\n",
       "      <td>1</td>\n",
       "      <td>7.802343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  \\\n",
       "0    568657      0.000000  0.000000  0.250000   0.000000   0.166667   \n",
       "1    582999      0.000000  0.857143  0.000000   0.071429   0.000000   \n",
       "2    642382      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "3    751790      0.000000  0.000000  1.000000   0.000000   0.000000   \n",
       "4    778705      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "5   1629671      1.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "6   1895581      0.076923  0.000000  0.000000   0.000000   0.076923   \n",
       "7   2061593      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "8   2078097      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "9   2366870      0.000000  0.333333  0.000000   0.000000   0.000000   \n",
       "10  2482631      0.000000  0.000000  0.133333   0.066667   0.133333   \n",
       "11  2521897      0.000000  0.000000  0.083333   0.083333   0.333333   \n",
       "12  2918824      1.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "13  3108700      0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "14  3625731      0.000000  0.000000  0.000000   0.000000   1.000000   \n",
       "\n",
       "    other_vote  spectrogram_id     min     max  eeg_off_min  eeg_off_max  \\\n",
       "0     0.583333       789577333     0.0    16.0          0.0         16.0   \n",
       "1     0.071429      1552638400     0.0    38.0          0.0         38.0   \n",
       "2     1.000000        14960202  1008.0  1032.0          0.0         24.0   \n",
       "3     0.000000       618728447   908.0   908.0          0.0          0.0   \n",
       "4     1.000000        52296320     0.0     0.0          0.0          0.0   \n",
       "5     0.000000      2036345030     0.0   160.0          0.0        160.0   \n",
       "6     0.846154       128369999  1138.0  1138.0          0.0          0.0   \n",
       "7     1.000000       320962633  1450.0  1450.0          0.0          0.0   \n",
       "8     1.000000      2074135650  3342.0  3342.0          0.0          0.0   \n",
       "9     0.666667      1232582129     0.0    30.0          0.0         30.0   \n",
       "10    0.666667       978166025  1902.0  1944.0          0.0         42.0   \n",
       "11    0.500000       673742515     0.0     4.0          0.0          4.0   \n",
       "12    0.000000      1211648246  3462.0  3462.0          0.0          0.0   \n",
       "13    1.000000       223960986     0.0     0.0          0.0          0.0   \n",
       "14    0.000000      2091405434     0.0     0.0          0.0          0.0   \n",
       "\n",
       "    patient_id   target  total_votes  average_entropy  is_hard  \n",
       "0        20654    Other           48         4.584192      1.0  \n",
       "1        20230      LPD          154         4.870032      1.0  \n",
       "2         5955    Other            2         7.802343      0.0  \n",
       "3        38549      GPD            1         7.802343      0.0  \n",
       "4        40955    Other            2         7.802343      0.0  \n",
       "5        37481  Seizure           51         7.802343      0.0  \n",
       "6        47999    Other           13         4.847483      1.0  \n",
       "7        23828    Other            1         7.802343      0.0  \n",
       "8        61174    Other            2         7.802343      0.0  \n",
       "9        23633    Other           18         6.134196      0.0  \n",
       "10       20606    Other          105         3.236383      1.0  \n",
       "11       62117    Other           24         3.172763      1.0  \n",
       "12       14965  Seizure            2         7.802343      0.0  \n",
       "13       55677    Other            3         7.802343      0.0  \n",
       "14        6935     GRDA            1         7.802343      0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(PATHS.TRAIN_CSV)\n",
    "targets = train_csv.columns[-6:].tolist()\n",
    "\n",
    "print(\"targets: \", targets)\n",
    "\n",
    "train_csv['total_votes'] = train_csv[targets].sum(axis=1)\n",
    "\n",
    "targets_prob = [f\"{t.split('_')[0]}_prob\" for t in targets]\n",
    "train_csv[targets_prob] = train_csv[targets].div(train_csv['total_votes'], axis=0)\n",
    "\n",
    "train_csv['entropy'] = train_csv[targets_prob].apply(lambda row: sum(rel_entr([1/6]*6, row.values+1e-5)), axis=1)\n",
    "train_csv['is_hard'] = (train_csv['entropy'] < 5.5).astype(int)\n",
    "\n",
    "train_all = get_non_overlap(train_csv, targets)\n",
    "train_hard = get_non_overlap(train_csv[train_csv['is_hard'] == 1].copy().reset_index(), targets)\n",
    "\n",
    "print(\"train_all: \", train_all.shape)\n",
    "print(\"hard samples: \", train_hard.shape)\n",
    "\n",
    "train_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelConfig.MODEL_BACKBONE = 'reset_gru'\n",
    "ModelConfig.MODEL_NAME = \"EEGSeq_Wavenet\"\n",
    "ModelConfig.BATCH_SIZE = 16\n",
    "ModelConfig.GRADIENT_ACCUMULATION_STEPS = 1\n",
    "ModelConfig.EPOCHS = 30\n",
    "ModelConfig.DROP_RATE = 0.1\n",
    "ModelConfig.EARLY_STOP_ROUNDS = 5\n",
    "\n",
    "ModelConfig.RESNET_GRU_HIDDEN_SIZE = 424 # 448\n",
    "ModelConfig.RESNET_GRU_INPUT_SIZE = 8\n",
    "ModelConfig.RESNET_GRU_N_BLOCKS = 9\n",
    "ModelConfig.RESNET_GRU_KERNELS = [3, 5, 7, 9] # [3, 5, 7, 9, 11]\n",
    "ModelConfig.RESNET_GRU_FIXED_KERNEL_SIZE = 5\n",
    "ModelConfig.RESNET_GRU_DROPOUT = 0.0\n",
    "ModelConfig.RESNET_GRU_DOWNSAMPLE = None # None\n",
    "\n",
    "logger = get_logger(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_train.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def denoise_filter(x):\n",
    "    # Sample rate and desired cutoff frequencies (in Hz).\n",
    "    fs = 200.0\n",
    "    lowcut = 1.0\n",
    "    highcut = 25.0\n",
    "    \n",
    "    # Filter a noisy signal.\n",
    "    T = 50\n",
    "    nsamples = T * fs\n",
    "    t = np.arange(0, nsamples) / fs\n",
    "    y = butter_bandpass_filter(x, lowcut, highcut, fs, order=6)\n",
    "    y = (y + np.roll(y,-1)+ np.roll(y,-2)+ np.roll(y,-3))/4\n",
    "    y = y[0:-1:4]\n",
    "    \n",
    "    return y\n",
    "\n",
    "def mu_law_encoding(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "def mu_law_expansion(data, mu):\n",
    "    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n",
    "    return s\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encoding(data, classes)\n",
    "    return mu_x #quantized\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff_freq=20, sampling_rate=200, order=4):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGSeqDataset(Dataset):\n",
    "    def __init__(self, df, config, eegs, mode='train', verbose=False):\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X, y_prob = self.__data_generation(idx)\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y_prob, dtype=torch.float32)\n",
    "    \n",
    "    def __data_generation(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Row {index}\", row[['eeg_id', 'eeg_off_min', 'eeg_off_max', 'target']].tolist())\n",
    "\n",
    "        X = np.zeros((10_000, 8), dtype='float32')\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            start_sec = int((row['eeg_off_min'] + row['eeg_off_max']) // 2)\n",
    "            data = self.eegs[row.eeg_id][start_sec*200 : (start_sec+50)*200]\n",
    "        else:\n",
    "            data = self.eegs[row.eeg_id]\n",
    "\n",
    "        # === Feature engineering ===\n",
    "        X[:,0] = data[:,EEF_FEAT_INDEX['Fp1']] - data[:,EEF_FEAT_INDEX['T3']]\n",
    "        X[:,1] = data[:,EEF_FEAT_INDEX['T3']] - data[:,EEF_FEAT_INDEX['O1']]\n",
    "\n",
    "        X[:,2] = data[:,EEF_FEAT_INDEX['Fp1']] - data[:,EEF_FEAT_INDEX['C3']]\n",
    "        X[:,3] = data[:,EEF_FEAT_INDEX['C3']] - data[:,EEF_FEAT_INDEX['O1']]\n",
    "\n",
    "        X[:,4] = data[:,EEF_FEAT_INDEX['Fp2']] - data[:,EEF_FEAT_INDEX['C4']]\n",
    "        X[:,5] = data[:,EEF_FEAT_INDEX['C4']] - data[:,EEF_FEAT_INDEX['O2']]\n",
    "\n",
    "        X[:,6] = data[:,EEF_FEAT_INDEX['Fp2']] - data[:,EEF_FEAT_INDEX['T4']]\n",
    "        X[:,7] = data[:,EEF_FEAT_INDEX['T4']] - data[:,EEF_FEAT_INDEX['O2']]\n",
    "\n",
    "        # === Standarize ===\n",
    "        X = np.clip(X,-1024, 1024)\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        # === Butter Low-pass Filter ===\n",
    "        # !!! change to bandpass filter (low=0.5, hight=20, order=2) !!!\n",
    "        # X = butter_lowpass_filter(X)\n",
    "        X = butter_bandpass_filter(X, .5, 20, 200, order=2)\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            y_prob = row[TARGETS].values.astype(np.float32)\n",
    "        else:\n",
    "            y_prob = np.zeros(6, dtype='float32')\n",
    "\n",
    "        return X, y_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "\n",
    "# train_dataset = EEGSeqDataset(train_all, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "# train_loader = DataLoader(train_dataset, drop_last=True, batch_size=16, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     X, y = batch\n",
    "#     print(f\"X shape: {X.shape}\")\n",
    "#     print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # fig, axes = plt.subplots(4, 1, figsize=(20, 20))\n",
    "    # for item in range(4):\n",
    "    #     offset = 0\n",
    "    #     for col in range(X.shape[-1]):\n",
    "    #         if col != 0:\n",
    "    #             offset -= X[item,:,col].min()\n",
    "    #         axes[item].plot(np.arange(X.shape[1]), X[item,:,col]+offset, label=f'feature {col+1}')\n",
    "    #         offset += X[item,:,col].max()\n",
    "    #     tt = f'{y[col][0]:0.1f}'\n",
    "    #     for t in y[col][1:]:\n",
    "    #         tt += f', {t:0.1f}'\n",
    "    #     axes[item].set_title(f'Target = {tt}',size=14)\n",
    "    #     axes[item].legend()\n",
    "    # fig.tight_layout()\n",
    "    # plt.show()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGRUEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size=128, num_layers=2, bidirectional=True):\n",
    "        super(SeqGRUEncoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Initialize the GRU to have 128 hidden units per direction and to be bidirectional\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=in_channels, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Initialize the attention mechanism\n",
    "        if bidirectional:\n",
    "            emb_dim = 2 * hidden_size\n",
    "        else:\n",
    "            emb_dim = hidden_size\n",
    "        \n",
    "        self.attention_dense = nn.Linear(emb_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, in_channels]\n",
    "        rnn_out, _ = self.rnn(x)  # -> [batch, seq_len, emb_dim]\n",
    "        identity = rnn_out\n",
    "        scores = self.attention_dense(rnn_out).squeeze(-1)\n",
    "        scores = self.softmax(scores).unsqueeze(1)\n",
    "        pooled_out = torch.matmul(scores, identity).squeeze(1)\n",
    "        # -> [batch_size, emb_dim]\n",
    "        return pooled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsampling, dropout=0.0):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsampling(x)\n",
    "        out = self.block(x)\n",
    "        out += identity\n",
    "        return out\n",
    "    \n",
    "class ResNet_1D_Encoder(nn.Module):\n",
    "    def __init__(self, kernels=[3,7,9,11], planes=24, in_channels=8, fixed_kernel_size=5, n_blocks=9, dropout=0.0):\n",
    "        super(ResNet_1D_Encoder, self).__init__()\n",
    "\n",
    "        self.parallel_conv = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=planes, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ) for kernel_size in kernels\n",
    "        ])\n",
    "\n",
    "        self.resnet_layer = ResNet_1D_Block(\n",
    "            in_channels=planes, \n",
    "            out_channels=planes, \n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=1, \n",
    "            padding=fixed_kernel_size//2, \n",
    "            downsampling=nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.resnet_part = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=planes),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(\n",
    "                in_channels=planes, \n",
    "                out_channels=planes, \n",
    "                kernel_size=fixed_kernel_size, \n",
    "                stride=2, \n",
    "                padding=2, \n",
    "                bias=False\n",
    "            ),\n",
    "            *[self.resnet_layer for _ in range(n_blocks)],\n",
    "            nn.BatchNorm1d(num_features=planes),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, in_channels]\n",
    "        print(\"Resnet Input Shape: \", x.shape)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)\n",
    "       \n",
    "        out_sep = [conv(x) for conv in self.parallel_conv]\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.resnet_part(out)\n",
    "        \n",
    "        # Return the flattened features from the last convolutional layer\n",
    "        features = out.reshape(out.shape[0], -1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class DilatedInception(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_sizes: List[int], dilation: int) -> None:\n",
    "        super().__init__()\n",
    "        assert out_channels % len(kernel_sizes) == 0, \"`out_channels` must be divisible by the number of kernel sizes.\"\n",
    "        hidden_dim = out_channels // len(kernel_sizes)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels, hidden_dim, k, padding='same', dilation=dilation)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [conv(x) for conv in self.convs]\n",
    "        out = torch.cat(outputs, dim=1)\n",
    "        return out\n",
    "\n",
    "class GatedTCN(nn.Module):\n",
    "    def __init__(self, in_dim: int, h_dim: int, kernel_sizes: List[int], dilation_factor: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.filt = DilatedInception(in_dim, h_dim, kernel_sizes, dilation=dilation_factor)\n",
    "        self.gate = DilatedInception(in_dim, h_dim, kernel_sizes, dilation=dilation_factor)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_filt = torch.tanh(self.filt(x))\n",
    "        x_gate = torch.sigmoid(self.gate(x))\n",
    "        h = x_filt * x_gate\n",
    "        h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "class WaveBlock(nn.Module):\n",
    "    def __init__(self, n_layers: int, in_dim: int, h_dim: int, kernel_sizes: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.dilation_rates = [2**i for i in range(n_layers)]\n",
    "        self.in_conv = nn.Conv1d(in_dim, h_dim, kernel_size=1)\n",
    "        self.gated_tcns = nn.ModuleList([\n",
    "            GatedTCN(h_dim, h_dim, kernel_sizes, dilation)\n",
    "            for dilation in self.dilation_rates\n",
    "        ])\n",
    "        self.skip_convs = nn.ModuleList([\n",
    "            nn.Conv1d(h_dim, h_dim, kernel_size=1)\n",
    "            for _ in range(n_layers)\n",
    "            ])\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.in_conv.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.in_conv.bias)\n",
    "        for conv in self.skip_convs:\n",
    "            nn.init.xavier_uniform_(conv.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        x = self.in_conv(x)\n",
    "        x_skip = x\n",
    "        for gated_tcn, skip_conv in zip(self.gated_tcns, self.skip_convs):\n",
    "            x = gated_tcn(x)\n",
    "            x = skip_conv(x)\n",
    "            x_skip = x_skip + x\n",
    "        return x_skip\n",
    "\n",
    "class DilatedWaveNet(nn.Module):\n",
    "    \"\"\"WaveNet architecture with dilated inception conv, enhanced with list comprehension for input processing.\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_sizes: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        \n",
    "        # Initialize wave blocks with specified kernel sizes\n",
    "        self.wave_module = nn.Sequential(\n",
    "            WaveBlock(12, 1, 16, self.kernel_sizes),\n",
    "            WaveBlock(8, 16, 32, self.kernel_sizes),\n",
    "            WaveBlock(4, 32, 64, self.kernel_sizes),\n",
    "            WaveBlock(1, 64, 64, self.kernel_sizes),\n",
    "        )\n",
    "        self.pool_layer = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # x: (B, L, C)\n",
    "        bs, seq_len, n_channels = x.shape\n",
    "        x = x.permute(0, 2, 1) # -> (B, C, L)\n",
    "        # Process different parts of the input with list comprehension\n",
    "        pooled_outputs = [\n",
    "            (\n",
    "                self.pool_layer(self.wave_module(x[:, i:i+1])) + \n",
    "                self.pool_layer(self.wave_module(x[:, i+1:i+2]))\n",
    "            ) / 2\n",
    "            for i in range(0, n_channels, 2)\n",
    "        ]\n",
    "\n",
    "        # Combine the pooled features and reshape for classification\n",
    "        features = torch.cat(pooled_outputs, dim=1).reshape(bs, -1)\n",
    "       \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGSeqClassifier(nn.Module):\n",
    "    def __init__(self, config, num_classes=6):\n",
    "        super(EEGSeqClassifier, self).__init__()\n",
    "\n",
    "        # downsample layer (by a factor of 2)\n",
    "        self.ds_layer = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # self.seqgru = SeqGRUEncoder(in_channels=8, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        # self.resnet = ResNet_1D_Encoder(\n",
    "        #     kernels=[3, 7, 9, 11], \n",
    "        #     planes=24, \n",
    "        #     in_channels=8, \n",
    "        #     fixed_kernel_size=5, \n",
    "        #     n_blocks=8, \n",
    "        #     dropout=0.0\n",
    "        # )\n",
    "        self.wavenet = DilatedWaveNet(kernel_sizes=[2, 3, 6, 7])\n",
    "\n",
    "        self.predict_head = nn.Linear(64*4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, in_channels]\n",
    "        x = self.ds_layer(x.permute(0, 2, 1)) # ->(batch, channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # resnet_out = self.resnet(x)\n",
    "        # seqgru_out = self.seqgru(x)\n",
    "        wavenet_out = self.wavenet(x)\n",
    "        \n",
    "        # features = torch.cat([seqgru_out, resnet_out, wavenet_out], dim=1)\n",
    "\n",
    "        logits = self.predict_head(wavenet_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet_1D_Block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsampling, dropout=0.0):\n",
    "#         super(ResNet_1D_Block, self).__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.BatchNorm1d(num_features=in_channels),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "#             nn.BatchNorm1d(num_features=out_channels),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         )\n",
    "#         self.downsampling = downsampling\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = self.downsampling(x)\n",
    "#         out = self.block(x)\n",
    "#         out += identity\n",
    "#         return out\n",
    "\n",
    "# class SeqPool(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(emb_dim, 1)  # Dense layer to compute attention scores\n",
    "#         self.softmax = nn.Softmax(dim=-1)   # Softmax to normalize scores\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: [batch_size, seq_len, emb_dim]\n",
    "#         bs, seq_len, emb_dim = x.shape\n",
    "#         identity = x\n",
    "#         scores = self.dense(x)\n",
    "#         scores = scores.squeeze(-1).softmax(dim=-1)\n",
    "#         scores = scores.unsqueeze(1)\n",
    "#         pooled_output = torch.matmul(scores, identity)\n",
    "#         # Final shape: [batch_size, emb_dim]\n",
    "#         pooled_output = pooled_output.squeeze(1)\n",
    "#         return pooled_output\n",
    "\n",
    "\n",
    "# class SeqGRU(nn.Module):\n",
    "#     def __init__(self, in_channels, ):\n",
    "#         super(SeqGRU, self).__init__()\n",
    "\n",
    "#         self.rnn = nn.GRU(input_size=self.in_channels, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "#         self.seq_pool = SeqPool(emb_dim=256)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # x_down = self.downsample_block(x.permute(0, 2, 1))\n",
    "#         rnn_out, _ = self.rnn(x)\n",
    "#         new_rnn_h = self.seq_pool(rnn_out)\n",
    "#         return new_rnn_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define ResNetGRU model\n",
    "# class ResNet_1D(nn.Module):\n",
    "\n",
    "#     def __init__(self, config, num_classes=6):\n",
    "#         super(ResNet_1D, self).__init__()\n",
    "\n",
    "#         self.kernels = config.RESNET_GRU_KERNELS\n",
    "#         self.planes = 24\n",
    "#         self.in_channels = config.RESNET_GRU_INPUT_SIZE\n",
    "\n",
    "#         # Define the separate convolutional layers\n",
    "#         self.parallel_conv = nn.ModuleList([\n",
    "#             nn.Conv1d(\n",
    "#                 in_channels=self.in_channels, \n",
    "#                 out_channels=self.planes, \n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=1, \n",
    "#                 padding=0, \n",
    "#                 bias=False\n",
    "#             ) for kernel_size in self.kernels\n",
    "#         ])\n",
    "\n",
    "#         fixed_kernel_size = config.RESNET_GRU_FIXED_KERNEL_SIZE\n",
    "\n",
    "#         self.resnet_layer = ResNet_1D_Block(\n",
    "#             in_channels=self.planes, \n",
    "#             out_channels=self.planes, \n",
    "#             kernel_size=fixed_kernel_size,\n",
    "#             stride=1, \n",
    "#             padding=fixed_kernel_size//2, \n",
    "#             downsampling= nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "#             dropout=config.RESNET_GRU_DROPOUT\n",
    "#             )\n",
    "        \n",
    "#         # Define the ResNet part of the model\n",
    "#         self.resnet_part = nn.Sequential(\n",
    "#             nn.BatchNorm1d(num_features=self.planes),\n",
    "#             nn.ReLU(inplace=False),\n",
    "#             nn.Conv1d(\n",
    "#                 in_channels=self.planes, \n",
    "#                 out_channels=self.planes, \n",
    "#                 kernel_size=fixed_kernel_size, \n",
    "#                 stride=2, \n",
    "#                 padding=2, \n",
    "#                 bias=False\n",
    "#             ),\n",
    "#             *[self.resnet_layer for _ in range(config.RESNET_GRU_N_BLOCKS)],\n",
    "#             nn.BatchNorm1d(num_features=self.planes),\n",
    "#             nn.ReLU(inplace=False),\n",
    "#             nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "#         )\n",
    "\n",
    "#         # Define the final fully connected layer\n",
    "#         self.fc = nn.Linear(in_features=168, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1) # (batch, channels, seq_len)\n",
    "       \n",
    "#         out_sep = [conv(x) for conv in self.parallel_conv]\n",
    "#         out = torch.cat(out_sep, dim=2)\n",
    "#         out = self.resnet_part(out)\n",
    "#         out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "#         result = self.fc(out)\n",
    "#         return result\n",
    "\n",
    "\n",
    "# class ResNetGRU(nn.Module):\n",
    "#     def __init__(self, config, num_classes=6):\n",
    "#         super(ResNetGRU, self).__init__()\n",
    "\n",
    "#         self.kernels = config.RESNET_GRU_KERNELS\n",
    "#         self.planes = 24\n",
    "#         self.in_channels = config.RESNET_GRU_INPUT_SIZE\n",
    "\n",
    "#         # self.downsample_block = nn.Sequential(\n",
    "#         #     nn.Conv1d(\n",
    "#         #         in_channels=self.in_channels, \n",
    "#         #         out_channels=self.in_channels, \n",
    "#         #         kernel_size=5, \n",
    "#         #         stride=5,\n",
    "#         #         dilation=2, \n",
    "#         #         padding=2, \n",
    "#         #     ),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.BatchNorm1d(num_features=self.in_channels)\n",
    "#         # )\n",
    "\n",
    "#         # # Define the separate convolutional layers\n",
    "#         self.parallel_conv = self._make_parallel_conv_layers()\n",
    "\n",
    "#         # Define the ResNet part of the model\n",
    "#         self.resnet_part = self._make_resnet_part(\n",
    "#             config.RESNET_GRU_FIXED_KERNEL_SIZE, config.RESNET_GRU_N_BLOCKS, config.RESNET_GRU_DROPOUT)\n",
    "        \n",
    "#         # Define the GRU part of the model\n",
    "#         self.rnn = nn.GRU(input_size=self.in_channels, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "#         self.seq_pool = SeqPool(emb_dim=256)\n",
    "\n",
    "#         # Define the final fully connected layer\n",
    "#         self.fc = nn.Linear(in_features=424, out_features=num_classes)\n",
    "#         # self.mlp = nn.Sequential(\n",
    "#         #     nn.Linear(in_features=config.RESNET_GRU_HIDDEN_SIZE, out_features=256),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(p=config.DROP_RATE),\n",
    "#         #     nn.Linear(in_features=256, out_features=num_classes)\n",
    "#         # )\n",
    "\n",
    "#     def _make_parallel_conv_layers(self):\n",
    "#         return nn.ModuleList([\n",
    "#             nn.Conv1d(\n",
    "#                 in_channels=self.in_channels, \n",
    "#                 out_channels=self.planes, \n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=1, \n",
    "#                 padding=0, \n",
    "#                 bias=False\n",
    "#             ) for kernel_size in self.kernels\n",
    "#         ])\n",
    "\n",
    "#     def _make_resnet_part(self, fixed_kernel_size, blocks=9, dropout=0.0):\n",
    "#         # prepare resnet layers\n",
    "#         downsampling = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         resnet_layers = [\n",
    "#             ResNet_1D_Block(\n",
    "#                 in_channels=self.planes, \n",
    "#                 out_channels=self.planes, \n",
    "#                 kernel_size=fixed_kernel_size, \n",
    "#                 stride=1, \n",
    "#                 padding=fixed_kernel_size//2, \n",
    "#                 downsampling=downsampling,\n",
    "#                 dropout=dropout)\n",
    "#             for _ in range(blocks)\n",
    "#         ]\n",
    "#         # return the resnet encoder\n",
    "#         return nn.Sequential(\n",
    "#             nn.BatchNorm1d(num_features=self.planes),\n",
    "#             nn.ReLU(inplace=False),\n",
    "#             nn.Conv1d(\n",
    "#                 in_channels=self.planes, \n",
    "#                 out_channels=self.planes, \n",
    "#                 kernel_size=fixed_kernel_size, \n",
    "#                 stride=2, \n",
    "#                 padding=2, \n",
    "#                 bias=False\n",
    "#             ),\n",
    "#             *resnet_layers,\n",
    "#             nn.BatchNorm1d(num_features=self.planes),\n",
    "#             nn.ReLU(inplace=False),\n",
    "#             nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = x.permute(0, 2, 1) # (batch, channels, seq_len)\n",
    "#         # x_down = self.downsample_block(x)\n",
    "\n",
    "#         # extract features using resnet \n",
    "#         out_sep = [conv(x) for conv in self.parallel_conv]\n",
    "#         out = torch.cat(out_sep, dim=2)\n",
    "#         out = self.resnet_part(out)\n",
    "#         out = out.reshape(out.shape[0], -1)\n",
    "        \n",
    "#         # extract features using rnn\n",
    "#         rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "#         new_rnn_h = x = self.seq_pool(rnn_out)\n",
    "\n",
    "#         # # concatenate the features\n",
    "#         new_out = torch.cat([out, new_rnn_h], dim=1) \n",
    "\n",
    "#         # pass through the final fully connected layer\n",
    "#         # result = self.mlp(new_out)\n",
    "#         result = self.fc(new_out)  \n",
    "        \n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EEGSeqDataset(train_all, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "train_loader = DataLoader(train_dataset, drop_last=True, batch_size=16, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "# model = SeqGRU( in_channels=8, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "# model = ResNet_1D_Encoder(in_channels=8)\n",
    "model = EEGSeqClassifier(ModelConfig, num_classes=6)\n",
    "model.to(DEVICE)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    X, y = batch\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    y_pred = model(X)\n",
    "    print(y_pred.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model, train_dataset, train_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 29 20:24:47 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 26%   33C    P8     5W / 260W |   3883MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1423      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1822      G   /usr/bin/gnome-shell                4MiB |\n",
      "|    0   N/A  N/A    415458      C   ...a3/envs/kaggle/bin/python     3865MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_id = 0\n",
    "# feat = rnn_out.detach().cpu().numpy()\n",
    "# print(feat[0].mean(axis=0).shape)\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1, figsize=(20, 5))\n",
    "# # for i in range(0, feat.shape[1], 16):\n",
    "# #     ax[0].plot(feat[:, i].detach().cpu().numpy())\n",
    "# ax[0].plot(feat[0, -1, :])\n",
    "# ax[0].plot(feat[0].mean(axis=0))\n",
    "# ax[1].plot(resnet_out[0].detach().cpu().numpy())\n",
    "\n",
    "# plt.title(f\"Feature {feature_id}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_k_fold(df, k_folds=5):\n",
    "\n",
    "    kf = KFold(n_splits=k_folds)\n",
    "    unique_spec_id = df['spectrogram_id'].unique()\n",
    "    df['fold'] = k_folds\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(unique_spec_id)):\n",
    "        df.loc[df['spectrogram_id'].isin(unique_spec_id[valid_index]), 'fold'] = fold\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_fold(model, fold_id, train_folds, valid_folds, logger, stage=1, checkpoint=None):\n",
    "\n",
    "    train_dataset = EEGSeqDataset(train_folds, ModelConfig, ALL_EEG_SIGNALS, mode=\"train\")\n",
    "    valid_dataset = EEGSeqDataset(valid_folds, ModelConfig, ALL_EEG_SIGNALS, mode=\"valid\")\n",
    "\n",
    "    # ======== DATALOADERS ==========\n",
    "    loader_kwargs = {\n",
    "        \"batch_size\": ModelConfig.BATCH_SIZE,\n",
    "        \"num_workers\": ModelConfig.NUM_WORKERS,\n",
    "        \"pin_memory\": True,\n",
    "        \"shuffle\": False,\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, drop_last=True, collate_fn=None, **loader_kwargs)\n",
    "    valid_loader = DataLoader(valid_dataset, drop_last=False, collate_fn=None, **loader_kwargs)\n",
    "\n",
    "    trainer = Trainer(model, ModelConfig, logger)\n",
    "    best_weights, best_preds, loss_records = trainer.train(\n",
    "        train_loader, valid_loader, from_checkpoint=checkpoint)\n",
    "\n",
    "    save_model_name = f\"{ModelConfig.MODEL_NAME}_fold_{fold_id}_stage_{stage}.pth\"\n",
    "    torch.save(best_weights, os.path.join(PATHS.OUTPUT_DIR, save_model_name))\n",
    "\n",
    "    del train_dataset, valid_dataset, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return best_preds, loss_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "train_all = prepare_k_fold(train_all, k_folds=k_folds)\n",
    "\n",
    "# gkf = GroupKFold(n_splits=k_folds)\n",
    "# for fold, (train_index, valid_index) in enumerate(gkf.split(train_all, train_all.target, train_all.patient_id)):\n",
    "#     train_all.loc[valid_index, \"fold\"] = int(fold)\n",
    "\n",
    "# for fold in range(k_folds):\n",
    "#     print(f\"Fold {fold} = {len(train_all[train_all['fold'] == fold])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Script Start: Fri Mar 29 20:24:53 2024\n",
      "Model Configurations:\n",
      "SEED: 20\n",
      "SPLIT_ENTROPY: 5.5\n",
      "MODEL_NAME: EEGSeq_Wavenet\n",
      "MODEL_BACKBONE: reset_gru\n",
      "BATCH_SIZE: 16\n",
      "EPOCHS: 30\n",
      "GRADIENT_ACCUMULATION_STEPS: 1\n",
      "DROP_RATE: 0.1\n",
      "DROP_PATH_RATE: 0.25\n",
      "WEIGHT_DECAY: 0.01\n",
      "REGULARIZATION: None\n",
      "USE_KAGGLE_SPECTROGRAMS: True\n",
      "USE_EEG_SPECTROGRAMS: True\n",
      "AMP: True\n",
      "AUGMENT: False\n",
      "AUGMENTATIONS: ['h_flip', 'v_flip', 'xy_masking', 'cutmix']\n",
      "PRINT_FREQ: 50\n",
      "FREEZE: False\n",
      "NUM_FROZEN_LAYERS: 0\n",
      "NUM_WORKERS: 0\n",
      "MAX_GRAD_NORM: 10000000.0\n",
      "DUAL_ENCODER_BACKBONE: tf_efficientnet_b2\n",
      "MAE_PRETRAINED_WEIGHTS: facebook/vit-mae-base\n",
      "MAE_HIDDEN_DROPOUT_PROB: 0.05\n",
      "MAE_ATTENTION_DROPOUT_PROB: 0.05\n",
      "EARLY_STOP_ROUNDS: 5\n",
      "RESNET_GRU_HIDDEN_SIZE: 424\n",
      "RESNET_GRU_INPUT_SIZE: 8\n",
      "RESNET_GRU_N_BLOCKS: 9\n",
      "RESNET_GRU_KERNELS: [3, 5, 7, 9]\n",
      "RESNET_GRU_FIXED_KERNEL_SIZE: 5\n",
      "RESNET_GRU_DROPOUT: 0.0\n",
      "RESNET_GRU_DOWNSAMPLE: None\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "Stage 1: Train ResNetGRU\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Fold: 0 || Valid: 7314; \n",
      "====================================================================================================\n",
      "- Train: 12869; Epoch = 30; Dropout = 0.0 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1b73b57fd6482c9c48c7023ddeb290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [0]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/804] | Train Loss: 1.7715 Grad: 303500.9688 LR: 4.0000e-06 | Elapse: 1.82s\n",
      "Epoch 1 [50/804] | Train Loss: 1.5406 Grad: 328130.0312 LR: 4.1059e-06 | Elapse: 35.69s\n",
      "Epoch 1 [100/804] | Train Loss: 1.5040 Grad: 256567.5469 LR: 4.4151e-06 | Elapse: 69.75s\n",
      "Epoch 1 [150/804] | Train Loss: 1.4682 Grad: 147090.1719 LR: 4.9261e-06 | Elapse: 104.11s\n",
      "Epoch 1 [200/804] | Train Loss: 1.4453 Grad: 171396.2812 LR: 5.6369e-06 | Elapse: 138.73s\n",
      "Epoch 1 [250/804] | Train Loss: 1.4209 Grad: 153045.7188 LR: 6.5444e-06 | Elapse: 173.54s\n",
      "Epoch 1 [300/804] | Train Loss: 1.4039 Grad: 202389.0938 LR: 7.6448e-06 | Elapse: 208.36s\n",
      "Epoch 1 [350/804] | Train Loss: 1.3885 Grad: 309029.9062 LR: 8.9334e-06 | Elapse: 243.28s\n",
      "Epoch 1 [400/804] | Train Loss: 1.3706 Grad: 141204.2500 LR: 1.0405e-05 | Elapse: 278.18s\n",
      "Epoch 1 [450/804] | Train Loss: 1.3591 Grad: 294451.3750 LR: 1.2053e-05 | Elapse: 312.99s\n",
      "Epoch 1 [500/804] | Train Loss: 1.3511 Grad: 180521.8281 LR: 1.3870e-05 | Elapse: 347.88s\n",
      "Epoch 1 [550/804] | Train Loss: 1.3398 Grad: 116228.2891 LR: 1.5849e-05 | Elapse: 382.69s\n",
      "Epoch 1 [600/804] | Train Loss: 1.3324 Grad: 83873.8047 LR: 1.7982e-05 | Elapse: 417.55s\n",
      "Epoch 1 [650/804] | Train Loss: 1.3279 Grad: 270563.3438 LR: 2.0258e-05 | Elapse: 452.47s\n",
      "Epoch 1 [700/804] | Train Loss: 1.3213 Grad: 188751.8594 LR: 2.2670e-05 | Elapse: 487.47s\n",
      "Epoch 1 [750/804] | Train Loss: 1.3140 Grad: 267255.9062 LR: 2.5206e-05 | Elapse: 522.36s\n",
      "Epoch 1 [800/804] | Train Loss: 1.3084 Grad: 185354.4375 LR: 2.7856e-05 | Elapse: 557.28s\n",
      "Epoch 1 [803/804] | Train Loss: 1.3082 Grad: 153888.9219 LR: 2.8018e-05 | Elapse: 559.37s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc7b7d4012f4e998bc9a5ab5d176737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [0]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/458] | Valid Loss: 1.1598 | Elapse: 0.39s\n",
      "Epoch 1 [50/458] | Valid Loss: 1.2536 | Elapse: 17.42s\n",
      "Epoch 1 [100/458] | Valid Loss: 1.2701 | Elapse: 34.33s\n",
      "Epoch 1 [150/458] | Valid Loss: 1.2602 | Elapse: 51.25s\n",
      "Epoch 1 [200/458] | Valid Loss: 1.2615 | Elapse: 68.16s\n",
      "Epoch 1 [250/458] | Valid Loss: 1.2689 | Elapse: 85.05s\n",
      "Epoch 1 [300/458] | Valid Loss: 1.2746 | Elapse: 101.96s\n",
      "Epoch 1 [350/458] | Valid Loss: 1.2785 | Elapse: 118.88s\n",
      "Epoch 1 [400/458] | Valid Loss: 1.2831 | Elapse: 135.78s\n",
      "Epoch 1 [450/458] | Valid Loss: 1.2853 | Elapse: 152.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 - Average Loss: (train) 1.3082; (valid) 1.2839 | Time: 714.23s\n",
      "Best model found in epoch 1 | valid loss: 1.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [457/458] | Valid Loss: 1.2839 | Elapse: 154.86s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5aa21792e94e9c9733c9904a9fc2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [1]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [0/804] | Train Loss: 1.3867 Grad: 161401.7031 LR: 2.8072e-05 | Elapse: 0.70s\n",
      "Epoch 2 [50/804] | Train Loss: 1.2168 Grad: 136035.8594 LR: 3.0832e-05 | Elapse: 35.46s\n",
      "Epoch 2 [100/804] | Train Loss: 1.2343 Grad: 171919.2500 LR: 3.3682e-05 | Elapse: 70.56s\n",
      "Epoch 2 [150/804] | Train Loss: 1.2439 Grad: 275422.5625 LR: 3.6609e-05 | Elapse: 105.57s\n",
      "Epoch 2 [200/804] | Train Loss: 1.2522 Grad: 156495.4375 LR: 3.9602e-05 | Elapse: 140.53s\n",
      "Epoch 2 [250/804] | Train Loss: 1.2484 Grad: 115287.6797 LR: 4.2647e-05 | Elapse: 175.54s\n",
      "Epoch 2 [300/804] | Train Loss: 1.2483 Grad: 231472.0625 LR: 4.5732e-05 | Elapse: 210.65s\n",
      "Epoch 2 [350/804] | Train Loss: 1.2476 Grad: 327308.7500 LR: 4.8844e-05 | Elapse: 245.66s\n",
      "Epoch 2 [400/804] | Train Loss: 1.2440 Grad: 122475.8203 LR: 5.1969e-05 | Elapse: 280.66s\n",
      "Epoch 2 [450/804] | Train Loss: 1.2407 Grad: 271792.8125 LR: 5.5094e-05 | Elapse: 315.80s\n",
      "Epoch 2 [500/804] | Train Loss: 1.2407 Grad: 165766.0312 LR: 5.8206e-05 | Elapse: 350.80s\n",
      "Epoch 2 [550/804] | Train Loss: 1.2353 Grad: 139310.1406 LR: 6.1291e-05 | Elapse: 385.85s\n",
      "Epoch 2 [600/804] | Train Loss: 1.2336 Grad: 124816.5000 LR: 6.4338e-05 | Elapse: 420.91s\n",
      "Epoch 2 [650/804] | Train Loss: 1.2342 Grad: 280992.4062 LR: 6.7332e-05 | Elapse: 456.01s\n",
      "Epoch 2 [700/804] | Train Loss: 1.2307 Grad: 151342.1094 LR: 7.0260e-05 | Elapse: 491.08s\n",
      "Epoch 2 [750/804] | Train Loss: 1.2260 Grad: 246598.0000 LR: 7.3112e-05 | Elapse: 526.16s\n",
      "Epoch 2 [800/804] | Train Loss: 1.2219 Grad: 157599.6875 LR: 7.5874e-05 | Elapse: 561.34s\n",
      "Epoch 2 [803/804] | Train Loss: 1.2219 Grad: 177058.3281 LR: 7.6036e-05 | Elapse: 563.45s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6081ecf01c614e8ab97a40135faea2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [1]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [0/458] | Valid Loss: 1.0522 | Elapse: 0.34s\n",
      "Epoch 2 [50/458] | Valid Loss: 1.1795 | Elapse: 17.47s\n",
      "Epoch 2 [100/458] | Valid Loss: 1.2176 | Elapse: 34.56s\n",
      "Epoch 2 [150/458] | Valid Loss: 1.1984 | Elapse: 51.60s\n",
      "Epoch 2 [200/458] | Valid Loss: 1.2015 | Elapse: 68.61s\n",
      "Epoch 2 [250/458] | Valid Loss: 1.2078 | Elapse: 85.60s\n",
      "Epoch 2 [300/458] | Valid Loss: 1.2133 | Elapse: 102.55s\n",
      "Epoch 2 [350/458] | Valid Loss: 1.2171 | Elapse: 119.41s\n",
      "Epoch 2 [400/458] | Valid Loss: 1.2229 | Elapse: 136.28s\n",
      "Epoch 2 [450/458] | Valid Loss: 1.2240 | Elapse: 153.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2 - Average Loss: (train) 1.2219; (valid) 1.2230 | Time: 718.67s\n",
      "Best model found in epoch 2 | valid loss: 1.2230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [457/458] | Valid Loss: 1.2230 | Elapse: 155.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce93eb4387664dc8ab3a170775fd9e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [2]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [0/804] | Train Loss: 1.4614 Grad: 353677.8750 LR: 7.6090e-05 | Elapse: 0.70s\n",
      "Epoch 3 [50/804] | Train Loss: 1.1528 Grad: 145297.1250 LR: 7.8742e-05 | Elapse: 35.65s\n",
      "Epoch 3 [100/804] | Train Loss: 1.1581 Grad: 201286.7188 LR: 8.1280e-05 | Elapse: 70.60s\n",
      "Epoch 3 [150/804] | Train Loss: 1.1629 Grad: 301907.1562 LR: 8.3695e-05 | Elapse: 105.61s\n",
      "Epoch 3 [200/804] | Train Loss: 1.1714 Grad: 177117.9375 LR: 8.5974e-05 | Elapse: 140.61s\n",
      "Epoch 3 [250/804] | Train Loss: 1.1611 Grad: 260611.7344 LR: 8.8110e-05 | Elapse: 175.68s\n",
      "Epoch 3 [300/804] | Train Loss: 1.1531 Grad: 172462.4375 LR: 9.0092e-05 | Elapse: 210.71s\n",
      "Epoch 3 [350/804] | Train Loss: 1.1508 Grad: 346195.4375 LR: 9.1913e-05 | Elapse: 245.75s\n",
      "Epoch 3 [400/804] | Train Loss: 1.1480 Grad: 79137.2500 LR: 9.3564e-05 | Elapse: 280.75s\n",
      "Epoch 3 [450/804] | Train Loss: 1.1408 Grad: 165429.7969 LR: 9.5039e-05 | Elapse: 315.81s\n",
      "Epoch 3 [500/804] | Train Loss: 1.1370 Grad: 112914.0312 LR: 9.6331e-05 | Elapse: 350.90s\n",
      "Epoch 3 [550/804] | Train Loss: 1.1286 Grad: 55155.4844 LR: 9.7435e-05 | Elapse: 385.86s\n",
      "Epoch 3 [600/804] | Train Loss: 1.1251 Grad: 63494.1602 LR: 9.8347e-05 | Elapse: 420.84s\n",
      "Epoch 3 [650/804] | Train Loss: 1.1225 Grad: 110487.7109 LR: 9.9062e-05 | Elapse: 455.92s\n",
      "Epoch 3 [700/804] | Train Loss: 1.1167 Grad: 90263.0312 LR: 9.9577e-05 | Elapse: 491.00s\n",
      "Epoch 3 [750/804] | Train Loss: 1.1128 Grad: 123775.3203 LR: 9.9890e-05 | Elapse: 526.03s\n",
      "Epoch 3 [800/804] | Train Loss: 1.1085 Grad: 121356.3125 LR: 1.0000e-04 | Elapse: 561.02s\n",
      "Epoch 3 [803/804] | Train Loss: 1.1090 Grad: 127343.9688 LR: 1.0000e-04 | Elapse: 563.12s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b574f8ae7744b7d8ab834e608a4abfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [2]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [0/458] | Valid Loss: 0.9605 | Elapse: 0.34s\n",
      "Epoch 3 [50/458] | Valid Loss: 1.0547 | Elapse: 17.38s\n",
      "Epoch 3 [100/458] | Valid Loss: 1.0974 | Elapse: 34.36s\n",
      "Epoch 3 [150/458] | Valid Loss: 1.0728 | Elapse: 51.31s\n",
      "Epoch 3 [200/458] | Valid Loss: 1.0735 | Elapse: 68.24s\n",
      "Epoch 3 [250/458] | Valid Loss: 1.0825 | Elapse: 85.07s\n",
      "Epoch 3 [300/458] | Valid Loss: 1.0933 | Elapse: 101.92s\n",
      "Epoch 3 [350/458] | Valid Loss: 1.0990 | Elapse: 118.75s\n",
      "Epoch 3 [400/458] | Valid Loss: 1.1079 | Elapse: 135.59s\n",
      "Epoch 3 [450/458] | Valid Loss: 1.1101 | Elapse: 152.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3 - Average Loss: (train) 1.1090; (valid) 1.1087 | Time: 717.67s\n",
      "Best model found in epoch 3 | valid loss: 1.1087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [457/458] | Valid Loss: 1.1087 | Elapse: 154.55s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ab74ad2050454eb076645ba40f0f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [3]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [0/804] | Train Loss: 1.2546 Grad: 317557.9688 LR: 1.0000e-04 | Elapse: 0.70s\n",
      "Epoch 4 [50/804] | Train Loss: 1.0605 Grad: 137846.7188 LR: 9.9999e-05 | Elapse: 35.66s\n",
      "Epoch 4 [100/804] | Train Loss: 1.0628 Grad: 227075.2812 LR: 9.9995e-05 | Elapse: 70.73s\n",
      "Epoch 4 [150/804] | Train Loss: 1.0651 Grad: 386655.9062 LR: 9.9988e-05 | Elapse: 105.79s\n",
      "Epoch 4 [200/804] | Train Loss: 1.0758 Grad: 192606.3906 LR: 9.9979e-05 | Elapse: 140.86s\n",
      "Epoch 4 [250/804] | Train Loss: 1.0642 Grad: 293458.1562 LR: 9.9967e-05 | Elapse: 175.89s\n",
      "Epoch 4 [300/804] | Train Loss: 1.0590 Grad: 194768.1250 LR: 9.9952e-05 | Elapse: 210.98s\n",
      "Epoch 4 [350/804] | Train Loss: 1.0603 Grad: 307020.3125 LR: 9.9935e-05 | Elapse: 246.09s\n",
      "Epoch 4 [400/804] | Train Loss: 1.0615 Grad: 196356.0938 LR: 9.9915e-05 | Elapse: 281.08s\n",
      "Epoch 4 [450/804] | Train Loss: 1.0565 Grad: 292378.8438 LR: 9.9893e-05 | Elapse: 316.12s\n",
      "Epoch 4 [500/804] | Train Loss: 1.0556 Grad: 266725.1562 LR: 9.9868e-05 | Elapse: 351.23s\n",
      "Epoch 4 [550/804] | Train Loss: 1.0493 Grad: 150889.9531 LR: 9.9841e-05 | Elapse: 386.32s\n",
      "Epoch 4 [600/804] | Train Loss: 1.0497 Grad: 128437.2344 LR: 9.9810e-05 | Elapse: 421.31s\n",
      "Epoch 4 [650/804] | Train Loss: 1.0494 Grad: 252280.1719 LR: 9.9778e-05 | Elapse: 456.23s\n",
      "Epoch 4 [700/804] | Train Loss: 1.0451 Grad: 160919.2812 LR: 9.9742e-05 | Elapse: 491.23s\n",
      "Epoch 4 [750/804] | Train Loss: 1.0430 Grad: 245524.2344 LR: 9.9704e-05 | Elapse: 526.17s\n",
      "Epoch 4 [800/804] | Train Loss: 1.0408 Grad: 215011.6875 LR: 9.9664e-05 | Elapse: 561.19s\n",
      "Epoch 4 [803/804] | Train Loss: 1.0413 Grad: 222943.7188 LR: 9.9661e-05 | Elapse: 563.29s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8dc7d4e7ce4dee94bdcc12303acea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [3]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [0/458] | Valid Loss: 0.9019 | Elapse: 0.35s\n",
      "Epoch 4 [50/458] | Valid Loss: 1.0033 | Elapse: 17.43s\n",
      "Epoch 4 [100/458] | Valid Loss: 1.0440 | Elapse: 34.47s\n",
      "Epoch 4 [150/458] | Valid Loss: 1.0227 | Elapse: 51.48s\n",
      "Epoch 4 [200/458] | Valid Loss: 1.0231 | Elapse: 68.52s\n",
      "Epoch 4 [250/458] | Valid Loss: 1.0306 | Elapse: 85.48s\n",
      "Epoch 4 [300/458] | Valid Loss: 1.0415 | Elapse: 102.36s\n",
      "Epoch 4 [350/458] | Valid Loss: 1.0458 | Elapse: 119.26s\n",
      "Epoch 4 [400/458] | Valid Loss: 1.0538 | Elapse: 136.09s\n",
      "Epoch 4 [450/458] | Valid Loss: 1.0556 | Elapse: 153.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4 - Average Loss: (train) 1.0413; (valid) 1.0542 | Time: 718.45s\n",
      "Best model found in epoch 4 | valid loss: 1.0542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [457/458] | Valid Loss: 1.0542 | Elapse: 155.15s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fa1d106f234672a97f9936b18fd199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [4]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [0/804] | Train Loss: 1.1972 Grad: 308679.1250 LR: 9.9660e-05 | Elapse: 0.70s\n",
      "Epoch 5 [50/804] | Train Loss: 1.0164 Grad: 141183.1719 LR: 9.9617e-05 | Elapse: 35.64s\n",
      "Epoch 5 [100/804] | Train Loss: 1.0198 Grad: 203328.6094 LR: 9.9571e-05 | Elapse: 70.75s\n",
      "Epoch 5 [150/804] | Train Loss: 1.0220 Grad: inf LR: 9.9522e-05 | Elapse: 105.75s\n",
      "Epoch 5 [200/804] | Train Loss: 1.0327 Grad: 97592.4453 LR: 9.9471e-05 | Elapse: 140.80s\n",
      "Epoch 5 [250/804] | Train Loss: 1.0213 Grad: 139450.3906 LR: 9.9417e-05 | Elapse: 175.84s\n",
      "Epoch 5 [300/804] | Train Loss: 1.0167 Grad: 113856.9062 LR: 9.9361e-05 | Elapse: 210.98s\n",
      "Epoch 5 [350/804] | Train Loss: 1.0172 Grad: 148841.7656 LR: 9.9302e-05 | Elapse: 246.08s\n",
      "Epoch 5 [400/804] | Train Loss: 1.0188 Grad: 98241.4297 LR: 9.9241e-05 | Elapse: 281.14s\n",
      "Epoch 5 [450/804] | Train Loss: 1.0136 Grad: 146139.3438 LR: 9.9177e-05 | Elapse: 316.21s\n",
      "Epoch 5 [500/804] | Train Loss: 1.0115 Grad: 132025.7188 LR: 9.9110e-05 | Elapse: 351.23s\n",
      "Epoch 5 [550/804] | Train Loss: 1.0049 Grad: 82229.9609 LR: 9.9041e-05 | Elapse: 386.34s\n",
      "Epoch 5 [600/804] | Train Loss: 1.0056 Grad: 64302.5078 LR: 9.8969e-05 | Elapse: 421.52s\n",
      "Epoch 5 [650/804] | Train Loss: 1.0042 Grad: 166584.8438 LR: 9.8895e-05 | Elapse: 456.66s\n",
      "Epoch 5 [700/804] | Train Loss: 0.9988 Grad: 81087.4297 LR: 9.8818e-05 | Elapse: 491.71s\n",
      "Epoch 5 [750/804] | Train Loss: 0.9953 Grad: 126109.3047 LR: 9.8738e-05 | Elapse: 526.72s\n",
      "Epoch 5 [800/804] | Train Loss: 0.9924 Grad: 113422.4844 LR: 9.8656e-05 | Elapse: 561.89s\n",
      "Epoch 5 [803/804] | Train Loss: 0.9928 Grad: 125761.2344 LR: 9.8651e-05 | Elapse: 563.99s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86847481e414f2fad81bd26c0e56d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [4]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [0/458] | Valid Loss: 0.8542 | Elapse: 0.35s\n",
      "Epoch 5 [50/458] | Valid Loss: 0.9396 | Elapse: 17.40s\n",
      "Epoch 5 [100/458] | Valid Loss: 0.9752 | Elapse: 34.43s\n",
      "Epoch 5 [150/458] | Valid Loss: 0.9597 | Elapse: 51.53s\n",
      "Epoch 5 [200/458] | Valid Loss: 0.9613 | Elapse: 68.74s\n",
      "Epoch 5 [250/458] | Valid Loss: 0.9681 | Elapse: 85.76s\n",
      "Epoch 5 [300/458] | Valid Loss: 0.9774 | Elapse: 102.72s\n",
      "Epoch 5 [350/458] | Valid Loss: 0.9818 | Elapse: 119.70s\n",
      "Epoch 5 [400/458] | Valid Loss: 0.9895 | Elapse: 136.69s\n",
      "Epoch 5 [450/458] | Valid Loss: 0.9903 | Elapse: 153.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5 - Average Loss: (train) 0.9928; (valid) 0.9892 | Time: 719.80s\n",
      "Best model found in epoch 5 | valid loss: 0.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [457/458] | Valid Loss: 0.9892 | Elapse: 155.81s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ba0fe4add2437b9e9071af5e300ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [5]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [0/804] | Train Loss: 1.0435 Grad: 301736.3438 LR: 9.8649e-05 | Elapse: 0.70s\n",
      "Epoch 6 [50/804] | Train Loss: 0.9522 Grad: 166298.3125 LR: 9.8565e-05 | Elapse: 35.65s\n",
      "Epoch 6 [100/804] | Train Loss: 0.9473 Grad: 189159.4844 LR: 9.8477e-05 | Elapse: 70.72s\n",
      "Epoch 6 [150/804] | Train Loss: 0.9449 Grad: 421366.5000 LR: 9.8387e-05 | Elapse: 105.90s\n",
      "Epoch 6 [200/804] | Train Loss: 0.9531 Grad: 89069.3828 LR: 9.8295e-05 | Elapse: 140.99s\n",
      "Epoch 6 [250/804] | Train Loss: 0.9427 Grad: 141078.5469 LR: 9.8200e-05 | Elapse: 176.07s\n",
      "Epoch 6 [300/804] | Train Loss: 0.9391 Grad: 122098.2188 LR: 9.8103e-05 | Elapse: 211.15s\n",
      "Epoch 6 [350/804] | Train Loss: 0.9365 Grad: 161878.8438 LR: 9.8003e-05 | Elapse: 246.32s\n",
      "Epoch 6 [400/804] | Train Loss: 0.9372 Grad: 130686.1094 LR: 9.7900e-05 | Elapse: 281.46s\n",
      "Epoch 6 [450/804] | Train Loss: 0.9331 Grad: 156342.3750 LR: 9.7795e-05 | Elapse: 316.55s\n",
      "Epoch 6 [500/804] | Train Loss: 0.9306 Grad: 151237.3906 LR: 9.7688e-05 | Elapse: 351.69s\n",
      "Epoch 6 [550/804] | Train Loss: 0.9247 Grad: 97548.0859 LR: 9.7578e-05 | Elapse: 386.76s\n",
      "Epoch 6 [600/804] | Train Loss: 0.9267 Grad: 82833.1484 LR: 9.7465e-05 | Elapse: 421.93s\n",
      "Epoch 6 [650/804] | Train Loss: 0.9260 Grad: 226590.9844 LR: 9.7350e-05 | Elapse: 457.16s\n",
      "Epoch 6 [700/804] | Train Loss: 0.9212 Grad: 95911.4609 LR: 9.7233e-05 | Elapse: 492.27s\n",
      "Epoch 6 [750/804] | Train Loss: 0.9186 Grad: 132248.7656 LR: 9.7113e-05 | Elapse: 527.41s\n",
      "Epoch 6 [800/804] | Train Loss: 0.9159 Grad: 143796.6719 LR: 9.6991e-05 | Elapse: 562.65s\n",
      "Epoch 6 [803/804] | Train Loss: 0.9165 Grad: 143577.5625 LR: 9.6983e-05 | Elapse: 564.76s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86a0f850e1940159403b20b2ea2a95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [5]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [0/458] | Valid Loss: 0.7955 | Elapse: 0.35s\n",
      "Epoch 6 [50/458] | Valid Loss: 0.8571 | Elapse: 17.48s\n",
      "Epoch 6 [100/458] | Valid Loss: 0.8833 | Elapse: 34.55s\n",
      "Epoch 6 [150/458] | Valid Loss: 0.8737 | Elapse: 51.58s\n",
      "Epoch 6 [200/458] | Valid Loss: 0.8781 | Elapse: 68.61s\n",
      "Epoch 6 [250/458] | Valid Loss: 0.8857 | Elapse: 85.55s\n",
      "Epoch 6 [300/458] | Valid Loss: 0.8949 | Elapse: 102.47s\n",
      "Epoch 6 [350/458] | Valid Loss: 0.8995 | Elapse: 119.40s\n",
      "Epoch 6 [400/458] | Valid Loss: 0.9068 | Elapse: 136.29s\n",
      "Epoch 6 [450/458] | Valid Loss: 0.9072 | Elapse: 153.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6 - Average Loss: (train) 0.9165; (valid) 0.9069 | Time: 720.02s\n",
      "Best model found in epoch 6 | valid loss: 0.9069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [457/458] | Valid Loss: 0.9069 | Elapse: 155.26s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75c61a217aa4031a86684bb3845b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [6]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [0/804] | Train Loss: 0.8882 Grad: 261918.6250 LR: 9.6981e-05 | Elapse: 0.70s\n",
      "Epoch 7 [50/804] | Train Loss: 0.8824 Grad: 86952.8281 LR: 9.6856e-05 | Elapse: 35.61s\n",
      "Epoch 7 [100/804] | Train Loss: 0.8749 Grad: 105126.9453 LR: 9.6728e-05 | Elapse: 70.59s\n",
      "Epoch 7 [150/804] | Train Loss: 0.8756 Grad: 210978.6406 LR: 9.6598e-05 | Elapse: 105.69s\n",
      "Epoch 7 [200/804] | Train Loss: 0.8858 Grad: 121840.3906 LR: 9.6466e-05 | Elapse: 140.79s\n",
      "Epoch 7 [250/804] | Train Loss: 0.8799 Grad: 160530.4375 LR: 9.6331e-05 | Elapse: 175.95s\n",
      "Epoch 7 [300/804] | Train Loss: 0.8777 Grad: 117072.3984 LR: 9.6194e-05 | Elapse: 211.09s\n",
      "Epoch 7 [350/804] | Train Loss: 0.8763 Grad: 166639.3750 LR: 9.6054e-05 | Elapse: 246.13s\n",
      "Epoch 7 [400/804] | Train Loss: 0.8778 Grad: 135264.3281 LR: 9.5912e-05 | Elapse: 281.20s\n",
      "Epoch 7 [450/804] | Train Loss: 0.8756 Grad: 167228.3750 LR: 9.5768e-05 | Elapse: 316.23s\n",
      "Epoch 7 [500/804] | Train Loss: 0.8745 Grad: 176101.7812 LR: 9.5621e-05 | Elapse: 351.34s\n",
      "Epoch 7 [550/804] | Train Loss: 0.8703 Grad: 102888.6484 LR: 9.5472e-05 | Elapse: 386.39s\n",
      "Epoch 7 [600/804] | Train Loss: 0.8738 Grad: 96381.3750 LR: 9.5320e-05 | Elapse: 421.51s\n",
      "Epoch 7 [650/804] | Train Loss: 0.8745 Grad: 242294.4531 LR: 9.5166e-05 | Elapse: 456.57s\n",
      "Epoch 7 [700/804] | Train Loss: 0.8710 Grad: 106208.5625 LR: 9.5010e-05 | Elapse: 491.60s\n",
      "Epoch 7 [750/804] | Train Loss: 0.8697 Grad: 147021.4844 LR: 9.4851e-05 | Elapse: 526.83s\n",
      "Epoch 7 [800/804] | Train Loss: 0.8678 Grad: 152333.4375 LR: 9.4690e-05 | Elapse: 561.98s\n",
      "Epoch 7 [803/804] | Train Loss: 0.8684 Grad: 156195.3438 LR: 9.4681e-05 | Elapse: 564.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2c87d2ae414c8f9bdcd6fa553907b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [6]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [0/458] | Valid Loss: 0.7680 | Elapse: 0.35s\n",
      "Epoch 7 [50/458] | Valid Loss: 0.8128 | Elapse: 17.42s\n",
      "Epoch 7 [100/458] | Valid Loss: 0.8388 | Elapse: 34.47s\n",
      "Epoch 7 [150/458] | Valid Loss: 0.8318 | Elapse: 51.61s\n",
      "Epoch 7 [200/458] | Valid Loss: 0.8375 | Elapse: 68.68s\n",
      "Epoch 7 [250/458] | Valid Loss: 0.8446 | Elapse: 85.66s\n",
      "Epoch 7 [300/458] | Valid Loss: 0.8532 | Elapse: 102.64s\n",
      "Epoch 7 [350/458] | Valid Loss: 0.8577 | Elapse: 119.67s\n",
      "Epoch 7 [400/458] | Valid Loss: 0.8637 | Elapse: 136.66s\n",
      "Epoch 7 [450/458] | Valid Loss: 0.8640 | Elapse: 153.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7 - Average Loss: (train) 0.8684; (valid) 0.8641 | Time: 719.90s\n",
      "Best model found in epoch 7 | valid loss: 0.8641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [457/458] | Valid Loss: 0.8641 | Elapse: 155.81s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd53f5e45cf43e3b2e4d5d940129d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [7]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [0/804] | Train Loss: 0.8415 Grad: 267947.9688 LR: 9.4677e-05 | Elapse: 0.70s\n",
      "Epoch 8 [50/804] | Train Loss: 0.8392 Grad: 196777.0781 LR: 9.4514e-05 | Elapse: 35.75s\n",
      "Epoch 8 [100/804] | Train Loss: 0.8390 Grad: 215992.1406 LR: 9.4348e-05 | Elapse: 70.81s\n",
      "Epoch 8 [150/804] | Train Loss: 0.8402 Grad: 424153.4688 LR: 9.4180e-05 | Elapse: 106.02s\n",
      "Epoch 8 [200/804] | Train Loss: 0.8499 Grad: 132975.9531 LR: 9.4009e-05 | Elapse: 141.16s\n",
      "Epoch 8 [250/804] | Train Loss: 0.8466 Grad: 172033.1562 LR: 9.3836e-05 | Elapse: 176.32s\n",
      "Epoch 8 [300/804] | Train Loss: 0.8452 Grad: 117604.7656 LR: 9.3661e-05 | Elapse: 211.66s\n",
      "Epoch 8 [350/804] | Train Loss: 0.8441 Grad: 162244.8906 LR: 9.3484e-05 | Elapse: 246.88s\n",
      "Epoch 8 [400/804] | Train Loss: 0.8453 Grad: 138191.5938 LR: 9.3304e-05 | Elapse: 281.91s\n",
      "Epoch 8 [450/804] | Train Loss: 0.8432 Grad: 174590.5938 LR: 9.3122e-05 | Elapse: 317.01s\n",
      "Epoch 8 [500/804] | Train Loss: 0.8427 Grad: 192659.7188 LR: 9.2938e-05 | Elapse: 352.12s\n",
      "Epoch 8 [550/804] | Train Loss: 0.8393 Grad: 114568.2656 LR: 9.2751e-05 | Elapse: 387.37s\n",
      "Epoch 8 [600/804] | Train Loss: 0.8430 Grad: 102797.0938 LR: 9.2563e-05 | Elapse: 422.68s\n",
      "Epoch 8 [650/804] | Train Loss: 0.8440 Grad: 240253.3438 LR: 9.2372e-05 | Elapse: 457.78s\n",
      "Epoch 8 [700/804] | Train Loss: 0.8411 Grad: 110059.8516 LR: 9.2179e-05 | Elapse: 493.06s\n",
      "Epoch 8 [750/804] | Train Loss: 0.8404 Grad: 152916.8281 LR: 9.1983e-05 | Elapse: 528.17s\n",
      "Epoch 8 [800/804] | Train Loss: 0.8389 Grad: 149454.5312 LR: 9.1786e-05 | Elapse: 563.28s\n",
      "Epoch 8 [803/804] | Train Loss: 0.8396 Grad: 163386.3438 LR: 9.1774e-05 | Elapse: 565.39s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f8814744204109bc42e8f370212d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [7]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [0/458] | Valid Loss: 0.7468 | Elapse: 0.34s\n",
      "Epoch 8 [50/458] | Valid Loss: 0.7812 | Elapse: 17.43s\n",
      "Epoch 8 [100/458] | Valid Loss: 0.8078 | Elapse: 34.55s\n",
      "Epoch 8 [150/458] | Valid Loss: 0.8020 | Elapse: 51.69s\n",
      "Epoch 8 [200/458] | Valid Loss: 0.8079 | Elapse: 68.76s\n",
      "Epoch 8 [250/458] | Valid Loss: 0.8138 | Elapse: 85.74s\n",
      "Epoch 8 [300/458] | Valid Loss: 0.8216 | Elapse: 102.70s\n",
      "Epoch 8 [350/458] | Valid Loss: 0.8252 | Elapse: 119.62s\n",
      "Epoch 8 [400/458] | Valid Loss: 0.8299 | Elapse: 136.52s\n",
      "Epoch 8 [450/458] | Valid Loss: 0.8300 | Elapse: 153.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8 - Average Loss: (train) 0.8396; (valid) 0.8303 | Time: 720.92s\n",
      "Best model found in epoch 8 | valid loss: 0.8303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [457/458] | Valid Loss: 0.8303 | Elapse: 155.52s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79913b61d4f45d3bea8d105b78962b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [8]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [0/804] | Train Loss: 0.7991 Grad: 274087.9062 LR: 9.1770e-05 | Elapse: 0.70s\n",
      "Epoch 9 [50/804] | Train Loss: 0.8117 Grad: 217494.0781 LR: 9.1570e-05 | Elapse: 35.58s\n",
      "Epoch 9 [100/804] | Train Loss: 0.8161 Grad: 223709.0469 LR: 9.1368e-05 | Elapse: 70.53s\n",
      "Epoch 9 [150/804] | Train Loss: 0.8167 Grad: 425612.4688 LR: 9.1163e-05 | Elapse: 105.51s\n",
      "Epoch 9 [200/804] | Train Loss: 0.8255 Grad: 143255.0156 LR: 9.0957e-05 | Elapse: 140.50s\n",
      "Epoch 9 [250/804] | Train Loss: 0.8237 Grad: 183188.2031 LR: 9.0749e-05 | Elapse: 175.46s\n",
      "Epoch 9 [300/804] | Train Loss: 0.8233 Grad: 118509.7422 LR: 9.0538e-05 | Elapse: 210.44s\n",
      "Epoch 9 [350/804] | Train Loss: 0.8222 Grad: 154678.9375 LR: 9.0325e-05 | Elapse: 245.47s\n",
      "Epoch 9 [400/804] | Train Loss: 0.8234 Grad: 142505.3125 LR: 9.0110e-05 | Elapse: 280.48s\n",
      "Epoch 9 [450/804] | Train Loss: 0.8215 Grad: 178604.4844 LR: 8.9893e-05 | Elapse: 315.49s\n",
      "Epoch 9 [500/804] | Train Loss: 0.8215 Grad: 199043.8750 LR: 8.9674e-05 | Elapse: 350.43s\n",
      "Epoch 9 [550/804] | Train Loss: 0.8187 Grad: 124405.2188 LR: 8.9453e-05 | Elapse: 385.42s\n",
      "Epoch 9 [600/804] | Train Loss: 0.8224 Grad: 106780.4297 LR: 8.9230e-05 | Elapse: 420.38s\n",
      "Epoch 9 [650/804] | Train Loss: 0.8235 Grad: 237715.9844 LR: 8.9004e-05 | Elapse: 455.37s\n",
      "Epoch 9 [700/804] | Train Loss: 0.8209 Grad: 113779.9297 LR: 8.8777e-05 | Elapse: 490.32s\n",
      "Epoch 9 [750/804] | Train Loss: 0.8205 Grad: 156138.5469 LR: 8.8548e-05 | Elapse: 525.34s\n",
      "Epoch 9 [800/804] | Train Loss: 0.8192 Grad: 148702.0000 LR: 8.8316e-05 | Elapse: 560.29s\n",
      "Epoch 9 [803/804] | Train Loss: 0.8199 Grad: 169243.0625 LR: 8.8302e-05 | Elapse: 562.39s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7f11f437e843358f55019b6f989f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [8]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [0/458] | Valid Loss: 0.7253 | Elapse: 0.35s\n",
      "Epoch 9 [50/458] | Valid Loss: 0.7598 | Elapse: 17.35s\n",
      "Epoch 9 [100/458] | Valid Loss: 0.7866 | Elapse: 34.24s\n",
      "Epoch 9 [150/458] | Valid Loss: 0.7810 | Elapse: 51.16s\n",
      "Epoch 9 [200/458] | Valid Loss: 0.7869 | Elapse: 68.05s\n",
      "Epoch 9 [250/458] | Valid Loss: 0.7916 | Elapse: 84.88s\n",
      "Epoch 9 [300/458] | Valid Loss: 0.7989 | Elapse: 101.65s\n",
      "Epoch 9 [350/458] | Valid Loss: 0.8017 | Elapse: 118.37s\n",
      "Epoch 9 [400/458] | Valid Loss: 0.8058 | Elapse: 135.07s\n",
      "Epoch 9 [450/458] | Valid Loss: 0.8059 | Elapse: 151.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9 - Average Loss: (train) 0.8199; (valid) 0.8063 | Time: 716.32s\n",
      "Best model found in epoch 9 | valid loss: 0.8063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [457/458] | Valid Loss: 0.8063 | Elapse: 153.93s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0e6c821b85405a8d31903cbbf60499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [9]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 [0/804] | Train Loss: 0.7622 Grad: 275089.8125 LR: 8.8298e-05 | Elapse: 0.70s\n",
      "Epoch 10 [50/804] | Train Loss: 0.7929 Grad: 228236.9219 LR: 8.8064e-05 | Elapse: 35.63s\n",
      "Epoch 10 [100/804] | Train Loss: 0.7986 Grad: 117873.7266 LR: 8.7829e-05 | Elapse: 70.65s\n",
      "Epoch 10 [150/804] | Train Loss: 0.7987 Grad: 214844.3125 LR: 8.7591e-05 | Elapse: 105.55s\n",
      "Epoch 10 [200/804] | Train Loss: 0.8075 Grad: 148064.2969 LR: 8.7352e-05 | Elapse: 140.46s\n",
      "Epoch 10 [250/804] | Train Loss: 0.8067 Grad: 191054.7344 LR: 8.7110e-05 | Elapse: 175.38s\n",
      "Epoch 10 [300/804] | Train Loss: 0.8069 Grad: 120898.9766 LR: 8.6867e-05 | Elapse: 210.29s\n",
      "Epoch 10 [350/804] | Train Loss: 0.8058 Grad: 148264.2812 LR: 8.6621e-05 | Elapse: 245.23s\n",
      "Epoch 10 [400/804] | Train Loss: 0.8070 Grad: 144564.7500 LR: 8.6374e-05 | Elapse: 280.25s\n",
      "Epoch 10 [450/804] | Train Loss: 0.8055 Grad: 185308.5312 LR: 8.6125e-05 | Elapse: 315.23s\n",
      "Epoch 10 [500/804] | Train Loss: 0.8058 Grad: 199897.6406 LR: 8.5874e-05 | Elapse: 350.24s\n",
      "Epoch 10 [550/804] | Train Loss: 0.8031 Grad: 134014.5625 LR: 8.5621e-05 | Elapse: 385.18s\n",
      "Epoch 10 [600/804] | Train Loss: 0.8069 Grad: 112965.5234 LR: 8.5366e-05 | Elapse: 420.13s\n",
      "Epoch 10 [650/804] | Train Loss: 0.8079 Grad: 231804.8594 LR: 8.5110e-05 | Elapse: 455.10s\n",
      "Epoch 10 [700/804] | Train Loss: 0.8055 Grad: 115882.7031 LR: 8.4851e-05 | Elapse: 490.08s\n",
      "Epoch 10 [750/804] | Train Loss: 0.8052 Grad: 153252.3281 LR: 8.4591e-05 | Elapse: 525.06s\n",
      "Epoch 10 [800/804] | Train Loss: 0.8040 Grad: 148210.8281 LR: 8.4329e-05 | Elapse: 560.02s\n",
      "Epoch 10 [803/804] | Train Loss: 0.8046 Grad: 173042.6094 LR: 8.4313e-05 | Elapse: 562.11s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557469478afe4aad8f5c6f87241340d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [9]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 [0/458] | Valid Loss: 0.7052 | Elapse: 0.34s\n",
      "Epoch 10 [50/458] | Valid Loss: 0.7444 | Elapse: 17.32s\n",
      "Epoch 10 [100/458] | Valid Loss: 0.7714 | Elapse: 34.34s\n",
      "Epoch 10 [150/458] | Valid Loss: 0.7653 | Elapse: 51.33s\n",
      "Epoch 10 [200/458] | Valid Loss: 0.7714 | Elapse: 68.26s\n",
      "Epoch 10 [250/458] | Valid Loss: 0.7748 | Elapse: 85.05s\n",
      "Epoch 10 [300/458] | Valid Loss: 0.7817 | Elapse: 101.82s\n",
      "Epoch 10 [350/458] | Valid Loss: 0.7835 | Elapse: 118.54s\n",
      "Epoch 10 [400/458] | Valid Loss: 0.7875 | Elapse: 135.24s\n",
      "Epoch 10 [450/458] | Valid Loss: 0.7874 | Elapse: 151.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10 - Average Loss: (train) 0.8046; (valid) 0.7878 | Time: 716.15s\n",
      "Best model found in epoch 10 | valid loss: 0.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 [457/458] | Valid Loss: 0.7878 | Elapse: 154.03s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d967184fec4c51a8151112773ca4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [10]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 [0/804] | Train Loss: 0.7406 Grad: 282093.7188 LR: 8.4308e-05 | Elapse: 0.70s\n",
      "Epoch 11 [50/804] | Train Loss: 0.7783 Grad: 235330.9531 LR: 8.4044e-05 | Elapse: 35.59s\n",
      "Epoch 11 [100/804] | Train Loss: 0.7841 Grad: 122767.9062 LR: 8.3778e-05 | Elapse: 70.50s\n",
      "Epoch 11 [150/804] | Train Loss: 0.7840 Grad: 216814.7500 LR: 8.3510e-05 | Elapse: 105.44s\n",
      "Epoch 11 [200/804] | Train Loss: 0.7929 Grad: 152517.9062 LR: 8.3241e-05 | Elapse: 140.39s\n",
      "Epoch 11 [250/804] | Train Loss: 0.7927 Grad: 194925.2344 LR: 8.2970e-05 | Elapse: 175.33s\n",
      "Epoch 11 [300/804] | Train Loss: 0.7932 Grad: 124033.9297 LR: 8.2697e-05 | Elapse: 210.25s\n",
      "Epoch 11 [350/804] | Train Loss: 0.7921 Grad: 144273.2188 LR: 8.2423e-05 | Elapse: 245.20s\n",
      "Epoch 11 [400/804] | Train Loss: 0.7933 Grad: 146443.4688 LR: 8.2147e-05 | Elapse: 280.22s\n",
      "Epoch 11 [450/804] | Train Loss: 0.7922 Grad: 192069.9531 LR: 8.1869e-05 | Elapse: 315.19s\n",
      "Epoch 11 [500/804] | Train Loss: 0.7926 Grad: 201038.0938 LR: 8.1589e-05 | Elapse: 350.08s\n",
      "Epoch 11 [550/804] | Train Loss: 0.7901 Grad: 140653.8281 LR: 8.1308e-05 | Elapse: 385.08s\n",
      "Epoch 11 [600/804] | Train Loss: 0.7938 Grad: 118868.5703 LR: 8.1025e-05 | Elapse: 420.16s\n",
      "Epoch 11 [650/804] | Train Loss: 0.7946 Grad: 227862.4219 LR: 8.0741e-05 | Elapse: 455.12s\n",
      "Epoch 11 [700/804] | Train Loss: 0.7923 Grad: 117342.2578 LR: 8.0455e-05 | Elapse: 490.19s\n",
      "Epoch 11 [750/804] | Train Loss: 0.7922 Grad: 149433.2812 LR: 8.0167e-05 | Elapse: 525.10s\n",
      "Epoch 11 [800/804] | Train Loss: 0.7912 Grad: 147022.9375 LR: 7.9878e-05 | Elapse: 560.13s\n",
      "Epoch 11 [803/804] | Train Loss: 0.7917 Grad: 176709.9219 LR: 7.9860e-05 | Elapse: 562.24s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b066b1ee4dc5462d97897e99edb49bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [10]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 [0/458] | Valid Loss: 0.6913 | Elapse: 0.35s\n",
      "Epoch 11 [50/458] | Valid Loss: 0.7325 | Elapse: 17.35s\n",
      "Epoch 11 [100/458] | Valid Loss: 0.7598 | Elapse: 34.36s\n",
      "Epoch 11 [150/458] | Valid Loss: 0.7532 | Elapse: 51.33s\n",
      "Epoch 11 [200/458] | Valid Loss: 0.7595 | Elapse: 68.23s\n",
      "Epoch 11 [250/458] | Valid Loss: 0.7619 | Elapse: 85.06s\n",
      "Epoch 11 [300/458] | Valid Loss: 0.7685 | Elapse: 101.85s\n",
      "Epoch 11 [350/458] | Valid Loss: 0.7696 | Elapse: 118.59s\n",
      "Epoch 11 [400/458] | Valid Loss: 0.7736 | Elapse: 135.31s\n",
      "Epoch 11 [450/458] | Valid Loss: 0.7734 | Elapse: 152.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 11 - Average Loss: (train) 0.7917; (valid) 0.7738 | Time: 716.38s\n",
      "Best model found in epoch 11 | valid loss: 0.7738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 [457/458] | Valid Loss: 0.7738 | Elapse: 154.14s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a84b51c84114937a0e35b3e0926ef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [11]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 [0/804] | Train Loss: 0.7214 Grad: 284465.0938 LR: 7.9854e-05 | Elapse: 0.70s\n",
      "Epoch 12 [50/804] | Train Loss: 0.7659 Grad: 241889.7812 LR: 7.9563e-05 | Elapse: 35.58s\n",
      "Epoch 12 [100/804] | Train Loss: 0.7711 Grad: 125906.3672 LR: 7.9271e-05 | Elapse: 70.52s\n",
      "Epoch 12 [150/804] | Train Loss: 0.7709 Grad: 219373.3125 LR: 7.8977e-05 | Elapse: 105.55s\n",
      "Epoch 12 [200/804] | Train Loss: 0.7798 Grad: 158891.4688 LR: 7.8681e-05 | Elapse: 140.52s\n",
      "Epoch 12 [250/804] | Train Loss: 0.7803 Grad: 198899.8750 LR: 7.8384e-05 | Elapse: 175.54s\n",
      "Epoch 12 [300/804] | Train Loss: 0.7808 Grad: 125329.9062 LR: 7.8086e-05 | Elapse: 210.52s\n",
      "Epoch 12 [350/804] | Train Loss: 0.7799 Grad: 142494.8594 LR: 7.7786e-05 | Elapse: 245.48s\n",
      "Epoch 12 [400/804] | Train Loss: 0.7810 Grad: 150105.2500 LR: 7.7484e-05 | Elapse: 280.47s\n",
      "Epoch 12 [450/804] | Train Loss: 0.7802 Grad: 198857.0938 LR: 7.7182e-05 | Elapse: 315.46s\n",
      "Epoch 12 [500/804] | Train Loss: 0.7808 Grad: 201027.0469 LR: 7.6877e-05 | Elapse: 350.37s\n",
      "Epoch 12 [550/804] | Train Loss: 0.7785 Grad: 140093.2500 LR: 7.6572e-05 | Elapse: 385.33s\n",
      "Epoch 12 [600/804] | Train Loss: 0.7822 Grad: 123108.8984 LR: 7.6264e-05 | Elapse: 420.28s\n",
      "Epoch 12 [650/804] | Train Loss: 0.7828 Grad: 225329.2812 LR: 7.5956e-05 | Elapse: 455.21s\n",
      "Epoch 12 [700/804] | Train Loss: 0.7807 Grad: 118664.7422 LR: 7.5646e-05 | Elapse: 490.19s\n",
      "Epoch 12 [750/804] | Train Loss: 0.7808 Grad: 145507.4219 LR: 7.5335e-05 | Elapse: 525.17s\n",
      "Epoch 12 [800/804] | Train Loss: 0.7797 Grad: 145519.7812 LR: 7.5023e-05 | Elapse: 560.09s\n",
      "Epoch 12 [803/804] | Train Loss: 0.7802 Grad: 181797.3125 LR: 7.5004e-05 | Elapse: 562.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dc8ff186404b8ead9328e12b6c3773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [11]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 [0/458] | Valid Loss: 0.6810 | Elapse: 0.34s\n",
      "Epoch 12 [50/458] | Valid Loss: 0.7228 | Elapse: 17.40s\n",
      "Epoch 12 [100/458] | Valid Loss: 0.7506 | Elapse: 34.37s\n",
      "Epoch 12 [150/458] | Valid Loss: 0.7437 | Elapse: 51.32s\n",
      "Epoch 12 [200/458] | Valid Loss: 0.7503 | Elapse: 68.23s\n",
      "Epoch 12 [250/458] | Valid Loss: 0.7521 | Elapse: 85.02s\n",
      "Epoch 12 [300/458] | Valid Loss: 0.7584 | Elapse: 101.75s\n",
      "Epoch 12 [350/458] | Valid Loss: 0.7590 | Elapse: 118.43s\n",
      "Epoch 12 [400/458] | Valid Loss: 0.7629 | Elapse: 135.16s\n",
      "Epoch 12 [450/458] | Valid Loss: 0.7627 | Elapse: 151.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 12 - Average Loss: (train) 0.7802; (valid) 0.7631 | Time: 716.13s\n",
      "Best model found in epoch 12 | valid loss: 0.7631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 [457/458] | Valid Loss: 0.7631 | Elapse: 153.94s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd81095f6f478b853aabbe377d0acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [12]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 [0/804] | Train Loss: 0.7037 Grad: 284746.3750 LR: 7.4997e-05 | Elapse: 0.70s\n",
      "Epoch 13 [50/804] | Train Loss: 0.7547 Grad: 249899.7969 LR: 7.4684e-05 | Elapse: 35.50s\n",
      "Epoch 13 [100/804] | Train Loss: 0.7594 Grad: 128038.6328 LR: 7.4368e-05 | Elapse: 70.36s\n",
      "Epoch 13 [150/804] | Train Loss: 0.7591 Grad: 224244.9688 LR: 7.4052e-05 | Elapse: 105.21s\n",
      "Epoch 13 [200/804] | Train Loss: 0.7679 Grad: 165582.0625 LR: 7.3734e-05 | Elapse: 140.02s\n",
      "Epoch 13 [250/804] | Train Loss: 0.7690 Grad: 202022.2188 LR: 7.3415e-05 | Elapse: 174.79s\n",
      "Epoch 13 [300/804] | Train Loss: 0.7696 Grad: 127683.3438 LR: 7.3095e-05 | Elapse: 209.63s\n",
      "Epoch 13 [350/804] | Train Loss: 0.7689 Grad: 140207.9062 LR: 7.2774e-05 | Elapse: 244.43s\n",
      "Epoch 13 [400/804] | Train Loss: 0.7700 Grad: 152843.3594 LR: 7.2451e-05 | Elapse: 279.31s\n",
      "Epoch 13 [450/804] | Train Loss: 0.7694 Grad: 203129.3125 LR: 7.2127e-05 | Elapse: 314.16s\n",
      "Epoch 13 [500/804] | Train Loss: 0.7701 Grad: 199682.1562 LR: 7.1802e-05 | Elapse: 348.99s\n",
      "Epoch 13 [550/804] | Train Loss: 0.7680 Grad: 138841.7969 LR: 7.1476e-05 | Elapse: 383.76s\n",
      "Epoch 13 [600/804] | Train Loss: 0.7716 Grad: 129211.2188 LR: 7.1149e-05 | Elapse: 418.53s\n",
      "Epoch 13 [650/804] | Train Loss: 0.7721 Grad: 222777.0000 LR: 7.0821e-05 | Elapse: 453.34s\n",
      "Epoch 13 [700/804] | Train Loss: 0.7701 Grad: 120041.3984 LR: 7.0491e-05 | Elapse: 488.28s\n",
      "Epoch 13 [750/804] | Train Loss: 0.7704 Grad: 143023.6406 LR: 7.0161e-05 | Elapse: 523.27s\n",
      "Epoch 13 [800/804] | Train Loss: 0.7693 Grad: 144837.3594 LR: 6.9829e-05 | Elapse: 558.19s\n",
      "Epoch 13 [803/804] | Train Loss: 0.7697 Grad: 187407.3906 LR: 6.9809e-05 | Elapse: 560.28s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c799078e954dd3a9aaa54dcd988383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [12]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 [0/458] | Valid Loss: 0.6735 | Elapse: 0.34s\n",
      "Epoch 13 [50/458] | Valid Loss: 0.7147 | Elapse: 17.15s\n",
      "Epoch 13 [100/458] | Valid Loss: 0.7429 | Elapse: 33.97s\n",
      "Epoch 13 [150/458] | Valid Loss: 0.7360 | Elapse: 50.72s\n",
      "Epoch 13 [200/458] | Valid Loss: 0.7428 | Elapse: 67.44s\n",
      "Epoch 13 [250/458] | Valid Loss: 0.7442 | Elapse: 84.12s\n",
      "Epoch 13 [300/458] | Valid Loss: 0.7504 | Elapse: 100.87s\n",
      "Epoch 13 [350/458] | Valid Loss: 0.7506 | Elapse: 117.67s\n",
      "Epoch 13 [400/458] | Valid Loss: 0.7547 | Elapse: 134.47s\n",
      "Epoch 13 [450/458] | Valid Loss: 0.7545 | Elapse: 151.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 13 - Average Loss: (train) 0.7697; (valid) 0.7548 | Time: 713.66s\n",
      "Best model found in epoch 13 | valid loss: 0.7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 [457/458] | Valid Loss: 0.7548 | Elapse: 153.37s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8795b994e1049b3bc206d93a19afb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [13]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 [0/804] | Train Loss: 0.6896 Grad: 286112.9688 LR: 6.9803e-05 | Elapse: 0.70s\n",
      "Epoch 14 [50/804] | Train Loss: 0.7441 Grad: 262534.0000 LR: 6.9470e-05 | Elapse: 35.53s\n",
      "Epoch 14 [100/804] | Train Loss: 0.7488 Grad: 130642.0938 LR: 6.9136e-05 | Elapse: 70.39s\n",
      "Epoch 14 [150/804] | Train Loss: 0.7483 Grad: 230648.8594 LR: 6.8802e-05 | Elapse: 105.31s\n",
      "Epoch 14 [200/804] | Train Loss: 0.7573 Grad: 171237.0156 LR: 6.8466e-05 | Elapse: 140.16s\n",
      "Epoch 14 [250/804] | Train Loss: 0.7588 Grad: 203859.1875 LR: 6.8130e-05 | Elapse: 175.18s\n",
      "Epoch 14 [300/804] | Train Loss: 0.7595 Grad: 131620.1719 LR: 6.7792e-05 | Elapse: 210.10s\n",
      "Epoch 14 [350/804] | Train Loss: 0.7590 Grad: 136988.3125 LR: 6.7454e-05 | Elapse: 244.94s\n",
      "Epoch 14 [400/804] | Train Loss: 0.7602 Grad: 155650.0000 LR: 6.7114e-05 | Elapse: 279.75s\n",
      "Epoch 14 [450/804] | Train Loss: 0.7598 Grad: 208366.3438 LR: 6.6774e-05 | Elapse: 314.67s\n",
      "Epoch 14 [500/804] | Train Loss: 0.7605 Grad: 194486.1250 LR: 6.6433e-05 | Elapse: 349.47s\n",
      "Epoch 14 [550/804] | Train Loss: 0.7586 Grad: 138897.0938 LR: 6.6091e-05 | Elapse: 384.26s\n",
      "Epoch 14 [600/804] | Train Loss: 0.7620 Grad: 140976.2656 LR: 6.5748e-05 | Elapse: 419.20s\n",
      "Epoch 14 [650/804] | Train Loss: 0.7623 Grad: 220407.5312 LR: 6.5404e-05 | Elapse: 454.16s\n",
      "Epoch 14 [700/804] | Train Loss: 0.7606 Grad: 120971.7500 LR: 6.5060e-05 | Elapse: 489.04s\n",
      "Epoch 14 [750/804] | Train Loss: 0.7610 Grad: 141837.7344 LR: 6.4714e-05 | Elapse: 523.97s\n",
      "Epoch 14 [800/804] | Train Loss: 0.7598 Grad: 144498.6250 LR: 6.4368e-05 | Elapse: 558.91s\n",
      "Epoch 14 [803/804] | Train Loss: 0.7603 Grad: 192784.8750 LR: 6.4347e-05 | Elapse: 561.02s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea018dd122643ea9b5d583da5c7c79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [13]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 [0/458] | Valid Loss: 0.6680 | Elapse: 0.35s\n",
      "Epoch 14 [50/458] | Valid Loss: 0.7079 | Elapse: 17.34s\n",
      "Epoch 14 [100/458] | Valid Loss: 0.7365 | Elapse: 34.37s\n",
      "Epoch 14 [150/458] | Valid Loss: 0.7297 | Elapse: 51.38s\n",
      "Epoch 14 [200/458] | Valid Loss: 0.7368 | Elapse: 68.37s\n",
      "Epoch 14 [250/458] | Valid Loss: 0.7379 | Elapse: 85.22s\n",
      "Epoch 14 [300/458] | Valid Loss: 0.7441 | Elapse: 102.11s\n",
      "Epoch 14 [350/458] | Valid Loss: 0.7441 | Elapse: 118.92s\n",
      "Epoch 14 [400/458] | Valid Loss: 0.7483 | Elapse: 135.66s\n",
      "Epoch 14 [450/458] | Valid Loss: 0.7481 | Elapse: 152.38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 14 - Average Loss: (train) 0.7603; (valid) 0.7484 | Time: 715.52s\n",
      "Best model found in epoch 14 | valid loss: 0.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 [457/458] | Valid Loss: 0.7484 | Elapse: 154.50s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b57e35d5f48d39e4ad9bbc4f50345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [14]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 [0/804] | Train Loss: 0.6789 Grad: 288241.6562 LR: 6.4341e-05 | Elapse: 0.70s\n",
      "Epoch 15 [50/804] | Train Loss: 0.7343 Grad: 278884.5938 LR: 6.3994e-05 | Elapse: 35.61s\n",
      "Epoch 15 [100/804] | Train Loss: 0.7393 Grad: 135119.0781 LR: 6.3646e-05 | Elapse: 70.58s\n",
      "Epoch 15 [150/804] | Train Loss: 0.7389 Grad: 236539.7188 LR: 6.3298e-05 | Elapse: 105.45s\n",
      "Epoch 15 [200/804] | Train Loss: 0.7479 Grad: 174561.7344 LR: 6.2949e-05 | Elapse: 140.41s\n",
      "Epoch 15 [250/804] | Train Loss: 0.7496 Grad: 203493.8750 LR: 6.2599e-05 | Elapse: 175.38s\n",
      "Epoch 15 [300/804] | Train Loss: 0.7505 Grad: 136408.6562 LR: 6.2249e-05 | Elapse: 210.41s\n",
      "Epoch 15 [350/804] | Train Loss: 0.7502 Grad: 133163.0469 LR: 6.1898e-05 | Elapse: 245.44s\n",
      "Epoch 15 [400/804] | Train Loss: 0.7515 Grad: 159505.0469 LR: 6.1546e-05 | Elapse: 280.57s\n",
      "Epoch 15 [450/804] | Train Loss: 0.7512 Grad: 216774.3438 LR: 6.1194e-05 | Elapse: 315.59s\n",
      "Epoch 15 [500/804] | Train Loss: 0.7519 Grad: 187923.4375 LR: 6.0841e-05 | Elapse: 350.65s\n",
      "Epoch 15 [550/804] | Train Loss: 0.7500 Grad: 140778.4219 LR: 6.0488e-05 | Elapse: 385.67s\n",
      "Epoch 15 [600/804] | Train Loss: 0.7533 Grad: 152672.6406 LR: 6.0134e-05 | Elapse: 420.60s\n",
      "Epoch 15 [650/804] | Train Loss: 0.7535 Grad: 217390.4062 LR: 5.9779e-05 | Elapse: 455.72s\n",
      "Epoch 15 [700/804] | Train Loss: 0.7519 Grad: 120591.1250 LR: 5.9425e-05 | Elapse: 490.75s\n",
      "Epoch 15 [750/804] | Train Loss: 0.7524 Grad: 142271.7188 LR: 5.9069e-05 | Elapse: 525.77s\n",
      "Epoch 15 [800/804] | Train Loss: 0.7513 Grad: 144684.7500 LR: 5.8713e-05 | Elapse: 560.90s\n",
      "Epoch 15 [803/804] | Train Loss: 0.7517 Grad: 197273.1250 LR: 5.8692e-05 | Elapse: 563.03s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215ae120ca8b4e96afa8b8d95cb1b4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [14]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 [0/458] | Valid Loss: 0.6645 | Elapse: 0.35s\n",
      "Epoch 15 [50/458] | Valid Loss: 0.7020 | Elapse: 17.54s\n",
      "Epoch 15 [100/458] | Valid Loss: 0.7308 | Elapse: 34.64s\n",
      "Epoch 15 [150/458] | Valid Loss: 0.7241 | Elapse: 51.66s\n",
      "Epoch 15 [200/458] | Valid Loss: 0.7315 | Elapse: 68.58s\n",
      "Epoch 15 [250/458] | Valid Loss: 0.7325 | Elapse: 85.49s\n",
      "Epoch 15 [300/458] | Valid Loss: 0.7386 | Elapse: 102.40s\n",
      "Epoch 15 [350/458] | Valid Loss: 0.7386 | Elapse: 119.37s\n",
      "Epoch 15 [400/458] | Valid Loss: 0.7429 | Elapse: 136.32s\n",
      "Epoch 15 [450/458] | Valid Loss: 0.7428 | Elapse: 153.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 15 - Average Loss: (train) 0.7517; (valid) 0.7431 | Time: 718.44s\n",
      "Best model found in epoch 15 | valid loss: 0.7431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 [457/458] | Valid Loss: 0.7431 | Elapse: 155.41s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ea96c3681f4ca3a2e0a284aa0a297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [15]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 [0/804] | Train Loss: 0.6717 Grad: 292004.8750 LR: 5.8685e-05 | Elapse: 0.70s\n",
      "Epoch 16 [50/804] | Train Loss: 0.7254 Grad: 153902.0000 LR: 5.8328e-05 | Elapse: 35.56s\n",
      "Epoch 16 [100/804] | Train Loss: 0.7313 Grad: 138742.6250 LR: 5.7971e-05 | Elapse: 70.41s\n",
      "Epoch 16 [150/804] | Train Loss: 0.7302 Grad: 238993.2031 LR: 5.7614e-05 | Elapse: 105.33s\n",
      "Epoch 16 [200/804] | Train Loss: 0.7393 Grad: 176062.4531 LR: 5.7257e-05 | Elapse: 140.25s\n",
      "Epoch 16 [250/804] | Train Loss: 0.7411 Grad: 204302.3125 LR: 5.6899e-05 | Elapse: 175.17s\n",
      "Epoch 16 [300/804] | Train Loss: 0.7421 Grad: 140401.5781 LR: 5.6540e-05 | Elapse: 210.15s\n",
      "Epoch 16 [350/804] | Train Loss: 0.7420 Grad: 127806.1094 LR: 5.6181e-05 | Elapse: 245.08s\n",
      "Epoch 16 [400/804] | Train Loss: 0.7434 Grad: 164867.5625 LR: 5.5822e-05 | Elapse: 279.99s\n",
      "Epoch 16 [450/804] | Train Loss: 0.7431 Grad: 226430.9375 LR: 5.5463e-05 | Elapse: 314.88s\n",
      "Epoch 16 [500/804] | Train Loss: 0.7440 Grad: 183221.8906 LR: 5.5103e-05 | Elapse: 349.84s\n",
      "Epoch 16 [550/804] | Train Loss: 0.7419 Grad: 144658.0781 LR: 5.4743e-05 | Elapse: 384.82s\n",
      "Epoch 16 [600/804] | Train Loss: 0.7451 Grad: 156612.5625 LR: 5.4383e-05 | Elapse: 419.69s\n",
      "Epoch 16 [650/804] | Train Loss: 0.7453 Grad: 214629.1406 LR: 5.4023e-05 | Elapse: 454.61s\n",
      "Epoch 16 [700/804] | Train Loss: 0.7439 Grad: 123339.1172 LR: 5.3662e-05 | Elapse: 489.54s\n",
      "Epoch 16 [750/804] | Train Loss: 0.7444 Grad: 144144.8281 LR: 5.3301e-05 | Elapse: 524.47s\n",
      "Epoch 16 [800/804] | Train Loss: 0.7433 Grad: 144938.2188 LR: 5.2941e-05 | Elapse: 559.60s\n",
      "Epoch 16 [803/804] | Train Loss: 0.7437 Grad: 201063.4062 LR: 5.2919e-05 | Elapse: 561.70s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ee0683a6ef45c4bbfc56a818891e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [15]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 [0/458] | Valid Loss: 0.6625 | Elapse: 0.34s\n",
      "Epoch 16 [50/458] | Valid Loss: 0.6969 | Elapse: 17.29s\n",
      "Epoch 16 [100/458] | Valid Loss: 0.7255 | Elapse: 34.23s\n",
      "Epoch 16 [150/458] | Valid Loss: 0.7187 | Elapse: 51.12s\n",
      "Epoch 16 [200/458] | Valid Loss: 0.7265 | Elapse: 68.02s\n",
      "Epoch 16 [250/458] | Valid Loss: 0.7274 | Elapse: 84.78s\n",
      "Epoch 16 [300/458] | Valid Loss: 0.7335 | Elapse: 101.60s\n",
      "Epoch 16 [350/458] | Valid Loss: 0.7334 | Elapse: 118.42s\n",
      "Epoch 16 [400/458] | Valid Loss: 0.7377 | Elapse: 135.18s\n",
      "Epoch 16 [450/458] | Valid Loss: 0.7377 | Elapse: 151.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 16 - Average Loss: (train) 0.7437; (valid) 0.7379 | Time: 715.70s\n",
      "Best model found in epoch 16 | valid loss: 0.7379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 [457/458] | Valid Loss: 0.7379 | Elapse: 153.99s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6ad9369e384dd1aa557a70eae9c2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [16]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 [0/804] | Train Loss: 0.6644 Grad: 293486.5312 LR: 5.2912e-05 | Elapse: 0.70s\n",
      "Epoch 17 [50/804] | Train Loss: 0.7171 Grad: 162386.0625 LR: 5.2551e-05 | Elapse: 35.51s\n",
      "Epoch 17 [100/804] | Train Loss: 0.7238 Grad: 146569.7188 LR: 5.2189e-05 | Elapse: 70.43s\n",
      "Epoch 17 [150/804] | Train Loss: 0.7223 Grad: 240304.6250 LR: 5.1828e-05 | Elapse: 105.28s\n",
      "Epoch 17 [200/804] | Train Loss: 0.7315 Grad: 174049.0938 LR: 5.1466e-05 | Elapse: 140.18s\n",
      "Epoch 17 [250/804] | Train Loss: 0.7335 Grad: 206172.5469 LR: 5.1105e-05 | Elapse: 175.13s\n",
      "Epoch 17 [300/804] | Train Loss: 0.7344 Grad: 144532.9375 LR: 5.0743e-05 | Elapse: 210.08s\n",
      "Epoch 17 [350/804] | Train Loss: 0.7345 Grad: 120073.5938 LR: 5.0382e-05 | Elapse: 244.94s\n",
      "Epoch 17 [400/804] | Train Loss: 0.7361 Grad: 172030.1719 LR: 5.0020e-05 | Elapse: 279.82s\n",
      "Epoch 17 [450/804] | Train Loss: 0.7359 Grad: 235599.8438 LR: 4.9658e-05 | Elapse: 314.85s\n",
      "Epoch 17 [500/804] | Train Loss: 0.7368 Grad: 182310.0781 LR: 4.9297e-05 | Elapse: 349.86s\n",
      "Epoch 17 [550/804] | Train Loss: 0.7345 Grad: 148657.5625 LR: 4.8935e-05 | Elapse: 384.75s\n",
      "Epoch 17 [600/804] | Train Loss: 0.7377 Grad: 158173.7344 LR: 4.8574e-05 | Elapse: 419.81s\n",
      "Epoch 17 [650/804] | Train Loss: 0.7378 Grad: 212027.6250 LR: 4.8212e-05 | Elapse: 454.79s\n",
      "Epoch 17 [700/804] | Train Loss: 0.7364 Grad: 127242.2031 LR: 4.7851e-05 | Elapse: 489.79s\n",
      "Epoch 17 [750/804] | Train Loss: 0.7371 Grad: 146416.0938 LR: 4.7489e-05 | Elapse: 524.82s\n",
      "Epoch 17 [800/804] | Train Loss: 0.7360 Grad: 145457.3906 LR: 4.7128e-05 | Elapse: 559.80s\n",
      "Epoch 17 [803/804] | Train Loss: 0.7364 Grad: 203520.7812 LR: 4.7107e-05 | Elapse: 561.90s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e6096c4a1640bd8f808a18ae36e362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [16]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 [0/458] | Valid Loss: 0.6602 | Elapse: 0.34s\n",
      "Epoch 17 [50/458] | Valid Loss: 0.6924 | Elapse: 17.37s\n",
      "Epoch 17 [100/458] | Valid Loss: 0.7207 | Elapse: 34.44s\n",
      "Epoch 17 [150/458] | Valid Loss: 0.7137 | Elapse: 51.43s\n",
      "Epoch 17 [200/458] | Valid Loss: 0.7219 | Elapse: 68.41s\n",
      "Epoch 17 [250/458] | Valid Loss: 0.7228 | Elapse: 85.29s\n",
      "Epoch 17 [300/458] | Valid Loss: 0.7290 | Elapse: 102.12s\n",
      "Epoch 17 [350/458] | Valid Loss: 0.7288 | Elapse: 118.88s\n",
      "Epoch 17 [400/458] | Valid Loss: 0.7332 | Elapse: 135.70s\n",
      "Epoch 17 [450/458] | Valid Loss: 0.7332 | Elapse: 152.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 17 - Average Loss: (train) 0.7364; (valid) 0.7334 | Time: 716.55s\n",
      "Best model found in epoch 17 | valid loss: 0.7334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 [457/458] | Valid Loss: 0.7334 | Elapse: 154.65s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a911159d4cc4ca9a5d21d556d055d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [17]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 [0/804] | Train Loss: 0.6580 Grad: 296136.3438 LR: 4.7099e-05 | Elapse: 0.70s\n",
      "Epoch 18 [50/804] | Train Loss: 0.7093 Grad: 170862.9219 LR: 4.6739e-05 | Elapse: 35.56s\n",
      "Epoch 18 [100/804] | Train Loss: 0.7169 Grad: 159134.1875 LR: 4.6378e-05 | Elapse: 70.51s\n",
      "Epoch 18 [150/804] | Train Loss: 0.7151 Grad: 239181.6562 LR: 4.6017e-05 | Elapse: 105.45s\n",
      "Epoch 18 [200/804] | Train Loss: 0.7243 Grad: 169311.3594 LR: 4.5657e-05 | Elapse: 140.49s\n",
      "Epoch 18 [250/804] | Train Loss: 0.7263 Grad: 209409.5781 LR: 4.5297e-05 | Elapse: 175.54s\n",
      "Epoch 18 [300/804] | Train Loss: 0.7272 Grad: 147130.1250 LR: 4.4937e-05 | Elapse: 210.57s\n",
      "Epoch 18 [350/804] | Train Loss: 0.7276 Grad: 110868.6250 LR: 4.4577e-05 | Elapse: 245.50s\n",
      "Epoch 18 [400/804] | Train Loss: 0.7294 Grad: 179611.4375 LR: 4.4218e-05 | Elapse: 280.55s\n",
      "Epoch 18 [450/804] | Train Loss: 0.7292 Grad: 241272.5156 LR: 4.3859e-05 | Elapse: 315.53s\n",
      "Epoch 18 [500/804] | Train Loss: 0.7301 Grad: 184935.2344 LR: 4.3500e-05 | Elapse: 350.59s\n",
      "Epoch 18 [550/804] | Train Loss: 0.7277 Grad: 152447.4844 LR: 4.3141e-05 | Elapse: 385.61s\n",
      "Epoch 18 [600/804] | Train Loss: 0.7309 Grad: 160386.7500 LR: 4.2783e-05 | Elapse: 420.67s\n",
      "Epoch 18 [650/804] | Train Loss: 0.7309 Grad: 210472.7344 LR: 4.2426e-05 | Elapse: 455.71s\n",
      "Epoch 18 [700/804] | Train Loss: 0.7295 Grad: 130870.8125 LR: 4.2069e-05 | Elapse: 490.92s\n",
      "Epoch 18 [750/804] | Train Loss: 0.7302 Grad: 148400.4062 LR: 4.1712e-05 | Elapse: 526.02s\n",
      "Epoch 18 [800/804] | Train Loss: 0.7292 Grad: 146451.5469 LR: 4.1355e-05 | Elapse: 561.03s\n",
      "Epoch 18 [803/804] | Train Loss: 0.7296 Grad: 206429.5000 LR: 4.1334e-05 | Elapse: 563.14s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aef39c70fb41eda7410da204a74dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [17]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 [0/458] | Valid Loss: 0.6559 | Elapse: 0.35s\n",
      "Epoch 18 [50/458] | Valid Loss: 0.6885 | Elapse: 17.47s\n",
      "Epoch 18 [100/458] | Valid Loss: 0.7164 | Elapse: 34.45s\n",
      "Epoch 18 [150/458] | Valid Loss: 0.7093 | Elapse: 51.44s\n",
      "Epoch 18 [200/458] | Valid Loss: 0.7179 | Elapse: 68.40s\n",
      "Epoch 18 [250/458] | Valid Loss: 0.7190 | Elapse: 85.16s\n",
      "Epoch 18 [300/458] | Valid Loss: 0.7253 | Elapse: 101.88s\n",
      "Epoch 18 [350/458] | Valid Loss: 0.7251 | Elapse: 118.69s\n",
      "Epoch 18 [400/458] | Valid Loss: 0.7297 | Elapse: 135.43s\n",
      "Epoch 18 [450/458] | Valid Loss: 0.7298 | Elapse: 152.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 18 - Average Loss: (train) 0.7296; (valid) 0.7300 | Time: 717.44s\n",
      "Best model found in epoch 18 | valid loss: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 [457/458] | Valid Loss: 0.7300 | Elapse: 154.29s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f112a958f13439f92cda05c9c672263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [18]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 [0/804] | Train Loss: 0.6516 Grad: 300252.2188 LR: 4.1327e-05 | Elapse: 0.70s\n",
      "Epoch 19 [50/804] | Train Loss: 0.7028 Grad: 179050.4531 LR: 4.0971e-05 | Elapse: 35.63s\n",
      "Epoch 19 [100/804] | Train Loss: 0.7109 Grad: 168475.6406 LR: 4.0615e-05 | Elapse: 70.54s\n",
      "Epoch 19 [150/804] | Train Loss: 0.7085 Grad: 237860.7656 LR: 4.0261e-05 | Elapse: 105.52s\n",
      "Epoch 19 [200/804] | Train Loss: 0.7177 Grad: 165078.1094 LR: 3.9906e-05 | Elapse: 140.59s\n",
      "Epoch 19 [250/804] | Train Loss: 0.7196 Grad: 210807.0000 LR: 3.9552e-05 | Elapse: 175.55s\n",
      "Epoch 19 [300/804] | Train Loss: 0.7204 Grad: 148282.0781 LR: 3.9199e-05 | Elapse: 210.58s\n",
      "Epoch 19 [350/804] | Train Loss: 0.7211 Grad: 102965.0469 LR: 3.8846e-05 | Elapse: 245.64s\n",
      "Epoch 19 [400/804] | Train Loss: 0.7229 Grad: 184560.8281 LR: 3.8494e-05 | Elapse: 280.63s\n",
      "Epoch 19 [450/804] | Train Loss: 0.7227 Grad: 244524.5938 LR: 3.8142e-05 | Elapse: 315.68s\n",
      "Epoch 19 [500/804] | Train Loss: 0.7236 Grad: 188426.9062 LR: 3.7791e-05 | Elapse: 350.75s\n",
      "Epoch 19 [550/804] | Train Loss: 0.7211 Grad: 156211.4219 LR: 3.7441e-05 | Elapse: 385.75s\n",
      "Epoch 19 [600/804] | Train Loss: 0.7243 Grad: 162247.1562 LR: 3.7091e-05 | Elapse: 420.75s\n",
      "Epoch 19 [650/804] | Train Loss: 0.7243 Grad: 211657.9062 LR: 3.6742e-05 | Elapse: 455.68s\n",
      "Epoch 19 [700/804] | Train Loss: 0.7229 Grad: 135028.7344 LR: 3.6394e-05 | Elapse: 490.63s\n",
      "Epoch 19 [750/804] | Train Loss: 0.7238 Grad: 149713.2500 LR: 3.6046e-05 | Elapse: 525.58s\n",
      "Epoch 19 [800/804] | Train Loss: 0.7229 Grad: 147128.4062 LR: 3.5699e-05 | Elapse: 560.56s\n",
      "Epoch 19 [803/804] | Train Loss: 0.7232 Grad: 210308.0938 LR: 3.5679e-05 | Elapse: 562.65s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773f463e671f46bf90b29b8cec852ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [18]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 [0/458] | Valid Loss: 0.6505 | Elapse: 0.34s\n",
      "Epoch 19 [50/458] | Valid Loss: 0.6856 | Elapse: 17.39s\n",
      "Epoch 19 [100/458] | Valid Loss: 0.7132 | Elapse: 34.41s\n",
      "Epoch 19 [150/458] | Valid Loss: 0.7060 | Elapse: 51.41s\n",
      "Epoch 19 [200/458] | Valid Loss: 0.7150 | Elapse: 68.39s\n",
      "Epoch 19 [250/458] | Valid Loss: 0.7165 | Elapse: 85.39s\n",
      "Epoch 19 [300/458] | Valid Loss: 0.7229 | Elapse: 102.35s\n",
      "Epoch 19 [350/458] | Valid Loss: 0.7228 | Elapse: 119.27s\n",
      "Epoch 19 [400/458] | Valid Loss: 0.7277 | Elapse: 136.14s\n",
      "Epoch 19 [450/458] | Valid Loss: 0.7280 | Elapse: 153.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 19 - Average Loss: (train) 0.7232; (valid) 0.7281 | Time: 717.80s\n",
      "Best model found in epoch 19 | valid loss: 0.7281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 [457/458] | Valid Loss: 0.7281 | Elapse: 155.14s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db46c8084d74d89bf0fc8a37018a2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [19]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 [0/804] | Train Loss: 0.6487 Grad: 309546.6562 LR: 3.5672e-05 | Elapse: 0.70s\n",
      "Epoch 20 [50/804] | Train Loss: 0.6970 Grad: 185847.2344 LR: 3.5326e-05 | Elapse: 35.55s\n",
      "Epoch 20 [100/804] | Train Loss: 0.7055 Grad: 175530.0625 LR: 3.4980e-05 | Elapse: 70.47s\n",
      "Epoch 20 [150/804] | Train Loss: 0.7025 Grad: 236408.4531 LR: 3.4636e-05 | Elapse: 105.47s\n",
      "Epoch 20 [200/804] | Train Loss: 0.7116 Grad: 161367.6406 LR: 3.4292e-05 | Elapse: 140.62s\n",
      "Epoch 20 [250/804] | Train Loss: 0.7135 Grad: 210796.4375 LR: 3.3949e-05 | Elapse: 175.61s\n",
      "Epoch 20 [300/804] | Train Loss: 0.7143 Grad: 148089.3125 LR: 3.3607e-05 | Elapse: 210.62s\n",
      "Epoch 20 [350/804] | Train Loss: 0.7151 Grad: 97905.5391 LR: 3.3266e-05 | Elapse: 245.70s\n",
      "Epoch 20 [400/804] | Train Loss: 0.7168 Grad: 185631.0781 LR: 3.2926e-05 | Elapse: 280.77s\n",
      "Epoch 20 [450/804] | Train Loss: 0.7168 Grad: 247658.8438 LR: 3.2586e-05 | Elapse: 315.75s\n",
      "Epoch 20 [500/804] | Train Loss: 0.7176 Grad: 190830.5469 LR: 3.2248e-05 | Elapse: 350.75s\n",
      "Epoch 20 [550/804] | Train Loss: 0.7150 Grad: 159559.2969 LR: 3.1910e-05 | Elapse: 385.65s\n",
      "Epoch 20 [600/804] | Train Loss: 0.7183 Grad: 162494.2656 LR: 3.1574e-05 | Elapse: 420.75s\n",
      "Epoch 20 [650/804] | Train Loss: 0.7182 Grad: 214720.2500 LR: 3.1238e-05 | Elapse: 455.79s\n",
      "Epoch 20 [700/804] | Train Loss: 0.7169 Grad: 141194.2656 LR: 3.0904e-05 | Elapse: 490.75s\n",
      "Epoch 20 [750/804] | Train Loss: 0.7179 Grad: 149632.7500 LR: 3.0570e-05 | Elapse: 525.73s\n",
      "Epoch 20 [800/804] | Train Loss: 0.7170 Grad: 148273.8750 LR: 3.0237e-05 | Elapse: 560.70s\n",
      "Epoch 20 [803/804] | Train Loss: 0.7174 Grad: 214818.5469 LR: 3.0217e-05 | Elapse: 562.79s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dba32cbdfc47d1b26ff1a354e418ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [19]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 [0/458] | Valid Loss: 0.6451 | Elapse: 0.35s\n",
      "Epoch 20 [50/458] | Valid Loss: 0.6840 | Elapse: 17.29s\n",
      "Epoch 20 [100/458] | Valid Loss: 0.7114 | Elapse: 34.20s\n",
      "Epoch 20 [150/458] | Valid Loss: 0.7039 | Elapse: 51.15s\n",
      "Epoch 20 [200/458] | Valid Loss: 0.7135 | Elapse: 68.02s\n",
      "Epoch 20 [250/458] | Valid Loss: 0.7155 | Elapse: 84.76s\n",
      "Epoch 20 [300/458] | Valid Loss: 0.7221 | Elapse: 101.47s\n",
      "Epoch 20 [350/458] | Valid Loss: 0.7221 | Elapse: 118.26s\n",
      "Epoch 20 [400/458] | Valid Loss: 0.7272 | Elapse: 135.07s\n",
      "Epoch 20 [450/458] | Valid Loss: 0.7278 | Elapse: 151.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 20 - Average Loss: (train) 0.7174; (valid) 0.7279 | Time: 716.80s\n",
      "Best model found in epoch 20 | valid loss: 0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 [457/458] | Valid Loss: 0.7279 | Elapse: 154.01s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3dc6bddaaf4d1f82fd35fbf4d6f08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [20]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 [0/804] | Train Loss: 0.6509 Grad: 323147.0312 LR: 3.0211e-05 | Elapse: 0.70s\n",
      "Epoch 21 [50/804] | Train Loss: 0.6920 Grad: 192097.0781 LR: 2.9879e-05 | Elapse: 35.65s\n",
      "Epoch 21 [100/804] | Train Loss: 0.7005 Grad: 179894.4375 LR: 2.9549e-05 | Elapse: 70.67s\n",
      "Epoch 21 [150/804] | Train Loss: 0.6972 Grad: 234351.9219 LR: 2.9219e-05 | Elapse: 105.73s\n",
      "Epoch 21 [200/804] | Train Loss: 0.7061 Grad: 159903.6250 LR: 2.8891e-05 | Elapse: 140.79s\n",
      "Epoch 21 [250/804] | Train Loss: 0.7080 Grad: 210289.8438 LR: 2.8564e-05 | Elapse: 175.78s\n",
      "Epoch 21 [300/804] | Train Loss: 0.7089 Grad: 146920.3594 LR: 2.8238e-05 | Elapse: 210.70s\n",
      "Epoch 21 [350/804] | Train Loss: 0.7096 Grad: 96083.7891 LR: 2.7913e-05 | Elapse: 245.69s\n",
      "Epoch 21 [400/804] | Train Loss: 0.7113 Grad: 184715.2812 LR: 2.7589e-05 | Elapse: 280.72s\n",
      "Epoch 21 [450/804] | Train Loss: 0.7114 Grad: 250581.5156 LR: 2.7266e-05 | Elapse: 315.67s\n",
      "Epoch 21 [500/804] | Train Loss: 0.7121 Grad: 192223.0625 LR: 2.6945e-05 | Elapse: 350.59s\n",
      "Epoch 21 [550/804] | Train Loss: 0.7095 Grad: 162053.0312 LR: 2.6625e-05 | Elapse: 385.47s\n",
      "Epoch 21 [600/804] | Train Loss: 0.7128 Grad: 162292.1250 LR: 2.6306e-05 | Elapse: 420.46s\n",
      "Epoch 21 [650/804] | Train Loss: 0.7128 Grad: 217757.3594 LR: 2.5988e-05 | Elapse: 455.37s\n",
      "Epoch 21 [700/804] | Train Loss: 0.7114 Grad: 147392.1875 LR: 2.5672e-05 | Elapse: 490.41s\n",
      "Epoch 21 [750/804] | Train Loss: 0.7125 Grad: 148466.3438 LR: 2.5356e-05 | Elapse: 525.40s\n",
      "Epoch 21 [800/804] | Train Loss: 0.7117 Grad: 150495.5625 LR: 2.5043e-05 | Elapse: 560.42s\n",
      "Epoch 21 [803/804] | Train Loss: 0.7121 Grad: 217749.6406 LR: 2.5024e-05 | Elapse: 562.52s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eea98674ea4c1c90c34e5ee1a87180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [20]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 [0/458] | Valid Loss: 0.6408 | Elapse: 0.34s\n",
      "Epoch 21 [50/458] | Valid Loss: 0.6824 | Elapse: 17.32s\n",
      "Epoch 21 [100/458] | Valid Loss: 0.7098 | Elapse: 34.30s\n",
      "Epoch 21 [150/458] | Valid Loss: 0.7022 | Elapse: 51.22s\n",
      "Epoch 21 [200/458] | Valid Loss: 0.7124 | Elapse: 68.10s\n",
      "Epoch 21 [250/458] | Valid Loss: 0.7146 | Elapse: 84.98s\n",
      "Epoch 21 [300/458] | Valid Loss: 0.7214 | Elapse: 101.86s\n",
      "Epoch 21 [350/458] | Valid Loss: 0.7213 | Elapse: 118.72s\n",
      "Epoch 21 [400/458] | Valid Loss: 0.7268 | Elapse: 135.61s\n",
      "Epoch 21 [450/458] | Valid Loss: 0.7274 | Elapse: 152.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 21 - Average Loss: (train) 0.7121; (valid) 0.7275 | Time: 717.24s\n",
      "Best model found in epoch 21 | valid loss: 0.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 [457/458] | Valid Loss: 0.7275 | Elapse: 154.72s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ab2f3e6c8b4af78266b1492ef0abde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [21]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 [0/804] | Train Loss: 0.6540 Grad: 335200.4688 LR: 2.5017e-05 | Elapse: 0.70s\n",
      "Epoch 22 [50/804] | Train Loss: 0.6876 Grad: 196097.1562 LR: 2.4705e-05 | Elapse: 35.75s\n",
      "Epoch 22 [100/804] | Train Loss: 0.6961 Grad: 183859.7188 LR: 2.4394e-05 | Elapse: 70.85s\n",
      "Epoch 22 [150/804] | Train Loss: 0.6924 Grad: 231076.9688 LR: 2.4084e-05 | Elapse: 106.11s\n",
      "Epoch 22 [200/804] | Train Loss: 0.7012 Grad: 162064.0312 LR: 2.3776e-05 | Elapse: 141.13s\n",
      "Epoch 22 [250/804] | Train Loss: 0.7033 Grad: 209699.1562 LR: 2.3468e-05 | Elapse: 176.18s\n",
      "Epoch 22 [300/804] | Train Loss: 0.7041 Grad: 144139.0469 LR: 2.3163e-05 | Elapse: 211.35s\n",
      "Epoch 22 [350/804] | Train Loss: 0.7047 Grad: 96517.8750 LR: 2.2858e-05 | Elapse: 246.55s\n",
      "Epoch 22 [400/804] | Train Loss: 0.7065 Grad: 183894.5938 LR: 2.2556e-05 | Elapse: 281.60s\n",
      "Epoch 22 [450/804] | Train Loss: 0.7066 Grad: 253832.3594 LR: 2.2254e-05 | Elapse: 316.63s\n",
      "Epoch 22 [500/804] | Train Loss: 0.7072 Grad: 193642.7969 LR: 2.1954e-05 | Elapse: 351.58s\n",
      "Epoch 22 [550/804] | Train Loss: 0.7047 Grad: 164392.2656 LR: 2.1656e-05 | Elapse: 386.78s\n",
      "Epoch 22 [600/804] | Train Loss: 0.7080 Grad: 160472.3438 LR: 2.1359e-05 | Elapse: 421.96s\n",
      "Epoch 22 [650/804] | Train Loss: 0.7080 Grad: 219422.1250 LR: 2.1063e-05 | Elapse: 457.09s\n",
      "Epoch 22 [700/804] | Train Loss: 0.7066 Grad: 152826.0781 LR: 2.0769e-05 | Elapse: 492.09s\n",
      "Epoch 22 [750/804] | Train Loss: 0.7078 Grad: 147381.7812 LR: 2.0477e-05 | Elapse: 527.02s\n",
      "Epoch 22 [800/804] | Train Loss: 0.7071 Grad: 152185.4062 LR: 2.0186e-05 | Elapse: 561.85s\n",
      "Epoch 22 [803/804] | Train Loss: 0.7074 Grad: 219381.7188 LR: 2.0168e-05 | Elapse: 563.94s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d9dcfe5266408392cc1ced2ac42a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [21]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 [0/458] | Valid Loss: 0.6383 | Elapse: 0.34s\n",
      "Epoch 22 [50/458] | Valid Loss: 0.6805 | Elapse: 17.19s\n",
      "Epoch 22 [100/458] | Valid Loss: 0.7077 | Elapse: 33.98s\n",
      "Epoch 22 [150/458] | Valid Loss: 0.7002 | Elapse: 50.81s\n",
      "Epoch 22 [200/458] | Valid Loss: 0.7109 | Elapse: 67.66s\n",
      "Epoch 22 [250/458] | Valid Loss: 0.7132 | Elapse: 84.37s\n",
      "Epoch 22 [300/458] | Valid Loss: 0.7199 | Elapse: 101.03s\n",
      "Epoch 22 [350/458] | Valid Loss: 0.7198 | Elapse: 117.69s\n",
      "Epoch 22 [400/458] | Valid Loss: 0.7254 | Elapse: 134.34s\n",
      "Epoch 22 [450/458] | Valid Loss: 0.7261 | Elapse: 151.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 22 - Average Loss: (train) 0.7074; (valid) 0.7262 | Time: 717.13s\n",
      "Best model found in epoch 22 | valid loss: 0.7262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 [457/458] | Valid Loss: 0.7262 | Elapse: 153.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6039f5414335491f813ad2184634db79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [22]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 [0/804] | Train Loss: 0.6570 Grad: 345400.0000 LR: 2.0162e-05 | Elapse: 0.70s\n",
      "Epoch 23 [50/804] | Train Loss: 0.6839 Grad: 197586.5781 LR: 1.9873e-05 | Elapse: 35.46s\n",
      "Epoch 23 [100/804] | Train Loss: 0.6924 Grad: 187371.0000 LR: 1.9585e-05 | Elapse: 70.29s\n",
      "Epoch 23 [150/804] | Train Loss: 0.6884 Grad: 226868.9688 LR: 1.9299e-05 | Elapse: 105.21s\n",
      "Epoch 23 [200/804] | Train Loss: 0.6969 Grad: 167121.9062 LR: 1.9015e-05 | Elapse: 140.05s\n",
      "Epoch 23 [250/804] | Train Loss: 0.6992 Grad: 211136.7031 LR: 1.8732e-05 | Elapse: 174.88s\n",
      "Epoch 23 [300/804] | Train Loss: 0.7001 Grad: 139513.8750 LR: 1.8451e-05 | Elapse: 209.80s\n",
      "Epoch 23 [350/804] | Train Loss: 0.7003 Grad: 98058.0234 LR: 1.8171e-05 | Elapse: 244.70s\n",
      "Epoch 23 [400/804] | Train Loss: 0.7023 Grad: 183525.1094 LR: 1.7893e-05 | Elapse: 279.46s\n",
      "Epoch 23 [450/804] | Train Loss: 0.7025 Grad: 256311.1875 LR: 1.7617e-05 | Elapse: 314.29s\n",
      "Epoch 23 [500/804] | Train Loss: 0.7030 Grad: 196299.4062 LR: 1.7343e-05 | Elapse: 349.11s\n",
      "Epoch 23 [550/804] | Train Loss: 0.7005 Grad: 165876.7500 LR: 1.7070e-05 | Elapse: 384.02s\n",
      "Epoch 23 [600/804] | Train Loss: 0.7038 Grad: 158988.3281 LR: 1.6799e-05 | Elapse: 418.98s\n",
      "Epoch 23 [650/804] | Train Loss: 0.7038 Grad: 221507.9219 LR: 1.6530e-05 | Elapse: 453.86s\n",
      "Epoch 23 [700/804] | Train Loss: 0.7025 Grad: 157700.6875 LR: 1.6262e-05 | Elapse: 488.77s\n",
      "Epoch 23 [750/804] | Train Loss: 0.7038 Grad: 148498.0625 LR: 1.5996e-05 | Elapse: 523.78s\n",
      "Epoch 23 [800/804] | Train Loss: 0.7030 Grad: 150561.3594 LR: 1.5732e-05 | Elapse: 558.81s\n",
      "Epoch 23 [803/804] | Train Loss: 0.7033 Grad: 222298.8438 LR: 1.5716e-05 | Elapse: 560.89s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8b799a79884a9d96b384c6d5faf504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [22]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 [0/458] | Valid Loss: 0.6369 | Elapse: 0.34s\n",
      "Epoch 23 [50/458] | Valid Loss: 0.6793 | Elapse: 17.20s\n",
      "Epoch 23 [100/458] | Valid Loss: 0.7064 | Elapse: 34.02s\n",
      "Epoch 23 [150/458] | Valid Loss: 0.6990 | Elapse: 50.93s\n",
      "Epoch 23 [200/458] | Valid Loss: 0.7101 | Elapse: 67.82s\n",
      "Epoch 23 [250/458] | Valid Loss: 0.7126 | Elapse: 84.59s\n",
      "Epoch 23 [300/458] | Valid Loss: 0.7194 | Elapse: 101.36s\n",
      "Epoch 23 [350/458] | Valid Loss: 0.7196 | Elapse: 118.14s\n",
      "Epoch 23 [400/458] | Valid Loss: 0.7253 | Elapse: 134.95s\n",
      "Epoch 23 [450/458] | Valid Loss: 0.7261 | Elapse: 151.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 23 - Average Loss: (train) 0.7033; (valid) 0.7262 | Time: 714.74s\n",
      "Best model found in epoch 23 | valid loss: 0.7262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 [457/458] | Valid Loss: 0.7262 | Elapse: 153.85s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44199e362c16488d83bf17f5da6b7e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [23]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 [0/804] | Train Loss: 0.6568 Grad: 349560.1250 LR: 1.5711e-05 | Elapse: 0.70s\n",
      "Epoch 24 [50/804] | Train Loss: 0.6809 Grad: 196467.9844 LR: 1.5449e-05 | Elapse: 35.51s\n",
      "Epoch 24 [100/804] | Train Loss: 0.6894 Grad: 188746.8906 LR: 1.5189e-05 | Elapse: 70.46s\n",
      "Epoch 24 [150/804] | Train Loss: 0.6849 Grad: 221922.5469 LR: 1.4930e-05 | Elapse: 105.38s\n",
      "Epoch 24 [200/804] | Train Loss: 0.6932 Grad: 174127.2656 LR: 1.4674e-05 | Elapse: 140.39s\n",
      "Epoch 24 [250/804] | Train Loss: 0.6957 Grad: 214003.4688 LR: 1.4419e-05 | Elapse: 175.36s\n",
      "Epoch 24 [300/804] | Train Loss: 0.6967 Grad: 134336.6250 LR: 1.4166e-05 | Elapse: 210.45s\n",
      "Epoch 24 [350/804] | Train Loss: 0.6967 Grad: 100309.6406 LR: 1.3915e-05 | Elapse: 245.57s\n",
      "Epoch 24 [400/804] | Train Loss: 0.6988 Grad: 184510.9375 LR: 1.3666e-05 | Elapse: 280.57s\n",
      "Epoch 24 [450/804] | Train Loss: 0.6991 Grad: 259821.4531 LR: 1.3419e-05 | Elapse: 315.64s\n",
      "Epoch 24 [500/804] | Train Loss: 0.6994 Grad: 198094.5469 LR: 1.3173e-05 | Elapse: 350.61s\n",
      "Epoch 24 [550/804] | Train Loss: 0.6969 Grad: 166964.6406 LR: 1.2930e-05 | Elapse: 385.68s\n",
      "Epoch 24 [600/804] | Train Loss: 0.7003 Grad: 157149.2031 LR: 1.2688e-05 | Elapse: 420.71s\n",
      "Epoch 24 [650/804] | Train Loss: 0.7004 Grad: 226359.1250 LR: 1.2449e-05 | Elapse: 455.76s\n",
      "Epoch 24 [700/804] | Train Loss: 0.6991 Grad: 161063.3906 LR: 1.2211e-05 | Elapse: 490.71s\n",
      "Epoch 24 [750/804] | Train Loss: 0.7004 Grad: 152356.1562 LR: 1.1976e-05 | Elapse: 525.64s\n",
      "Epoch 24 [800/804] | Train Loss: 0.6996 Grad: 146129.4844 LR: 1.1742e-05 | Elapse: 560.64s\n",
      "Epoch 24 [803/804] | Train Loss: 0.7000 Grad: 226412.3125 LR: 1.1728e-05 | Elapse: 562.73s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c96ffb67d664b01994376ed42294d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [23]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 [0/458] | Valid Loss: 0.6372 | Elapse: 0.35s\n",
      "Epoch 24 [50/458] | Valid Loss: 0.6793 | Elapse: 17.37s\n",
      "Epoch 24 [100/458] | Valid Loss: 0.7061 | Elapse: 34.33s\n",
      "Epoch 24 [150/458] | Valid Loss: 0.6986 | Elapse: 51.30s\n",
      "Epoch 24 [200/458] | Valid Loss: 0.7100 | Elapse: 68.25s\n",
      "Epoch 24 [250/458] | Valid Loss: 0.7129 | Elapse: 85.18s\n",
      "Epoch 24 [300/458] | Valid Loss: 0.7198 | Elapse: 102.13s\n",
      "Epoch 24 [350/458] | Valid Loss: 0.7204 | Elapse: 119.05s\n",
      "Epoch 24 [400/458] | Valid Loss: 0.7263 | Elapse: 135.92s\n",
      "Epoch 24 [450/458] | Valid Loss: 0.7272 | Elapse: 152.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 24 - Average Loss: (train) 0.7000; (valid) 0.7273 | Time: 717.71s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 [457/458] | Valid Loss: 0.7273 | Elapse: 154.97s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218246b333ed415e9458df33fa26428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [24]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 [0/804] | Train Loss: 0.6544 Grad: 347606.2500 LR: 1.1724e-05 | Elapse: 0.70s\n",
      "Epoch 25 [50/804] | Train Loss: 0.6782 Grad: 193906.4062 LR: 1.1492e-05 | Elapse: 35.65s\n",
      "Epoch 25 [100/804] | Train Loss: 0.6866 Grad: 188460.9688 LR: 1.1263e-05 | Elapse: 70.68s\n",
      "Epoch 25 [150/804] | Train Loss: 0.6818 Grad: 217934.5312 LR: 1.1036e-05 | Elapse: 105.70s\n",
      "Epoch 25 [200/804] | Train Loss: 0.6900 Grad: 182103.7500 LR: 1.0810e-05 | Elapse: 140.56s\n",
      "Epoch 25 [250/804] | Train Loss: 0.6926 Grad: 218597.0156 LR: 1.0587e-05 | Elapse: 175.44s\n",
      "Epoch 25 [300/804] | Train Loss: 0.6937 Grad: 130322.5391 LR: 1.0366e-05 | Elapse: 210.45s\n",
      "Epoch 25 [350/804] | Train Loss: 0.6935 Grad: 102848.4297 LR: 1.0147e-05 | Elapse: 245.49s\n",
      "Epoch 25 [400/804] | Train Loss: 0.6958 Grad: 187759.8906 LR: 9.9299e-06 | Elapse: 280.46s\n",
      "Epoch 25 [450/804] | Train Loss: 0.6961 Grad: 264424.7188 LR: 9.7150e-06 | Elapse: 315.30s\n",
      "Epoch 25 [500/804] | Train Loss: 0.6964 Grad: 196561.5156 LR: 9.5022e-06 | Elapse: 350.08s\n",
      "Epoch 25 [550/804] | Train Loss: 0.6939 Grad: 168715.0938 LR: 9.2915e-06 | Elapse: 384.92s\n",
      "Epoch 25 [600/804] | Train Loss: 0.6973 Grad: 152821.6562 LR: 9.0829e-06 | Elapse: 419.88s\n",
      "Epoch 25 [650/804] | Train Loss: 0.6976 Grad: 234077.2812 LR: 8.8765e-06 | Elapse: 454.76s\n",
      "Epoch 25 [700/804] | Train Loss: 0.6963 Grad: 163412.0156 LR: 8.6723e-06 | Elapse: 489.64s\n",
      "Epoch 25 [750/804] | Train Loss: 0.6975 Grad: 156774.4531 LR: 8.4702e-06 | Elapse: 524.61s\n",
      "Epoch 25 [800/804] | Train Loss: 0.6968 Grad: 142845.6250 LR: 8.2703e-06 | Elapse: 559.50s\n",
      "Epoch 25 [803/804] | Train Loss: 0.6971 Grad: 225856.0625 LR: 8.2583e-06 | Elapse: 561.60s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9613ca4cb741a48a87cfdc4615bb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [24]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 [0/458] | Valid Loss: 0.6383 | Elapse: 0.35s\n",
      "Epoch 25 [50/458] | Valid Loss: 0.6772 | Elapse: 17.32s\n",
      "Epoch 25 [100/458] | Valid Loss: 0.7035 | Elapse: 34.20s\n",
      "Epoch 25 [150/458] | Valid Loss: 0.6958 | Elapse: 51.01s\n",
      "Epoch 25 [200/458] | Valid Loss: 0.7073 | Elapse: 67.79s\n",
      "Epoch 25 [250/458] | Valid Loss: 0.7100 | Elapse: 84.49s\n",
      "Epoch 25 [300/458] | Valid Loss: 0.7170 | Elapse: 101.16s\n",
      "Epoch 25 [350/458] | Valid Loss: 0.7176 | Elapse: 117.87s\n",
      "Epoch 25 [400/458] | Valid Loss: 0.7235 | Elapse: 134.58s\n",
      "Epoch 25 [450/458] | Valid Loss: 0.7244 | Elapse: 151.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 25 - Average Loss: (train) 0.6971; (valid) 0.7244 | Time: 715.03s\n",
      "Best model found in epoch 25 | valid loss: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 [457/458] | Valid Loss: 0.7244 | Elapse: 153.43s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122f897ce7f541f182408637793782e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [25]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 [0/804] | Train Loss: 0.6510 Grad: 340453.9375 LR: 8.2544e-06 | Elapse: 0.70s\n",
      "Epoch 26 [50/804] | Train Loss: 0.6755 Grad: 193190.1094 LR: 8.0568e-06 | Elapse: 35.50s\n",
      "Epoch 26 [100/804] | Train Loss: 0.6842 Grad: 185847.8906 LR: 7.8615e-06 | Elapse: 70.42s\n",
      "Epoch 26 [150/804] | Train Loss: 0.6792 Grad: 214900.6719 LR: 7.6683e-06 | Elapse: 105.27s\n",
      "Epoch 26 [200/804] | Train Loss: 0.6873 Grad: 189682.1406 LR: 7.4774e-06 | Elapse: 140.21s\n",
      "Epoch 26 [250/804] | Train Loss: 0.6901 Grad: 224027.4844 LR: 7.2887e-06 | Elapse: 175.12s\n",
      "Epoch 26 [300/804] | Train Loss: 0.6911 Grad: 127760.2500 LR: 7.1022e-06 | Elapse: 210.17s\n",
      "Epoch 26 [350/804] | Train Loss: 0.6908 Grad: 105118.6328 LR: 6.9180e-06 | Elapse: 245.11s\n",
      "Epoch 26 [400/804] | Train Loss: 0.6933 Grad: 190497.7812 LR: 6.7361e-06 | Elapse: 280.09s\n",
      "Epoch 26 [450/804] | Train Loss: 0.6937 Grad: 267509.3125 LR: 6.5564e-06 | Elapse: 315.07s\n",
      "Epoch 26 [500/804] | Train Loss: 0.6940 Grad: 191635.1875 LR: 6.3789e-06 | Elapse: 349.93s\n",
      "Epoch 26 [550/804] | Train Loss: 0.6914 Grad: 171241.2969 LR: 6.2038e-06 | Elapse: 384.85s\n",
      "Epoch 26 [600/804] | Train Loss: 0.6949 Grad: 148593.7656 LR: 6.0310e-06 | Elapse: 419.80s\n",
      "Epoch 26 [650/804] | Train Loss: 0.6953 Grad: 240222.3906 LR: 5.8604e-06 | Elapse: 454.64s\n",
      "Epoch 26 [700/804] | Train Loss: 0.6940 Grad: 165201.6562 LR: 5.6922e-06 | Elapse: 489.47s\n",
      "Epoch 26 [750/804] | Train Loss: 0.6951 Grad: 159680.7031 LR: 5.5263e-06 | Elapse: 524.36s\n",
      "Epoch 26 [800/804] | Train Loss: 0.6945 Grad: 142051.8750 LR: 5.3627e-06 | Elapse: 559.29s\n",
      "Epoch 26 [803/804] | Train Loss: 0.6948 Grad: 220890.4219 LR: 5.3530e-06 | Elapse: 561.37s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958ad9fd63a44f5e9348424bedff8491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [25]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 [0/458] | Valid Loss: 0.6402 | Elapse: 0.34s\n",
      "Epoch 26 [50/458] | Valid Loss: 0.6731 | Elapse: 17.30s\n",
      "Epoch 26 [100/458] | Valid Loss: 0.6986 | Elapse: 34.33s\n",
      "Epoch 26 [150/458] | Valid Loss: 0.6907 | Elapse: 51.30s\n",
      "Epoch 26 [200/458] | Valid Loss: 0.7021 | Elapse: 68.18s\n",
      "Epoch 26 [250/458] | Valid Loss: 0.7042 | Elapse: 84.99s\n",
      "Epoch 26 [300/458] | Valid Loss: 0.7107 | Elapse: 101.80s\n",
      "Epoch 26 [350/458] | Valid Loss: 0.7112 | Elapse: 118.64s\n",
      "Epoch 26 [400/458] | Valid Loss: 0.7167 | Elapse: 135.50s\n",
      "Epoch 26 [450/458] | Valid Loss: 0.7173 | Elapse: 152.36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 26 - Average Loss: (train) 0.6948; (valid) 0.7174 | Time: 715.87s\n",
      "Best model found in epoch 26 | valid loss: 0.7174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 [457/458] | Valid Loss: 0.7174 | Elapse: 154.49s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef4418aca2c454b9a71a87413049c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [26]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 [0/804] | Train Loss: 0.6464 Grad: 331138.9062 LR: 5.3498e-06 | Elapse: 0.70s\n",
      "Epoch 27 [50/804] | Train Loss: 0.6728 Grad: 193464.6875 LR: 5.1887e-06 | Elapse: 35.57s\n",
      "Epoch 27 [100/804] | Train Loss: 0.6821 Grad: 181791.4219 LR: 5.0300e-06 | Elapse: 70.39s\n",
      "Epoch 27 [150/804] | Train Loss: 0.6770 Grad: 212400.0000 LR: 4.8737e-06 | Elapse: 105.18s\n",
      "Epoch 27 [200/804] | Train Loss: 0.6851 Grad: 195069.0781 LR: 4.7197e-06 | Elapse: 140.00s\n",
      "Epoch 27 [250/804] | Train Loss: 0.6881 Grad: 228827.3438 LR: 4.5681e-06 | Elapse: 174.82s\n",
      "Epoch 27 [300/804] | Train Loss: 0.6892 Grad: 125962.1172 LR: 4.4188e-06 | Elapse: 209.66s\n",
      "Epoch 27 [350/804] | Train Loss: 0.6887 Grad: 105884.8828 LR: 4.2720e-06 | Elapse: 244.50s\n",
      "Epoch 27 [400/804] | Train Loss: 0.6913 Grad: 191263.9531 LR: 4.1276e-06 | Elapse: 279.32s\n",
      "Epoch 27 [450/804] | Train Loss: 0.6919 Grad: 268387.3750 LR: 3.9855e-06 | Elapse: 314.15s\n",
      "Epoch 27 [500/804] | Train Loss: 0.6921 Grad: 184657.5781 LR: 3.8459e-06 | Elapse: 348.94s\n",
      "Epoch 27 [550/804] | Train Loss: 0.6895 Grad: 172857.3594 LR: 3.7087e-06 | Elapse: 383.80s\n",
      "Epoch 27 [600/804] | Train Loss: 0.6930 Grad: 145803.8125 LR: 3.5739e-06 | Elapse: 418.59s\n",
      "Epoch 27 [650/804] | Train Loss: 0.6934 Grad: 240629.5781 LR: 3.4415e-06 | Elapse: 453.46s\n",
      "Epoch 27 [700/804] | Train Loss: 0.6921 Grad: 167479.4531 LR: 3.3116e-06 | Elapse: 488.33s\n",
      "Epoch 27 [750/804] | Train Loss: 0.6932 Grad: 159885.4375 LR: 3.1841e-06 | Elapse: 523.15s\n",
      "Epoch 27 [800/804] | Train Loss: 0.6927 Grad: 144923.3594 LR: 3.0591e-06 | Elapse: 558.00s\n",
      "Epoch 27 [803/804] | Train Loss: 0.6930 Grad: 215116.0938 LR: 3.0517e-06 | Elapse: 560.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27ed818f154451bb4c012a63c75fd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [26]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 [0/458] | Valid Loss: 0.6443 | Elapse: 0.34s\n",
      "Epoch 27 [50/458] | Valid Loss: 0.6696 | Elapse: 17.24s\n",
      "Epoch 27 [100/458] | Valid Loss: 0.6943 | Elapse: 34.11s\n",
      "Epoch 27 [150/458] | Valid Loss: 0.6860 | Elapse: 50.89s\n",
      "Epoch 27 [200/458] | Valid Loss: 0.6973 | Elapse: 67.63s\n",
      "Epoch 27 [250/458] | Valid Loss: 0.6986 | Elapse: 84.22s\n",
      "Epoch 27 [300/458] | Valid Loss: 0.7047 | Elapse: 100.79s\n",
      "Epoch 27 [350/458] | Valid Loss: 0.7048 | Elapse: 117.33s\n",
      "Epoch 27 [400/458] | Valid Loss: 0.7099 | Elapse: 133.95s\n",
      "Epoch 27 [450/458] | Valid Loss: 0.7103 | Elapse: 150.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 27 - Average Loss: (train) 0.6930; (valid) 0.7105 | Time: 712.80s\n",
      "Best model found in epoch 27 | valid loss: 0.7105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 [457/458] | Valid Loss: 0.7105 | Elapse: 152.70s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f7447a7ddb4ed2b98bda2eb3488b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [27]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 [0/804] | Train Loss: 0.6391 Grad: 317522.0938 LR: 3.0492e-06 | Elapse: 0.70s\n",
      "Epoch 28 [50/804] | Train Loss: 0.6700 Grad: 192464.4688 LR: 2.9269e-06 | Elapse: 35.43s\n",
      "Epoch 28 [100/804] | Train Loss: 0.6805 Grad: 175588.8281 LR: 2.8070e-06 | Elapse: 70.22s\n",
      "Epoch 28 [150/804] | Train Loss: 0.6754 Grad: 209386.7812 LR: 2.6895e-06 | Elapse: 104.99s\n",
      "Epoch 28 [200/804] | Train Loss: 0.6836 Grad: 197465.9062 LR: 2.5746e-06 | Elapse: 139.79s\n",
      "Epoch 28 [250/804] | Train Loss: 0.6866 Grad: 231108.4375 LR: 2.4621e-06 | Elapse: 174.59s\n",
      "Epoch 28 [300/804] | Train Loss: 0.6878 Grad: 125291.1250 LR: 2.3521e-06 | Elapse: 209.39s\n",
      "Epoch 28 [350/804] | Train Loss: 0.6873 Grad: 105107.7891 LR: 2.2447e-06 | Elapse: 244.26s\n",
      "Epoch 28 [400/804] | Train Loss: 0.6900 Grad: 192290.5000 LR: 2.1397e-06 | Elapse: 279.09s\n",
      "Epoch 28 [450/804] | Train Loss: 0.6905 Grad: 269775.3125 LR: 2.0372e-06 | Elapse: 313.92s\n",
      "Epoch 28 [500/804] | Train Loss: 0.6908 Grad: 178688.5156 LR: 1.9373e-06 | Elapse: 348.71s\n",
      "Epoch 28 [550/804] | Train Loss: 0.6881 Grad: 171932.1719 LR: 1.8398e-06 | Elapse: 383.51s\n",
      "Epoch 28 [600/804] | Train Loss: 0.6916 Grad: 144765.9375 LR: 1.7449e-06 | Elapse: 418.36s\n",
      "Epoch 28 [650/804] | Train Loss: 0.6920 Grad: 236967.7188 LR: 1.6525e-06 | Elapse: 453.23s\n",
      "Epoch 28 [700/804] | Train Loss: 0.6908 Grad: 170490.4219 LR: 1.5627e-06 | Elapse: 488.13s\n",
      "Epoch 28 [750/804] | Train Loss: 0.6918 Grad: 158570.6406 LR: 1.4753e-06 | Elapse: 523.12s\n",
      "Epoch 28 [800/804] | Train Loss: 0.6914 Grad: 148434.4219 LR: 1.3906e-06 | Elapse: 557.98s\n",
      "Epoch 28 [803/804] | Train Loss: 0.6917 Grad: 212119.4375 LR: 1.3855e-06 | Elapse: 560.07s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f49f04a4443ab88910407428700a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [27]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 [0/458] | Valid Loss: 0.6483 | Elapse: 0.34s\n",
      "Epoch 28 [50/458] | Valid Loss: 0.6686 | Elapse: 17.16s\n",
      "Epoch 28 [100/458] | Valid Loss: 0.6926 | Elapse: 34.01s\n",
      "Epoch 28 [150/458] | Valid Loss: 0.6841 | Elapse: 50.86s\n",
      "Epoch 28 [200/458] | Valid Loss: 0.6953 | Elapse: 67.70s\n",
      "Epoch 28 [250/458] | Valid Loss: 0.6961 | Elapse: 84.43s\n",
      "Epoch 28 [300/458] | Valid Loss: 0.7022 | Elapse: 101.15s\n",
      "Epoch 28 [350/458] | Valid Loss: 0.7022 | Elapse: 117.85s\n",
      "Epoch 28 [400/458] | Valid Loss: 0.7070 | Elapse: 134.53s\n",
      "Epoch 28 [450/458] | Valid Loss: 0.7074 | Elapse: 151.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 28 - Average Loss: (train) 0.6917; (valid) 0.7075 | Time: 713.38s\n",
      "Best model found in epoch 28 | valid loss: 0.7075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 [457/458] | Valid Loss: 0.7075 | Elapse: 153.30s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05099d1085f428b89a84ec9083d50b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [28]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 [0/804] | Train Loss: 0.6314 Grad: 303125.3438 LR: 1.3839e-06 | Elapse: 0.70s\n",
      "Epoch 29 [50/804] | Train Loss: 0.6675 Grad: 188419.8438 LR: 1.3019e-06 | Elapse: 35.52s\n",
      "Epoch 29 [100/804] | Train Loss: 0.6791 Grad: 171492.5312 LR: 1.2224e-06 | Elapse: 70.38s\n",
      "Epoch 29 [150/804] | Train Loss: 0.6743 Grad: 206758.5000 LR: 1.1455e-06 | Elapse: 105.24s\n",
      "Epoch 29 [200/804] | Train Loss: 0.6826 Grad: 198467.9688 LR: 1.0711e-06 | Elapse: 140.21s\n",
      "Epoch 29 [250/804] | Train Loss: 0.6856 Grad: 230126.5000 LR: 9.9929e-07 | Elapse: 175.11s\n",
      "Epoch 29 [300/804] | Train Loss: 0.6867 Grad: 126600.6172 LR: 9.3006e-07 | Elapse: 210.01s\n",
      "Epoch 29 [350/804] | Train Loss: 0.6863 Grad: 103280.1797 LR: 8.6340e-07 | Elapse: 244.93s\n",
      "Epoch 29 [400/804] | Train Loss: 0.6890 Grad: 192869.2500 LR: 7.9931e-07 | Elapse: 279.94s\n",
      "Epoch 29 [450/804] | Train Loss: 0.6896 Grad: 269772.0312 LR: 7.3780e-07 | Elapse: 314.94s\n",
      "Epoch 29 [500/804] | Train Loss: 0.6898 Grad: 175426.8594 LR: 6.7887e-07 | Elapse: 349.78s\n",
      "Epoch 29 [550/804] | Train Loss: 0.6872 Grad: 169741.6094 LR: 6.2252e-07 | Elapse: 384.71s\n",
      "Epoch 29 [600/804] | Train Loss: 0.6907 Grad: 144321.0781 LR: 5.6876e-07 | Elapse: 419.59s\n",
      "Epoch 29 [650/804] | Train Loss: 0.6911 Grad: 230356.3125 LR: 5.1758e-07 | Elapse: 454.53s\n",
      "Epoch 29 [700/804] | Train Loss: 0.6898 Grad: 172351.6875 LR: 4.6900e-07 | Elapse: 489.40s\n",
      "Epoch 29 [750/804] | Train Loss: 0.6909 Grad: 157982.3281 LR: 4.2302e-07 | Elapse: 524.36s\n",
      "Epoch 29 [800/804] | Train Loss: 0.6905 Grad: 151149.9688 LR: 3.7963e-07 | Elapse: 559.23s\n",
      "Epoch 29 [803/804] | Train Loss: 0.6908 Grad: 210819.5000 LR: 3.7711e-07 | Elapse: 561.32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ae05eff58c49dab6e26e2e3226f3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [28]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 [0/458] | Valid Loss: 0.6514 | Elapse: 0.34s\n",
      "Epoch 29 [50/458] | Valid Loss: 0.6689 | Elapse: 17.22s\n",
      "Epoch 29 [100/458] | Valid Loss: 0.6924 | Elapse: 34.11s\n",
      "Epoch 29 [150/458] | Valid Loss: 0.6838 | Elapse: 51.02s\n",
      "Epoch 29 [200/458] | Valid Loss: 0.6949 | Elapse: 67.96s\n",
      "Epoch 29 [250/458] | Valid Loss: 0.6956 | Elapse: 84.77s\n",
      "Epoch 29 [300/458] | Valid Loss: 0.7017 | Elapse: 101.55s\n",
      "Epoch 29 [350/458] | Valid Loss: 0.7017 | Elapse: 118.27s\n",
      "Epoch 29 [400/458] | Valid Loss: 0.7064 | Elapse: 134.98s\n",
      "Epoch 29 [450/458] | Valid Loss: 0.7069 | Elapse: 151.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 29 - Average Loss: (train) 0.6908; (valid) 0.7070 | Time: 715.16s\n",
      "Best model found in epoch 29 | valid loss: 0.7070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 [457/458] | Valid Loss: 0.7070 | Elapse: 153.84s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949742651d4f48158adf3adbe008e4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [29]:   0%|          | 0/804 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 [0/804] | Train Loss: 0.6255 Grad: 291766.1250 LR: 3.7627e-07 | Elapse: 0.70s\n",
      "Epoch 30 [50/804] | Train Loss: 0.6662 Grad: 184776.4375 LR: 3.3569e-07 | Elapse: 35.48s\n",
      "Epoch 30 [100/804] | Train Loss: 0.6780 Grad: 171704.9375 LR: 2.9771e-07 | Elapse: 70.45s\n",
      "Epoch 30 [150/804] | Train Loss: 0.6735 Grad: 206622.5938 LR: 2.6233e-07 | Elapse: 105.40s\n",
      "Epoch 30 [200/804] | Train Loss: 0.6817 Grad: 196601.2344 LR: 2.2956e-07 | Elapse: 140.25s\n",
      "Epoch 30 [250/804] | Train Loss: 0.6848 Grad: 228956.2969 LR: 1.9939e-07 | Elapse: 175.10s\n",
      "Epoch 30 [300/804] | Train Loss: 0.6860 Grad: 127880.5078 LR: 1.7184e-07 | Elapse: 210.02s\n",
      "Epoch 30 [350/804] | Train Loss: 0.6857 Grad: 101419.5469 LR: 1.4689e-07 | Elapse: 245.01s\n",
      "Epoch 30 [400/804] | Train Loss: 0.6883 Grad: 190295.9219 LR: 1.2456e-07 | Elapse: 279.88s\n",
      "Epoch 30 [450/804] | Train Loss: 0.6889 Grad: 270342.5938 LR: 1.0484e-07 | Elapse: 314.76s\n",
      "Epoch 30 [500/804] | Train Loss: 0.6892 Grad: 173127.2969 LR: 8.7728e-08 | Elapse: 349.64s\n",
      "Epoch 30 [550/804] | Train Loss: 0.6865 Grad: 167456.4844 LR: 7.3234e-08 | Elapse: 384.51s\n",
      "Epoch 30 [600/804] | Train Loss: 0.6900 Grad: 146447.5625 LR: 6.1355e-08 | Elapse: 419.41s\n",
      "Epoch 30 [650/804] | Train Loss: 0.6904 Grad: 229644.0469 LR: 5.2092e-08 | Elapse: 454.35s\n",
      "Epoch 30 [700/804] | Train Loss: 0.6892 Grad: 172934.0938 LR: 4.5445e-08 | Elapse: 489.28s\n",
      "Epoch 30 [750/804] | Train Loss: 0.6903 Grad: 158347.3438 LR: 4.1415e-08 | Elapse: 524.11s\n",
      "Epoch 30 [800/804] | Train Loss: 0.6899 Grad: 151624.9219 LR: 4.0002e-08 | Elapse: 558.99s\n",
      "Epoch 30 [803/804] | Train Loss: 0.6902 Grad: 210720.4375 LR: 4.0001e-08 | Elapse: 561.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d083b2f97084436a83d3f12ec81647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid [29]:   0%|          | 0/458 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 [0/458] | Valid Loss: 0.6520 | Elapse: 0.34s\n",
      "Epoch 30 [50/458] | Valid Loss: 0.6691 | Elapse: 17.26s\n",
      "Epoch 30 [100/458] | Valid Loss: 0.6925 | Elapse: 34.13s\n",
      "Epoch 30 [150/458] | Valid Loss: 0.6838 | Elapse: 51.08s\n",
      "Epoch 30 [200/458] | Valid Loss: 0.6949 | Elapse: 67.98s\n",
      "Epoch 30 [250/458] | Valid Loss: 0.6957 | Elapse: 84.77s\n",
      "Epoch 30 [300/458] | Valid Loss: 0.7018 | Elapse: 101.53s\n",
      "Epoch 30 [350/458] | Valid Loss: 0.7018 | Elapse: 118.29s\n",
      "Epoch 30 [400/458] | Valid Loss: 0.7065 | Elapse: 135.05s\n",
      "Epoch 30 [450/458] | Valid Loss: 0.7069 | Elapse: 151.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 30 - Average Loss: (train) 0.6902; (valid) 0.7071 | Time: 715.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 [457/458] | Valid Loss: 0.7071 | Elapse: 153.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Fold 0 Valid Loss: 0.7070181369781494\n",
      "Elapse: 358.36 min \n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Fold: 1 || Valid: 4196; \n",
      "====================================================================================================\n",
      "- Train: 15987; Epoch = 30; Dropout = 0.0 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9589cced11049c0b3a8948f15bd2146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train [0]:   0%|          | 0/999 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/999] | Train Loss: 1.5677 Grad: 315611.3750 LR: 4.0000e-06 | Elapse: 0.72s\n",
      "Epoch 1 [50/999] | Train Loss: 1.5018 Grad: 212703.7812 LR: 4.0686e-06 | Elapse: 35.47s\n",
      "Epoch 1 [100/999] | Train Loss: 1.4641 Grad: 212747.8281 LR: 4.2689e-06 | Elapse: 70.21s\n",
      "Epoch 1 [150/999] | Train Loss: 1.4339 Grad: 225451.8438 LR: 4.6004e-06 | Elapse: 104.92s\n"
     ]
    }
   ],
   "source": [
    "# Major Train Loop\n",
    "# ================== Logger ==================\n",
    "logger.info(f\"{'*' * 100}\")\n",
    "logger.info(f\"Script Start: {ctime()}\")\n",
    "logger.info(f\"Model Configurations:\")\n",
    "for key, value in ModelConfig.__dict__.items():\n",
    "    if not key.startswith(\"__\"):\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "logger.info(f\"{'*' * 100}\")\n",
    "\n",
    "# ================== Run Training ==================\n",
    "oof_stage_1 = pd.DataFrame()\n",
    "loss_history_1 = []\n",
    "\n",
    "logger.info(f\"{'=' * 100}\\nStage 1: Train ResNetGRU\\n{'=' * 100}\")\n",
    "for fold in range(k_folds):\n",
    "    tik = time()\n",
    "\n",
    "    ModelConfig.RESNET_GRU_DROPOUT = 0.0\n",
    "\n",
    "    # model = ResNetGRU( ModelConfig, num_classes=6 )\n",
    "    model = EEGSeqClassifier(ModelConfig, num_classes=6)\n",
    "    \n",
    "    valid_folds = train_all[train_all['fold'] == fold].reset_index(drop=True)\n",
    "    train_folds = train_all[train_all['fold'] != fold].reset_index(drop=True)\n",
    "\n",
    "    ## STAGE 1\n",
    "    logger.info(f\"{'=' * 100}\\nFold: {fold} || Valid: {valid_folds.shape[0]}; \\n{'=' * 100}\")\n",
    "    logger.info(f\"- Train: {train_folds.shape[0]}; Epoch = {ModelConfig.EPOCHS}; Dropout = {ModelConfig.RESNET_GRU_DROPOUT} -\")\n",
    "    valid_predicts, loss_records = train_fold(\n",
    "        model, fold, train_folds, valid_folds, logger, stage=1, checkpoint=None)\n",
    "\n",
    "    loss_history_1.append(loss_records)\n",
    "    valid_folds[TARGETS_PRED] = valid_predicts\n",
    "    oof_stage_1 = pd.concat([oof_stage_1, valid_folds], axis=0).reset_index(drop=True)\n",
    "    kl_loss_torch = evaluate_oof(valid_folds)\n",
    "    info = f\"{'=' * 100}\\nFold {fold} Valid Loss: {kl_loss_torch}\\n\"\n",
    "    info += f\"Elapse: {(time() - tik) / 60:.2f} min \\n{'=' * 100}\"\n",
    "    logger.info(info)\n",
    "    oof_stage_1.to_csv(os.path.join(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_oof_1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for i, loss in enumerate(loss_history_1):\n",
    "    ax.plot(loss['train'], marker=\"*\", ls=\"-\", label=f\"Fold {i} Train\")\n",
    "    ax.plot(loss['valid'], marker=\"o\", ls=\":\", label=f\"Fold {i} Valid\")\n",
    "\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STAGE 2\n",
    "oof_stage_2 = pd.DataFrame()\n",
    "loss_history_2 = []\n",
    "\n",
    "k_folds = 5\n",
    "train_all = prepare_k_fold(train_hard, k_folds=k_folds)\n",
    "\n",
    "logger.info(f\"{'=' * 100}\\nStage 2: Train ResNetGRU\\n{'=' * 100}\")\n",
    "for fold in range(k_folds):\n",
    "    tik = time()\n",
    "    \n",
    "    ModelConfig.EPOCHS = 10\n",
    "    ModelConfig.RESNET_GRU_DROPOUT = 0.0\n",
    "\n",
    "    train_folds = train_hard[train_hard['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = train_hard[train_hard['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    logger.info(f\"- Hard: {train_folds.shape[0]}; Epoch: {ModelConfig.EPOCHS}; Dropout: {ModelConfig.RESNET_GRU_DROPOUT} -\")\n",
    "    check_point = os.path.join(\n",
    "        PATHS.OUTPUT_DIR,\n",
    "        f\"{ModelConfig.MODEL_NAME}_fold_{fold}_stage_1.pth\"\n",
    "    )\n",
    "    logger.info(f\"Use Checkpoint: {check_point.split('/')[-1]}\")\n",
    "\n",
    "    model = ResNetGRU(ModelConfig, num_classes=6)\n",
    "\n",
    "    valid_predicts, loss_records = train_fold(\n",
    "        model, fold, train_folds, valid_folds, logger, stage=2, checkpoint=check_point)\n",
    "\n",
    "    loss_history_2.append(loss_records)\n",
    "    valid_folds[TARGETS_PRED] = valid_predicts\n",
    "    oof_stage_2 = pd.concat([oof_stage_2, valid_folds], axis=0).reset_index(drop=True)\n",
    "    kl_loss_torch = evaluate_oof(valid_folds)\n",
    "    info = f\"{'=' * 100}\\nFold {fold} Valid Loss: {kl_loss_torch}\\n\"\n",
    "    info += f\"Elapse: {(time() - tik) / 60:.2f} min \\n{'=' * 100}\"\n",
    "    logger.info(info)\n",
    "    oof_stage_2.to_csv(os.path.join(PATHS.OUTPUT_DIR, f\"{ModelConfig.MODEL_NAME}_oof_2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for i, loss in enumerate(loss_history_2):\n",
    "    ax.plot(loss['train'], marker=\"*\", ls=\"-\", label=f\"Fold {i} Train\")\n",
    "    ax.plot(loss['valid'], marker=\"o\", ls=\":\", label=f\"Fold {i} Valid\")\n",
    "\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kl_divergence import score as kaggle_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "TARGET2ID = {'Seizure': 0, 'LPD': 1, 'GPD': 2, 'LRDA': 3, 'GRDA': 4, 'Other': 5}\n",
    "\n",
    "def calc_kaggle_score(oof_df):\n",
    "    submission_df = oof_df[['eeg_id']+TARGETS_PRED].copy()\n",
    "    submission_df.columns = ['eeg_id'] + TARGETS\n",
    "    solution_df = oof_df[['eeg_id']+TARGETS].copy()\n",
    "    return kaggle_score(solution_df, submission_df, 'eeg_id')\n",
    "\n",
    "def analyze_oof(oof_csv):\n",
    "\n",
    "    kl_criteria = nn.KLDivLoss(reduction='batchmean')\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    oof_df = pd.read_csv(oof_csv)\n",
    "    oof_df['target_pred'] = oof_df[TARGETS_PRED].apply(lambda x: np.argmax(x), axis=1)\n",
    "    oof_df['target_id'] = oof_df[TARGETS].apply(lambda x: np.argmax(x), axis=1)\n",
    "    \n",
    "    oof_df[\"kl_loss\"] = oof_df.apply(\n",
    "    lambda row: \n",
    "        kl_criteria(\n",
    "            F.log_softmax(\n",
    "                    torch.tensor(row[TARGETS_PRED].values.astype(np.float32)).unsqueeze(0)\n",
    "                , dim=1\n",
    "                ), \n",
    "            torch.tensor(row[TARGETS].values.astype(np.float32))\n",
    "            ).numpy(),\n",
    "    axis=1)\n",
    "\n",
    "    oof_df[\"kl_loss\"] = oof_df['kl_loss'].astype(np.float32)\n",
    "\n",
    "    oof_df[TARGETS_PRED] = softmax( torch.tensor(oof_df[TARGETS_PRED].values.astype(np.float32)) )\n",
    "\n",
    "    oof_df.head()\n",
    "\n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Path:  ./outputs/EfficientNet_b2_two_stage_oof_2.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './outputs/EfficientNet_b2_two_stage_oof_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mModelConfig\u001b[38;5;241m.\u001b[39mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_oof_2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV Path: \u001b[39m\u001b[38;5;124m\"\u001b[39m, csv_path)\n\u001b[0;32m----> 4\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_oof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKaggle Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, calc_kaggle_score(oof_df))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage KL Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, oof_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36manalyze_oof\u001b[0;34m(oof_csv)\u001b[0m\n\u001b[1;32m     15\u001b[0m kl_criteria \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mKLDivLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m softmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moof_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m oof_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_pred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m oof_df[TARGETS_PRED]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39margmax(x), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m oof_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m oof_df[TARGETS]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39margmax(x), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/EfficientNet_b2_two_stage_oof_2.csv'"
     ]
    }
   ],
   "source": [
    "csv_path = f'./outputs/{ModelConfig.MODEL_NAME}_oof_2.csv'\n",
    "print(\"CSV Path: \", csv_path)\n",
    "\n",
    "oof_df = analyze_oof(csv_path)\n",
    "\n",
    "print(\"Kaggle Score: \", calc_kaggle_score(oof_df))\n",
    "print(\"Average KL Loss: \", oof_df[\"kl_loss\"].mean())\n",
    "\n",
    "display(oof_df.head())\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(oof_df['target_id'], oof_df['target_pred']) # (y_true, y_pred)\n",
    "cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=TARGET2ID.keys(), yticklabels=TARGET2ID.keys())\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title(csv_path.split('/')[-1].split('.')[0], fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./outputs/{csv_path.split('/')[-1].split('.')[0]}_CM.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Path:  ./outputs/EEGSeq_ResNet_GRU_Wavenet_oof_1.csv\n",
      "Kaggle Score:  0.7013359133315267\n",
      "Average KL Loss:  0.70133597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>...</th>\n",
       "      <th>fold</th>\n",
       "      <th>seizure_pred</th>\n",
       "      <th>lpd_pred</th>\n",
       "      <th>gpd_pred</th>\n",
       "      <th>lrda_pred</th>\n",
       "      <th>grda_pred</th>\n",
       "      <th>other_pred</th>\n",
       "      <th>target_pred</th>\n",
       "      <th>target_id</th>\n",
       "      <th>kl_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109125</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.067567</td>\n",
       "      <td>0.030916</td>\n",
       "      <td>0.286464</td>\n",
       "      <td>0.495960</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.331470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233216</td>\n",
       "      <td>0.206535</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.145550</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.375407</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.017170</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.054486</td>\n",
       "      <td>0.043614</td>\n",
       "      <td>0.868982</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.140432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.746248</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193257</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.107219</td>\n",
       "      <td>0.178399</td>\n",
       "      <td>0.487647</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.718164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "0  568657           0.0  0.000000      0.25   0.000000   0.166667    0.583333   \n",
       "1  582999           0.0  0.857143      0.00   0.071429   0.000000    0.071429   \n",
       "2  642382           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "3  751790           0.0  0.000000      1.00   0.000000   0.000000    0.000000   \n",
       "4  778705           0.0  0.000000      0.00   0.000000   0.000000    1.000000   \n",
       "\n",
       "   spectrogram_id     min     max  ...  fold  seizure_pred  lpd_pred  \\\n",
       "0       789577333     0.0    16.0  ...     0      0.109125  0.009968   \n",
       "1      1552638400     0.0    38.0  ...     0      0.233216  0.206535   \n",
       "2        14960202  1008.0  1032.0  ...     0      0.008237  0.017170   \n",
       "3       618728447   908.0   908.0  ...     0      0.012146  0.237113   \n",
       "4        52296320     0.0     0.0  ...     0      0.193257  0.021848   \n",
       "\n",
       "   gpd_pred  lrda_pred  grda_pred  other_pred  target_pred  target_id  \\\n",
       "0  0.067567   0.030916   0.286464    0.495960            5          5   \n",
       "1  0.005596   0.145550   0.033696    0.375407            5          1   \n",
       "2  0.007510   0.054486   0.043614    0.868982            5          5   \n",
       "3  0.746248   0.000135   0.000251    0.004108            2          2   \n",
       "4  0.011632   0.107219   0.178399    0.487647            5          5   \n",
       "\n",
       "    kl_loss  \n",
       "0  0.331470  \n",
       "1  1.050465  \n",
       "2  0.140432  \n",
       "3  0.292698  \n",
       "4  0.718164  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = f'./outputs/EEGSeq_ResNet_GRU_Wavenet_oof_1.csv'\n",
    "print(\"CSV Path: \", csv_path)\n",
    "\n",
    "oof_df = analyze_oof(csv_path)\n",
    "\n",
    "print(\"Kaggle Score: \", calc_kaggle_score(oof_df))\n",
    "print(\"Average KL Loss: \", oof_df[\"kl_loss\"].mean())\n",
    "\n",
    "display(oof_df.head())\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(oof_df['target_id'], oof_df['target_pred']) # (y_true, y_pred)\n",
    "cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=TARGET2ID.keys(), yticklabels=TARGET2ID.keys())\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title(csv_path.split('/')[-1].split('.')[0], fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./outputs/{csv_path.split('/')[-1].split('.')[0]}_CM.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
